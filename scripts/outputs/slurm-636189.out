[2024-05-30 10:58:49,745][HYDRA] Launching 490 jobs locally
[2024-05-30 10:58:49,745][HYDRA] 	#0 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 10:58:50,029][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 10:58:51,606][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
2024-05-30 10:58:53 [ERROR]: ❌ No module named 'torch_geometric'
Note torch_geometric is missing, please install it with 'pip install torch_geometric torch_scatter torch_sparse' or 'conda install -c pyg pyg pytorch-scatter pytorch-sparse'
2024-05-30 10:58:53 [ERROR]: ❌ name 'MessagePassing' is not defined
Note torch_geometric is missing, please install it with 'pip install torch_geometric torch_scatter torch_sparse' or 'conda install -c pyg pyg pytorch-scatter pytorch-sparse'
[2024-05-30 10:58:53,281][train.py][INFO] - Instantiating callbacks...
[2024-05-30 10:58:53,282][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 10:58:53,312][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 10:58:53,314][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 10:58:53,318][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 10:58:53,319][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 10:58:53,322][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 10:58:53,323][train.py][INFO] - Instantiating loggers...
[2024-05-30 10:58:53,323][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 10:58:53,325][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
/home/khickey/test_impute/env/lib/python3.12/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 10:58:53,848][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.12it/s v_num: 1.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-05-30 11:00:27,251][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/0>
[2024-05-30 11:00:27,252][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:00:27,256][HYDRA] 	#1 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 11:00:27,548][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:00:27,550][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:00:27,552][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:00:27,552][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:00:27,556][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:00:27,557][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:00:27,557][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:00:27,558][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:00:27,560][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:00:27,563][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:00:27,563][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:00:27,565][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:00:27,612][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.99it/s v_num: 1.000      
                                                              val/auc: 0.506    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.976  
                                                              train/f1: 0.975   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.025             
[2024-05-30 11:02:00,046][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/1>
[2024-05-30 11:02:00,047][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:02:00,050][HYDRA] 	#2 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 11:02:00,347][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:02:00,349][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:02:00,351][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:02:00,351][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:02:00,355][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:02:00,355][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:02:00,356][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:02:00,357][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:02:00,359][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:02:00,360][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:02:00,360][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:02:00,362][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:02:00,406][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.01it/s v_num: 1.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.976  
                                                              train/f1: 0.976   
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.022             
[2024-05-30 11:03:33,754][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/2>
[2024-05-30 11:03:33,757][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:03:33,775][HYDRA] 	#3 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 11:03:34,095][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:03:34,098][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:03:34,099][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:03:34,100][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:03:34,103][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:03:34,104][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:03:34,105][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:03:34,106][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:03:34,107][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:03:34,109][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:03:34,109][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:03:34,111][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:03:34,156][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.12it/s v_num: 1.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-05-30 11:05:06,460][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/3>
[2024-05-30 11:05:06,461][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:05:06,465][HYDRA] 	#4 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 11:05:06,745][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:05:06,747][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:05:06,749][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:05:06,749][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:05:06,753][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:05:06,753][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:05:06,754][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:05:06,755][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:05:06,757][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:05:06,760][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:05:06,761][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:05:06,763][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:05:06,849][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.03it/s v_num: 1.000      
                                                              val/auc: 0.672    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.444 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-05-30 11:06:38,021][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/4>
[2024-05-30 11:06:38,022][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:06:38,028][HYDRA] 	#5 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 11:06:38,306][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:06:38,308][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:06:38,310][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:06:38,310][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:06:38,313][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:06:38,314][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:06:38,315][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:06:38,316][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:06:38,317][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:06:38,318][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:06:38,319][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:06:38,321][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:06:38,362][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.03it/s v_num: 1.000      
                                                              val/auc: 0.678    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.556 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-05-30 11:08:10,731][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/5>
[2024-05-30 11:08:10,732][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:08:10,735][HYDRA] 	#6 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 11:08:11,028][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:08:11,031][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:08:11,033][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:08:11,033][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:08:11,036][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:08:11,037][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:08:11,038][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:08:11,039][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:08:11,041][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:08:11,042][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:08:11,043][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:08:11,045][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:08:11,088][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.99it/s v_num: 1.000      
                                                              val/auc: 0.561    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-05-30 11:09:44,575][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/6>
[2024-05-30 11:09:44,576][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:09:44,579][HYDRA] 	#7 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 11:09:44,859][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:09:44,862][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:09:44,863][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:09:44,864][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:09:44,867][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:09:44,868][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:09:44,868][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:09:44,869][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:09:44,871][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:09:44,873][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:09:44,873][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:09:44,875][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:09:44,954][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.43it/s v_num: 1.000      
                                                              val/auc: 0.672    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.444 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-05-30 11:11:18,208][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/7>
[2024-05-30 11:11:18,209][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:11:18,211][HYDRA] 	#8 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 11:11:18,563][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:11:18,566][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:11:18,568][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:11:18,568][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:11:18,572][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:11:18,573][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:11:18,573][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:11:18,574][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:11:18,576][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:11:18,577][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:11:18,577][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:11:18,579][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:11:18,625][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.99it/s v_num: 1.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-05-30 11:12:50,923][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/8>
[2024-05-30 11:12:50,924][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:12:50,927][HYDRA] 	#9 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 11:12:51,328][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:12:51,333][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:12:51,336][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:12:51,337][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:12:51,342][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:12:51,343][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:12:51,343][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:12:51,344][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:12:51,346][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:12:51,350][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:12:51,350][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:12:51,352][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:12:51,475][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.96it/s v_num: 1.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 11:14:24,159][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.0/9>
[2024-05-30 11:14:24,160][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:14:24,164][HYDRA] 	#10 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 11:14:24,448][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:14:24,451][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:14:24,453][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:14:24,453][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:14:24,457][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:14:24,458][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:14:24,458][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:14:24,459][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:14:24,461][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:14:24,462][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:14:24,462][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:14:24,465][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:14:24,510][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.04it/s v_num: 1.000      
                                                              val/auc: 0.561    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.222 val/mre:    
                                                              0.071 train/auc:  
                                                              0.984 train/f1:   
                                                              0.984             
                                                              train/precision:  
                                                              0.969             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-05-30 11:15:56,856][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/0>
[2024-05-30 11:15:56,857][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:15:56,860][HYDRA] 	#11 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 11:15:57,150][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:15:57,152][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:15:57,154][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:15:57,154][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:15:57,157][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:15:57,158][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:15:57,159][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:15:57,160][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:15:57,162][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:15:57,163][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:15:57,164][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:15:57,166][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:15:57,269][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.10it/s v_num: 1.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.070 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-05-30 11:17:28,772][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/1>
[2024-05-30 11:17:28,773][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:17:28,776][HYDRA] 	#12 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 11:17:29,055][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:17:29,058][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:17:29,059][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:17:29,060][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:17:29,063][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:17:29,064][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:17:29,065][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:17:29,066][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:17:29,067][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:17:29,068][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:17:29,069][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:17:29,071][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:17:29,113][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.09it/s v_num: 1.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.111 val/mre:    
                                                              0.065 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-05-30 11:18:59,231][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/2>
[2024-05-30 11:18:59,232][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:18:59,235][HYDRA] 	#13 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 11:18:59,523][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:18:59,525][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:18:59,527][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:18:59,527][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:18:59,531][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:18:59,532][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:18:59,532][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:18:59,533][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:18:59,535][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:18:59,538][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:18:59,538][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:18:59,540][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:18:59,588][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.13it/s v_num: 1.000      
                                                              val/auc: 0.678    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.556 val/mre:    
                                                              0.064 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 11:20:29,609][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/3>
[2024-05-30 11:20:29,609][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:20:29,613][HYDRA] 	#14 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 11:20:29,896][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:20:29,898][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:20:29,900][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:20:29,900][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:20:29,903][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:20:29,904][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:20:29,905][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:20:29,906][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:20:29,907][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:20:29,909][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:20:29,910][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:20:29,912][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:20:29,993][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.19it/s v_num: 1.000      
                                                              val/auc: 0.617    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.333 val/mre:    
                                                              0.067 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-05-30 11:21:59,744][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/4>
[2024-05-30 11:21:59,745][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:21:59,748][HYDRA] 	#15 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 11:22:00,024][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:22:00,027][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:22:00,028][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:22:00,029][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:22:00,032][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:22:00,033][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:22:00,033][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:22:00,034][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:22:00,036][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:22:00,037][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:22:00,037][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:22:00,039][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:22:00,083][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.672    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.444 val/mre:    
                                                              0.065 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 11:23:29,843][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/5>
[2024-05-30 11:23:29,844][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:23:29,847][HYDRA] 	#16 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 11:23:30,126][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:23:30,128][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:23:30,130][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:23:30,130][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:23:30,134][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:23:30,134][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:23:30,135][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:23:30,136][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:23:30,138][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:23:30,139][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:23:30,139][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:23:30,141][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:23:30,184][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.17it/s v_num: 1.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.111 val/mre:    
                                                              0.071 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.016             
[2024-05-30 11:24:59,567][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/6>
[2024-05-30 11:24:59,568][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:24:59,571][HYDRA] 	#17 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 11:24:59,849][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:24:59,852][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:24:59,853][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:24:59,854][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:24:59,857][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:24:59,858][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:24:59,858][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:24:59,859][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:24:59,861][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:24:59,862][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:24:59,862][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:24:59,864][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:24:59,908][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.15it/s v_num: 1.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.069 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.025             
[2024-05-30 11:26:29,688][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/7>
[2024-05-30 11:26:29,689][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:26:29,693][HYDRA] 	#18 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 11:26:29,979][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:26:29,982][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:26:29,983][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:26:29,984][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:26:29,987][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:26:29,988][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:26:29,989][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:26:29,990][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:26:29,991][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:26:29,992][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:26:29,992][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:26:29,994][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:26:30,036][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.10it/s v_num: 1.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.070 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.024             
[2024-05-30 11:28:00,186][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/8>
[2024-05-30 11:28:00,187][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:28:00,190][HYDRA] 	#19 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 11:28:00,472][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:28:00,475][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:28:00,476][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:28:00,477][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:28:00,480][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:28:00,481][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:28:00,482][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:28:00,482][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:28:00,484][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:28:00,485][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:28:00,485][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:28:00,487][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:28:00,530][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.10it/s v_num: 1.000      
                                                              val/auc: 0.506    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.111 val/mre:    
                                                              0.064 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-05-30 11:29:29,940][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.05/9>
[2024-05-30 11:29:29,941][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:29:29,944][HYDRA] 	#20 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 11:29:30,228][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:29:30,230][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:29:30,232][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:29:30,233][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:29:30,236][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:29:30,237][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:29:30,237][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:29:30,238][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:29:30,240][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:29:30,242][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:29:30,243][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:29:30,245][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:29:30,291][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.17it/s v_num: 1.000      
                                                              val/auc: 0.617    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.333 val/mre:    
                                                              0.075 train/auc:  
                                                              0.960 train/f1:   
                                                              0.959             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.019             
[2024-05-30 11:30:59,756][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/0>
[2024-05-30 11:30:59,757][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:30:59,760][HYDRA] 	#21 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 11:31:00,041][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:31:00,043][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:31:00,045][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:31:00,045][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:31:00,049][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:31:00,050][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:31:00,050][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:31:00,051][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:31:00,053][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:31:00,055][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:31:00,055][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:31:00,057][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:31:00,138][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.21it/s v_num: 1.000      
                                                              val/auc: 0.561    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.222 val/mre:    
                                                              0.078 train/auc:  
                                                              0.984 train/f1:   
                                                              0.984             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.024             
[2024-05-30 11:32:29,890][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/1>
[2024-05-30 11:32:29,891][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:32:29,894][HYDRA] 	#22 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 11:32:30,169][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:32:30,172][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:32:30,173][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:32:30,174][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:32:30,177][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:32:30,178][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:32:30,178][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:32:30,179][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:32:30,181][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:32:30,182][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:32:30,182][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:32:30,184][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:32:30,227][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.561    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.222 val/mre:    
                                                              0.075 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-05-30 11:34:00,167][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/2>
[2024-05-30 11:34:00,167][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:34:00,170][HYDRA] 	#23 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 11:34:00,447][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:34:00,449][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:34:00,451][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:34:00,451][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:34:00,454][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:34:00,455][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:34:00,456][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:34:00,457][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:34:00,458][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:34:00,459][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:34:00,459][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:34:00,461][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:34:00,503][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.17it/s v_num: 1.000      
                                                              val/auc: 0.733    
                                                              val/f1: 0.706     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.667 val/mre:    
                                                              0.067 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-05-30 11:35:30,430][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/3>
[2024-05-30 11:35:30,432][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:35:30,437][HYDRA] 	#24 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 11:35:30,716][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:35:30,718][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:35:30,720][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:35:30,720][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:35:30,723][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:35:30,724][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:35:30,725][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:35:30,726][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:35:30,727][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:35:30,728][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:35:30,729][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:35:30,731][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:35:30,773][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.722    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.444 val/mre:    
                                                              0.071 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 11:37:00,302][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/4>
[2024-05-30 11:37:00,303][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:37:00,306][HYDRA] 	#25 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 11:37:00,590][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:37:00,592][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:37:00,593][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:37:00,594][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:37:00,597][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:37:00,598][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:37:00,599][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:37:00,599][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:37:00,601][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:37:00,602][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:37:00,602][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:37:00,604][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:37:00,647][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.13it/s v_num: 1.000      
                                                              val/auc: 0.622    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.444 val/mre:    
                                                              0.066 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 11:38:30,432][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/5>
[2024-05-30 11:38:30,433][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:38:30,436][HYDRA] 	#26 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 11:38:30,719][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:38:30,722][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:38:30,723][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:38:30,724][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:38:30,727][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:38:30,728][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:38:30,729][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:38:30,729][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:38:30,731][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:38:30,733][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:38:30,733][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:38:30,735][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:38:30,779][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.13it/s v_num: 1.000      
                                                              val/auc: 0.617    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.333 val/mre:    
                                                              0.071 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-05-30 11:40:01,099][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/6>
[2024-05-30 11:40:01,100][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:40:01,104][HYDRA] 	#27 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 11:40:01,390][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:40:01,392][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:40:01,394][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:40:01,394][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:40:01,398][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:40:01,399][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:40:01,400][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:40:01,401][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:40:01,402][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:40:01,405][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:40:01,405][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:40:01,407][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:40:01,469][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.080 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-05-30 11:41:30,987][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/7>
[2024-05-30 11:41:30,988][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:41:30,991][HYDRA] 	#28 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 11:41:31,268][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:41:31,271][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:41:31,272][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:41:31,273][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:41:31,276][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:41:31,277][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:41:31,277][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:41:31,278][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:41:31,280][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:41:31,281][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:41:31,281][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:41:31,283][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:41:31,327][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.22it/s v_num: 1.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.075 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-05-30 11:43:01,488][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/8>
[2024-05-30 11:43:01,489][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:43:01,492][HYDRA] 	#29 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 11:43:01,769][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:43:01,772][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:43:01,773][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:43:01,774][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:43:01,777][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:43:01,778][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:43:01,778][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:43:01,779][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:43:01,781][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:43:01,782][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:43:01,782][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:43:01,784][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:43:01,828][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.15it/s v_num: 1.000      
                                                              val/auc: 0.561    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.222 val/mre:    
                                                              0.077 train/auc:  
                                                              0.968 train/f1:   
                                                              0.967             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.016             
[2024-05-30 11:44:31,766][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.1/9>
[2024-05-30 11:44:31,766][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:44:31,770][HYDRA] 	#30 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 11:44:32,048][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:44:32,050][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:44:32,052][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:44:32,052][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:44:32,055][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:44:32,056][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:44:32,057][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:44:32,058][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:44:32,059][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:44:32,060][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:44:32,060][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:44:32,062][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:44:32,106][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.13it/s v_num: 1.000      
                                                              val/auc: 0.667    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.333 val/mre:    
                                                              0.071 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 11:46:01,969][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/0>
[2024-05-30 11:46:01,970][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:46:01,973][HYDRA] 	#31 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 11:46:02,281][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:46:02,284][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:46:02,286][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:46:02,286][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:46:02,289][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:46:02,290][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:46:02,291][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:46:02,292][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:46:02,294][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:46:02,294][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:46:02,295][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:46:02,297][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:46:02,340][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.14it/s v_num: 1.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.082 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.024             
[2024-05-30 11:47:32,456][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/1>
[2024-05-30 11:47:32,456][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:47:32,459][HYDRA] 	#32 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 11:47:32,736][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:47:32,739][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:47:32,740][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:47:32,741][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:47:32,744][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:47:32,745][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:47:32,745][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:47:32,746][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:47:32,748][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:47:32,749][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:47:32,749][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:47:32,751][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:47:32,793][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.12it/s v_num: 1.000      
                                                              val/auc: 0.672    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.444 val/mre:    
                                                              0.090 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-05-30 11:49:02,243][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/2>
[2024-05-30 11:49:02,243][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:49:02,246][HYDRA] 	#33 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 11:49:02,530][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:49:02,533][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:49:02,534][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:49:02,535][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:49:02,538][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:49:02,539][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:49:02,539][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:49:02,540][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:49:02,542][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:49:02,544][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:49:02,544][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:49:02,546][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:49:02,631][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.628    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre:    
                                                              0.086 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-05-30 11:50:32,766][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/3>
[2024-05-30 11:50:32,767][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:50:32,770][HYDRA] 	#34 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 11:50:33,049][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:50:33,052][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:50:33,053][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:50:33,054][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:50:33,057][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:50:33,058][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:50:33,059][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:50:33,059][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:50:33,061][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:50:33,062][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:50:33,062][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:50:33,064][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:50:33,106][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.13it/s v_num: 1.000      
                                                              val/auc: 0.622    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.444 val/mre:    
                                                              0.077 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 11:52:02,823][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/4>
[2024-05-30 11:52:02,824][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:52:02,828][HYDRA] 	#35 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 11:52:03,116][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:52:03,119][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:52:03,120][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:52:03,121][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:52:03,124][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:52:03,125][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:52:03,126][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:52:03,126][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:52:03,128][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:52:03,131][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:52:03,131][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:52:03,133][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:52:03,180][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.17it/s v_num: 1.000      
                                                              val/auc: 0.522    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.444 val/mre:    
                                                              0.072 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 11:53:33,369][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/5>
[2024-05-30 11:53:33,370][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:53:33,373][HYDRA] 	#36 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 11:53:33,652][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:53:33,654][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:53:33,656][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:53:33,656][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:53:33,659][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:53:33,660][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:53:33,661][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:53:33,662][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:53:33,663][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:53:33,664][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:53:33,665][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:53:33,667][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:53:33,710][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.617    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.333 val/mre:    
                                                              0.080 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-05-30 11:55:03,013][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/6>
[2024-05-30 11:55:03,013][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:55:03,017][HYDRA] 	#37 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 11:55:03,508][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:55:03,511][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:55:03,512][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:55:03,513][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:55:03,516][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:55:03,517][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:55:03,518][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:55:03,518][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:55:03,520][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:55:03,521][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:55:03,522][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:55:03,524][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:55:03,567][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.19it/s v_num: 1.000      
                                                              val/auc: 0.622    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.444 val/mre:    
                                                              0.081 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-05-30 11:56:32,768][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/7>
[2024-05-30 11:56:32,769][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:56:32,772][HYDRA] 	#38 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 11:56:33,051][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:56:33,053][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:56:33,055][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:56:33,055][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:56:33,059][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:56:33,059][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:56:33,060][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:56:33,061][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:56:33,063][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:56:33,064][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:56:33,064][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:56:33,066][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:56:33,109][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.10it/s v_num: 1.000      
                                                              val/auc: 0.578    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.556 val/mre:    
                                                              0.076 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-05-30 11:58:03,067][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/8>
[2024-05-30 11:58:03,068][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:58:03,071][HYDRA] 	#39 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 11:58:03,356][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:58:03,358][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:58:03,360][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:58:03,360][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:58:03,363][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:58:03,364][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:58:03,365][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:58:03,366][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:58:03,368][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:58:03,368][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:58:03,369][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:58:03,371][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:58:03,416][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.14it/s v_num: 1.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.078 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 11:59:33,403][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.15/9>
[2024-05-30 11:59:33,404][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 11:59:33,408][HYDRA] 	#40 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 11:59:33,688][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 11:59:33,690][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 11:59:33,692][train.py][INFO] - Instantiating callbacks...
[2024-05-30 11:59:33,692][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 11:59:33,696][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 11:59:33,696][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 11:59:33,697][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 11:59:33,698][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 11:59:33,700][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 11:59:33,701][train.py][INFO] - Instantiating loggers...
[2024-05-30 11:59:33,701][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 11:59:33,703][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 11:59:33,745][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.10it/s v_num: 1.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.222 val/mre:    
                                                              0.087 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 12:01:04,254][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/0>
[2024-05-30 12:01:04,254][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:01:04,257][HYDRA] 	#41 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 12:01:04,541][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:01:04,544][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:01:04,546][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:01:04,546][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:01:04,550][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:01:04,551][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:01:04,552][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:01:04,552][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:01:04,554][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:01:04,555][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:01:04,555][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:01:04,557][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:01:04,600][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.633    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.667 val/mre:    
                                                              0.079 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-05-30 12:02:34,124][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/1>
[2024-05-30 12:02:34,125][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:02:34,131][HYDRA] 	#42 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 12:02:34,447][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:02:34,451][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:02:34,453][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:02:34,453][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:02:34,457][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:02:34,458][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:02:34,458][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:02:34,459][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:02:34,461][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:02:34,462][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:02:34,462][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:02:34,464][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:02:34,524][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.12it/s v_num: 1.000      
                                                              val/auc: 0.722    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.444 val/mre:    
                                                              0.080 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 12:04:04,640][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/2>
[2024-05-30 12:04:04,641][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:04:04,645][HYDRA] 	#43 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 12:04:04,927][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:04:04,929][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:04:04,931][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:04:04,931][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:04:04,934][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:04:04,935][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:04:04,936][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:04:04,937][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:04:04,939][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:04:04,941][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:04:04,941][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:04:04,943][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:04:05,046][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.22it/s v_num: 1.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.083 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 12:05:34,716][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/3>
[2024-05-30 12:05:34,718][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:05:34,735][HYDRA] 	#44 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 12:05:35,043][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:05:35,045][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:05:35,047][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:05:35,047][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:05:35,050][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:05:35,051][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:05:35,052][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:05:35,053][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:05:35,054][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:05:35,055][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:05:35,056][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:05:35,058][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:05:35,101][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.14it/s v_num: 1.000      
                                                              val/auc: 0.678    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.556 val/mre:    
                                                              0.078 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 12:07:04,904][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/4>
[2024-05-30 12:07:04,905][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:07:04,909][HYDRA] 	#45 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 12:07:05,199][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:07:05,202][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:07:05,204][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:07:05,204][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:07:05,208][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:07:05,209][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:07:05,210][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:07:05,211][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:07:05,212][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:07:05,215][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:07:05,215][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:07:05,218][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:07:05,265][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.18it/s v_num: 1.000      
                                                              val/auc: 0.411    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.076 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 12:08:34,681][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/5>
[2024-05-30 12:08:34,681][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:08:34,685][HYDRA] 	#46 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 12:08:34,963][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:08:34,966][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:08:34,967][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:08:34,968][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:08:34,971][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:08:34,972][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:08:34,973][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:08:34,973][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:08:34,975][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:08:34,976][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:08:34,976][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:08:34,979][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:08:35,021][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.14it/s v_num: 1.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.086 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-05-30 12:10:04,487][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/6>
[2024-05-30 12:10:04,488][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:10:04,491][HYDRA] 	#47 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 12:10:04,772][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:10:04,774][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:10:04,776][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:10:04,776][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:10:04,780][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:10:04,781][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:10:04,781][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:10:04,782][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:10:04,784][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:10:04,785][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:10:04,785][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:10:04,788][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:10:04,831][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.17it/s v_num: 1.000      
                                                              val/auc: 0.561    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.222 val/mre:    
                                                              0.076 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-05-30 12:11:34,760][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/7>
[2024-05-30 12:11:34,760][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:11:34,763][HYDRA] 	#48 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 12:11:35,039][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:11:35,041][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:11:35,043][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:11:35,043][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:11:35,047][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:11:35,048][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:11:35,048][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:11:35,049][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:11:35,051][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:11:35,053][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:11:35,053][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:11:35,055][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:11:35,135][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.10it/s v_num: 1.000      
                                                              val/auc: 0.622    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.444 val/mre:    
                                                              0.078 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 12:13:04,668][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/8>
[2024-05-30 12:13:04,669][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:13:04,672][HYDRA] 	#49 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 12:13:04,952][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:13:04,954][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:13:04,956][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:13:04,956][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:13:04,960][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:13:04,960][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:13:04,961][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:13:04,962][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:13:04,964][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:13:04,965][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:13:04,965][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:13:04,967][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:13:05,010][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.18it/s v_num: 1.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.081 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-05-30 12:14:34,444][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.2/9>
[2024-05-30 12:14:34,445][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:14:34,448][HYDRA] 	#50 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 12:14:34,728][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:14:34,730][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:14:34,731][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:14:34,732][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:14:34,735][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:14:34,736][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:14:34,737][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:14:34,737][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:14:34,739][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:14:34,740][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:14:34,740][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:14:34,742][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:14:34,784][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.13it/s v_num: 1.000      
                                                              val/auc: 0.617    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.333 val/mre:    
                                                              0.085 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 12:16:04,954][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/0>
[2024-05-30 12:16:04,955][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:16:04,958][HYDRA] 	#51 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 12:16:05,245][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:16:05,247][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:16:05,249][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:16:05,249][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:16:05,253][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:16:05,254][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:16:05,254][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:16:05,255][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:16:05,257][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:16:05,259][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:16:05,260][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:16:05,262][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:16:05,309][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.23it/s v_num: 1.000      
                                                              val/auc: 0.583    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 val/mre:    
                                                              0.085 train/auc:  
                                                              0.976 train/f1:   
                                                              0.975             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.017             
[2024-05-30 12:17:35,219][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/1>
[2024-05-30 12:17:35,219][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:17:35,223][HYDRA] 	#52 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 12:17:35,503][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:17:35,505][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:17:35,507][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:17:35,507][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:17:35,511][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:17:35,511][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:17:35,512][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:17:35,513][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:17:35,515][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:17:35,516][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:17:35,517][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:17:35,519][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:17:35,598][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.18it/s v_num: 1.000      
                                                              val/auc: 0.778    
                                                              val/f1: 0.714     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.556 val/mre:    
                                                              0.084 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 12:19:05,157][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/2>
[2024-05-30 12:19:05,158][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:19:05,161][HYDRA] 	#53 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 12:19:05,444][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:19:05,446][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:19:05,448][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:19:05,448][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:19:05,451][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:19:05,452][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:19:05,453][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:19:05,454][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:19:05,455][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:19:05,456][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:19:05,457][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:19:05,459][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:19:05,501][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.15it/s v_num: 1.000      
                                                              val/auc: 0.617    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.333 val/mre:    
                                                              0.082 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 12:20:35,431][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/3>
[2024-05-30 12:20:35,431][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:20:35,435][HYDRA] 	#54 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 12:20:35,716][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:20:35,719][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:20:35,720][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:20:35,721][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:20:35,724][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:20:35,725][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:20:35,726][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:20:35,727][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:20:35,728][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:20:35,729][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:20:35,729][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:20:35,731][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:20:35,774][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.19it/s v_num: 1.000      
                                                              val/auc: 0.617    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.333 val/mre:    
                                                              0.079 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-05-30 12:22:04,987][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/4>
[2024-05-30 12:22:04,988][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:22:04,992][HYDRA] 	#55 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 12:22:05,282][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:22:05,284][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:22:05,286][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:22:05,286][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:22:05,289][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:22:05,290][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:22:05,291][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:22:05,292][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:22:05,293][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:22:05,296][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:22:05,296][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:22:05,298][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:22:05,344][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.18it/s v_num: 1.000      
                                                              val/auc: 0.667    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.333 val/mre:    
                                                              0.089 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-05-30 12:23:35,254][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/5>
[2024-05-30 12:23:35,255][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:23:35,259][HYDRA] 	#56 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 12:23:35,542][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:23:35,545][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:23:35,546][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:23:35,547][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:23:35,550][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:23:35,551][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:23:35,552][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:23:35,552][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:23:35,554][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:23:35,555][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:23:35,555][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:23:35,557][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:23:35,603][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.14it/s v_num: 1.000      
                                                              val/auc: 0.372    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.364 val/recall: 
                                                              0.444 val/mre:    
                                                              0.083 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-05-30 12:25:04,762][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/6>
[2024-05-30 12:25:04,762][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:25:04,766][HYDRA] 	#57 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 12:25:05,062][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:25:05,066][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:25:05,067][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:25:05,068][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:25:05,071][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:25:05,072][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:25:05,073][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:25:05,073][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:25:05,075][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:25:05,077][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:25:05,077][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:25:05,079][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:25:05,161][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.17it/s v_num: 1.000      
                                                              val/auc: 0.522    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.444 val/mre:    
                                                              0.080 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 12:26:35,194][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/7>
[2024-05-30 12:26:35,195][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:26:35,198][HYDRA] 	#58 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 12:26:35,477][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:26:35,479][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:26:35,481][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:26:35,481][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:26:35,485][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:26:35,486][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:26:35,486][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:26:35,487][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:26:35,489][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:26:35,490][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:26:35,490][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:26:35,492][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:26:35,537][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.23it/s v_num: 1.000      
                                                              val/auc: 0.522    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.444 val/mre:    
                                                              0.083 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-05-30 12:28:04,944][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/8>
[2024-05-30 12:28:04,944][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:28:04,948][HYDRA] 	#59 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 12:28:05,225][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:28:05,227][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:28:05,229][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:28:05,229][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:28:05,232][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:28:05,233][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:28:05,234][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:28:05,235][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:28:05,236][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:28:05,237][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:28:05,237][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:28:05,239][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:28:05,281][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.13it/s v_num: 1.000      
                                                              val/auc: 0.728    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.833 val/recall: 
                                                              0.556 val/mre:    
                                                              0.075 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 12:29:34,850][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.25/9>
[2024-05-30 12:29:34,851][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:29:34,855][HYDRA] 	#60 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 12:29:35,141][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:29:35,143][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:29:35,145][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:29:35,145][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:29:35,149][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:29:35,150][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:29:35,150][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:29:35,151][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:29:35,153][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:29:35,156][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:29:35,156][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:29:35,158][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:29:35,205][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.19it/s v_num: 1.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.222 val/mre:    
                                                              0.084 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-05-30 12:31:04,825][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/0>
[2024-05-30 12:31:04,826][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:31:04,829][HYDRA] 	#61 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 12:31:05,113][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:31:05,115][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:31:05,117][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:31:05,117][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:31:05,121][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:31:05,121][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:31:05,122][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:31:05,123][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:31:05,125][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:31:05,127][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:31:05,127][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:31:05,129][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:31:05,215][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.19it/s v_num: 1.000      
                                                              val/auc: 0.678    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.556 val/mre:    
                                                              0.091 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-05-30 12:32:34,621][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/1>
[2024-05-30 12:32:34,622][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:32:34,625][HYDRA] 	#62 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 12:32:34,904][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:32:34,906][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:32:34,908][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:32:34,908][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:32:34,912][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:32:34,912][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:32:34,913][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:32:34,914][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:32:34,916][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:32:34,917][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:32:34,917][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:32:34,919][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:32:34,961][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.12it/s v_num: 1.000      
                                                              val/auc: 0.622    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.444 val/mre:    
                                                              0.081 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 12:34:04,755][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/2>
[2024-05-30 12:34:04,755][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:34:04,758][HYDRA] 	#63 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 12:34:05,038][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:34:05,041][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:34:05,042][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:34:05,043][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:34:05,046][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:34:05,047][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:34:05,047][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:34:05,048][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:34:05,050][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:34:05,051][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:34:05,051][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:34:05,053][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:34:05,096][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.18it/s v_num: 1.000      
                                                              val/auc: 0.628    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre:    
                                                              0.083 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 12:35:34,407][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/3>
[2024-05-30 12:35:34,408][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:35:34,411][HYDRA] 	#64 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 12:35:34,687][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:35:34,689][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:35:34,691][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:35:34,691][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:35:34,695][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:35:34,695][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:35:34,696][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:35:34,697][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:35:34,699][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:35:34,700][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:35:34,700][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:35:34,702][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:35:34,746][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.672    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.444 val/mre:    
                                                              0.083 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 12:37:04,603][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/4>
[2024-05-30 12:37:04,604][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:37:04,607][HYDRA] 	#65 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 12:37:04,890][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:37:04,893][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:37:04,894][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:37:04,894][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:37:04,898][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:37:04,899][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:37:04,900][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:37:04,900][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:37:04,902][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:37:04,903][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:37:04,903][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:37:04,905][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:37:04,947][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.082 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 12:38:34,381][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/5>
[2024-05-30 12:38:34,382][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:38:34,385][HYDRA] 	#66 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 12:38:34,668][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:38:34,671][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:38:34,672][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:38:34,673][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:38:34,676][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:38:34,677][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:38:34,678][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:38:34,679][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:38:34,680][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:38:34,682][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:38:34,682][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:38:34,684][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:38:34,766][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.15it/s v_num: 1.000      
                                                              val/auc: 0.622    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.444 val/mre:    
                                                              0.082 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 12:40:04,690][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/6>
[2024-05-30 12:40:04,691][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:40:04,694][HYDRA] 	#67 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 12:40:04,970][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:40:04,972][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:40:04,973][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:40:04,974][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:40:04,977][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:40:04,978][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:40:04,980][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:40:04,981][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:40:04,982][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:40:04,983][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:40:04,984][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:40:04,986][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:40:05,028][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.19it/s v_num: 1.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.089 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-05-30 12:41:34,516][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/7>
[2024-05-30 12:41:34,517][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:41:34,520][HYDRA] 	#68 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 12:41:34,799][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:41:34,801][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:41:34,803][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:41:34,803][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:41:34,807][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:41:34,807][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:41:34,808][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:41:34,809][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:41:34,811][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:41:34,812][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:41:34,812][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:41:34,814][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:41:34,856][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.12it/s v_num: 1.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.091 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-05-30 12:43:04,708][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/8>
[2024-05-30 12:43:04,709][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:43:04,712][HYDRA] 	#69 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 12:43:04,994][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:43:04,997][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:43:04,998][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:43:04,999][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:43:05,002][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:43:05,003][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:43:05,004][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:43:05,004][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:43:05,006][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:43:05,007][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:43:05,007][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:43:05,009][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:43:05,102][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.18it/s v_num: 1.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.081 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 12:44:34,396][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/advice_yourself/0.3/9>
[2024-05-30 12:44:34,396][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:44:34,400][HYDRA] 	#70 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 12:44:34,684][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:44:34,686][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:44:34,688][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:44:34,688][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:44:34,691][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:44:34,692][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:44:34,693][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:44:34,694][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:44:34,696][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:44:34,699][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:44:34,699][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:44:34,701][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:44:34,791][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.18it/s v_num: 1.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 0.982  
                                                              train/f1: 0.982   
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.016             
[2024-05-30 12:46:01,549][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/0>
[2024-05-30 12:46:01,550][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:46:01,553][HYDRA] 	#71 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 12:46:01,869][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:46:01,872][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:46:01,874][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:46:01,874][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:46:01,878][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:46:01,879][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:46:01,880][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:46:01,881][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:46:01,883][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:46:01,884][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:46:01,884][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:46:01,886][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:46:01,931][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.21it/s v_num: 1.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-05-30 12:47:29,366][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/1>
[2024-05-30 12:47:29,367][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:47:29,370][HYDRA] 	#72 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 12:47:29,647][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:47:29,650][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:47:29,651][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:47:29,652][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:47:29,655][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:47:29,656][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:47:29,656][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:47:29,657][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:47:29,659][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:47:29,660][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:47:29,660][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:47:29,662][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:47:29,704][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.19it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-05-30 12:48:57,739][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/2>
[2024-05-30 12:48:57,739][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:48:57,742][HYDRA] 	#73 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 12:48:58,019][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:48:58,021][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:48:58,022][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:48:58,023][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:48:58,026][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:48:58,027][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:48:58,028][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:48:58,028][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:48:58,030][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:48:58,031][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:48:58,031][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:48:58,035][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:48:58,078][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.15it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.026             
[2024-05-30 12:50:27,183][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/3>
[2024-05-30 12:50:27,184][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:50:27,187][HYDRA] 	#74 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 12:50:27,480][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:50:27,482][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:50:27,484][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:50:27,484][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:50:27,488][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:50:27,489][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:50:27,489][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:50:27,490][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:50:27,492][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:50:27,495][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:50:27,495][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:50:27,498][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:50:27,582][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.06it/s v_num: 1.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.991  
                                                              train/f1: 0.991   
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-05-30 12:51:58,145][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/4>
[2024-05-30 12:51:58,146][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:51:58,149][HYDRA] 	#75 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 12:51:58,434][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:51:58,436][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:51:58,438][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:51:58,438][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:51:58,441][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:51:58,442][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:51:58,443][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:51:58,444][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:51:58,446][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:51:58,446][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:51:58,447][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:51:58,449][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:51:58,495][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.04it/s v_num: 1.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 12:53:28,896][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/5>
[2024-05-30 12:53:28,896][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:53:28,900][HYDRA] 	#76 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 12:53:29,192][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:53:29,195][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:53:29,197][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:53:29,198][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:53:29,202][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:53:29,202][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:53:29,203][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:53:29,204][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:53:29,206][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:53:29,207][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:53:29,207][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:53:29,209][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:53:29,255][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.11it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 12:54:58,880][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/6>
[2024-05-30 12:54:58,881][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:54:58,884][HYDRA] 	#77 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 12:54:59,169][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:54:59,171][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:54:59,173][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:54:59,173][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:54:59,176][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:54:59,177][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:54:59,178][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:54:59,179][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:54:59,181][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:54:59,181][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:54:59,182][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:54:59,184][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:54:59,226][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.12it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 0.982  
                                                              train/f1: 0.982   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.964 train/mre:  
                                                              0.015             
[2024-05-30 12:56:28,372][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/7>
[2024-05-30 12:56:28,373][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:56:28,377][HYDRA] 	#78 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 12:56:28,672][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:56:28,675][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:56:28,676][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:56:28,677][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:56:28,680][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:56:28,681][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:56:28,682][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:56:28,683][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:56:28,684][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:56:28,688][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:56:28,688][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:56:28,690][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:56:28,777][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.10it/s v_num: 1.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 12:57:58,558][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/8>
[2024-05-30 12:57:58,558][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:57:58,562][HYDRA] 	#79 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 12:57:58,846][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:57:58,848][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:57:58,850][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:57:58,850][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:57:58,854][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:57:58,854][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:57:58,855][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:57:58,856][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:57:58,858][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:57:58,859][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:57:58,859][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:57:58,861][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:57:58,904][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.11it/s v_num: 1.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-05-30 12:59:28,179][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.0/9>
[2024-05-30 12:59:28,180][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 12:59:28,184][HYDRA] 	#80 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 12:59:28,512][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 12:59:28,514][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 12:59:28,516][train.py][INFO] - Instantiating callbacks...
[2024-05-30 12:59:28,516][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 12:59:28,520][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 12:59:28,521][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 12:59:28,522][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 12:59:28,523][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 12:59:28,524][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 12:59:28,525][train.py][INFO] - Instantiating loggers...
[2024-05-30 12:59:28,526][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 12:59:28,528][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 12:59:28,655][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.12it/s v_num: 1.000      
                                                              val/auc: 0.667    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.333 val/mre:    
                                                              0.059 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 13:00:58,193][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/0>
[2024-05-30 13:00:58,194][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:00:58,197][HYDRA] 	#81 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 13:00:58,495][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:00:58,498][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:00:58,500][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:00:58,500][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:00:58,504][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:00:58,505][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:00:58,506][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:00:58,507][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:00:58,508][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:00:58,509][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:00:58,510][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:00:58,512][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:00:58,555][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.10it/s v_num: 1.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre:    
                                                              0.055 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 13:02:27,468][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/1>
[2024-05-30 13:02:27,469][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:02:27,473][HYDRA] 	#82 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 13:02:27,768][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:02:27,770][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:02:27,772][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:02:27,772][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:02:27,776][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:02:27,776][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:02:27,777][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:02:27,778][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:02:27,780][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:02:27,783][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:02:27,783][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:02:27,785][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:02:27,872][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.14it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.444 val/mre:    
                                                              0.063 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-05-30 13:03:58,408][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/2>
[2024-05-30 13:03:58,409][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:03:58,413][HYDRA] 	#83 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 13:03:58,699][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:03:58,701][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:03:58,703][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:03:58,703][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:03:58,707][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:03:58,707][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:03:58,708][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:03:58,709][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:03:58,711][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:03:58,712][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:03:58,712][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:03:58,714][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:03:58,758][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.09it/s v_num: 1.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.111 val/mre:    
                                                              0.064 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-05-30 13:05:28,810][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/3>
[2024-05-30 13:05:28,811][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:05:28,815][HYDRA] 	#84 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 13:05:29,102][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:05:29,105][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:05:29,106][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:05:29,107][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:05:29,110][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:05:29,111][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:05:29,112][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:05:29,112][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:05:29,114][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:05:29,115][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:05:29,115][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:05:29,117][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:05:29,163][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.17it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.111 val/mre:    
                                                              0.066 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.026             
[2024-05-30 13:06:57,839][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/4>
[2024-05-30 13:06:57,840][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:06:57,844][HYDRA] 	#85 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 13:06:58,150][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:06:58,155][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:06:58,156][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:06:58,157][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:06:58,161][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:06:58,161][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:06:58,162][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:06:58,163][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:06:58,165][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:06:58,166][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:06:58,166][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:06:58,168][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:06:58,233][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.11it/s v_num: 1.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.222 val/mre:    
                                                              0.061 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 13:08:26,867][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/5>
[2024-05-30 13:08:26,867][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:08:26,871][HYDRA] 	#86 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 13:08:27,157][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:08:27,159][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:08:27,161][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:08:27,161][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:08:27,165][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:08:27,165][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:08:27,166][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:08:27,167][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:08:27,169][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:08:27,170][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:08:27,170][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:08:27,172][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:08:27,215][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.15it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.054 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 13:09:55,851][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/6>
[2024-05-30 13:09:55,852][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:09:55,855][HYDRA] 	#87 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 13:09:56,141][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:09:56,143][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:09:56,145][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:09:56,145][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:09:56,148][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:09:56,149][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:09:56,150][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:09:56,151][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:09:56,153][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:09:56,154][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:09:56,154][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:09:56,156][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:09:56,199][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.15it/s v_num: 1.000      
                                                              val/auc: 0.333    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.333 val/mre:    
                                                              0.060 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-05-30 13:11:24,865][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/7>
[2024-05-30 13:11:24,867][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:11:24,879][HYDRA] 	#88 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 13:11:25,166][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:11:25,169][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:11:25,171][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:11:25,171][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:11:25,174][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:11:25,175][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:11:25,176][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:11:25,177][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:11:25,179][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:11:25,180][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:11:25,180][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:11:25,182][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:11:25,228][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.13it/s v_num: 1.000      
                                                              val/auc: 0.333    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.059 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 13:12:54,697][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/8>
[2024-05-30 13:12:54,698][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:12:54,701][HYDRA] 	#89 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 13:12:54,987][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:12:54,989][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:12:54,991][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:12:54,991][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:12:54,994][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:12:54,995][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:12:54,996][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:12:54,997][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:12:54,998][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:12:54,999][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:12:55,000][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:12:55,002][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:12:55,044][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.11it/s v_num: 1.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.064 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-05-30 13:14:24,259][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.05/9>
[2024-05-30 13:14:24,260][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:14:24,263][HYDRA] 	#90 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 13:14:24,545][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:14:24,547][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:14:24,549][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:14:24,549][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:14:24,552][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:14:24,553][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:14:24,554][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:14:24,555][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:14:24,556][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:14:24,557][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:14:24,558][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:14:24,560][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:14:24,606][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.14it/s v_num: 1.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.333 val/mre:    
                                                              0.058 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-05-30 13:15:53,534][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/0>
[2024-05-30 13:15:53,535][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:15:53,538][HYDRA] 	#91 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 13:15:53,827][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:15:53,830][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:15:53,831][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:15:53,832][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:15:53,835][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:15:53,836][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:15:53,837][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:15:53,838][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:15:53,839][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:15:53,842][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:15:53,843][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:15:53,845][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:15:53,955][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.22it/s v_num: 1.000      
                                                              val/auc: 0.222    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.222 val/recall: 
                                                              0.222 val/mre:    
                                                              0.061 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 13:17:22,448][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/1>
[2024-05-30 13:17:22,449][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:17:22,452][HYDRA] 	#92 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 13:17:22,737][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:17:22,739][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:17:22,741][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:17:22,741][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:17:22,745][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:17:22,745][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:17:22,746][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:17:22,747][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:17:22,749][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:17:22,750][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:17:22,750][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:17:22,752][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:17:22,797][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.08it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.074 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-05-30 13:18:53,313][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/2>
[2024-05-30 13:18:53,314][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:18:53,318][HYDRA] 	#93 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 13:18:53,772][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:18:53,774][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:18:53,776][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:18:53,776][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:18:53,782][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:18:53,783][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:18:53,783][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:18:53,784][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:18:53,786][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:18:53,790][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:18:53,790][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:18:53,792][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:18:53,944][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.09it/s v_num: 1.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre:    
                                                              0.071 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-05-30 13:20:22,964][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/3>
[2024-05-30 13:20:22,965][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:20:22,969][HYDRA] 	#94 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 13:20:23,255][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:20:23,257][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:20:23,259][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:20:23,259][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:20:23,263][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:20:23,264][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:20:23,264][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:20:23,265][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:20:23,267][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:20:23,268][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:20:23,268][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:20:23,270][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:20:23,314][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.18it/s v_num: 1.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.070 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-05-30 13:21:51,752][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/4>
[2024-05-30 13:21:51,753][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:21:51,756][HYDRA] 	#95 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 13:21:52,043][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:21:52,045][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:21:52,047][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:21:52,047][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:21:52,050][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:21:52,051][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:21:52,052][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:21:52,053][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:21:52,055][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:21:52,056][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:21:52,056][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:21:52,058][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:21:52,108][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.12it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.056 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 13:23:21,286][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/5>
[2024-05-30 13:23:21,286][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:23:21,290][HYDRA] 	#96 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 13:23:21,577][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:23:21,579][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:23:21,581][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:23:21,582][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:23:21,585][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:23:21,586][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:23:21,587][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:23:21,587][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:23:21,589][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:23:21,590][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:23:21,590][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:23:21,592][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:23:21,636][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.061 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 13:24:50,084][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/6>
[2024-05-30 13:24:50,085][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:24:50,088][HYDRA] 	#97 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 13:24:50,375][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:24:50,377][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:24:50,379][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:24:50,379][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:24:50,383][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:24:50,383][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:24:50,384][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:24:50,385][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:24:50,387][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:24:50,388][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:24:50,388][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:24:50,390][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:24:50,434][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.13it/s v_num: 1.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre:    
                                                              0.062 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-05-30 13:26:18,897][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/7>
[2024-05-30 13:26:18,897][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:26:18,901][HYDRA] 	#98 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 13:26:19,192][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:26:19,194][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:26:19,196][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:26:19,196][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:26:19,199][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:26:19,200][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:26:19,201][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:26:19,202][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:26:19,204][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:26:19,206][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:26:19,207][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:26:19,209][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:26:19,258][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.07it/s v_num: 1.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.073 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.024             
[2024-05-30 13:27:49,929][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/8>
[2024-05-30 13:27:49,930][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:27:49,933][HYDRA] 	#99 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 13:27:50,230][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:27:50,232][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:27:50,234][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:27:50,234][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:27:50,237][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:27:50,238][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:27:50,239][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:27:50,240][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:27:50,241][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:27:50,243][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:27:50,243][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:27:50,245][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:27:50,293][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.17it/s v_num: 1.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.060 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-05-30 13:29:19,080][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.1/9>
[2024-05-30 13:29:19,081][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:29:19,088][HYDRA] 	#100 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 13:29:19,394][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:29:19,397][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:29:19,398][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:29:19,399][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:29:19,402][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:29:19,403][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:29:19,404][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:29:19,404][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:29:19,406][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:29:19,407][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:29:19,407][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:29:19,409][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:29:19,452][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.10it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.062 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-05-30 13:30:48,760][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/0>
[2024-05-30 13:30:48,760][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:30:48,764][HYDRA] 	#101 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 13:30:49,049][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:30:49,051][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:30:49,053][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:30:49,053][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:30:49,056][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:30:49,057][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:30:49,058][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:30:49,059][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:30:49,061][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:30:49,061][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:30:49,062][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:30:49,064][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:30:49,107][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.08it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.444 val/mre:    
                                                              0.081 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.025             
[2024-05-30 13:32:17,570][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/1>
[2024-05-30 13:32:17,571][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:32:17,575][HYDRA] 	#102 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 13:32:17,872][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:32:17,874][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:32:17,876][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:32:17,876][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:32:17,880][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:32:17,881][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:32:17,881][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:32:17,882][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:32:17,884][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:32:17,887][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:32:17,887][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:32:17,890][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:32:17,937][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.11it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.063 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 13:33:48,840][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/2>
[2024-05-30 13:33:48,841][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:33:48,844][HYDRA] 	#103 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 13:33:49,133][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:33:49,135][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:33:49,137][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:33:49,137][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:33:49,140][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:33:49,141][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:33:49,142][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:33:49,143][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:33:49,145][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:33:49,145][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:33:49,146][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:33:49,148][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:33:49,191][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.12it/s v_num: 1.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.333 val/mre:    
                                                              0.076 train/auc:  
                                                              0.973 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              0.949             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-05-30 13:35:19,244][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/3>
[2024-05-30 13:35:19,245][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:35:19,248][HYDRA] 	#104 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 13:35:19,534][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:35:19,536][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:35:19,538][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:35:19,538][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:35:19,542][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:35:19,542][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:35:19,543][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:35:19,544][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:35:19,546][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:35:19,547][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:35:19,547][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:35:19,549][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:35:19,593][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.333 val/mre:    
                                                              0.061 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 13:36:48,730][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/4>
[2024-05-30 13:36:48,731][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:36:48,734][HYDRA] 	#105 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 13:36:49,024][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:36:49,026][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:36:49,028][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:36:49,028][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:36:49,032][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:36:49,033][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:36:49,034][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:36:49,034][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:36:49,036][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:36:49,037][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:36:49,037][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:36:49,040][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:36:49,129][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.12it/s v_num: 1.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.059 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-05-30 13:38:18,457][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/5>
[2024-05-30 13:38:18,458][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:38:18,461][HYDRA] 	#106 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 13:38:18,753][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:38:18,756][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:38:18,757][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:38:18,758][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:38:18,761][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:38:18,762][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:38:18,763][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:38:18,763][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:38:18,765][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:38:18,769][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:38:18,770][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:38:18,772][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:38:18,818][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.17it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.062 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-05-30 13:39:47,624][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/6>
[2024-05-30 13:39:47,625][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:39:47,628][HYDRA] 	#107 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 13:39:47,917][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:39:47,919][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:39:47,921][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:39:47,921][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:39:47,925][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:39:47,926][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:39:47,926][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:39:47,927][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:39:47,929][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:39:47,930][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:39:47,930][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:39:47,932][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:39:47,975][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.12it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.556 val/mre:    
                                                              0.062 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 13:41:17,101][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/7>
[2024-05-30 13:41:17,102][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:41:17,105][HYDRA] 	#108 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 13:41:17,395][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:41:17,398][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:41:17,399][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:41:17,400][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:41:17,403][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:41:17,404][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:41:17,405][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:41:17,406][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:41:17,407][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:41:17,408][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:41:17,409][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:41:17,411][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:41:17,455][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.12it/s v_num: 1.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.333 val/mre:    
                                                              0.070 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 13:42:47,583][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/8>
[2024-05-30 13:42:47,584][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:42:47,587][HYDRA] 	#109 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 13:42:47,868][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:42:47,871][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:42:47,872][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:42:47,873][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:42:47,876][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:42:47,877][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:42:47,878][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:42:47,878][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:42:47,880][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:42:47,881][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:42:47,881][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:42:47,883][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:42:47,966][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.09it/s v_num: 1.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre:    
                                                              0.066 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-05-30 13:44:17,204][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.15/9>
[2024-05-30 13:44:17,205][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:44:17,209][HYDRA] 	#110 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 13:44:17,497][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:44:17,499][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:44:17,501][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:44:17,501][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:44:17,505][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:44:17,505][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:44:17,506][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:44:17,507][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:44:17,509][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:44:17,510][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:44:17,510][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:44:17,513][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:44:17,557][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.278    
                                                              val/f1: 0.316     
                                                              val/precision:    
                                                              0.300 val/recall: 
                                                              0.333 val/mre:    
                                                              0.064 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 13:45:46,312][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/0>
[2024-05-30 13:45:46,313][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:45:46,317][HYDRA] 	#111 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 13:45:46,603][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:45:46,605][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:45:46,607][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:45:46,607][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:45:46,611][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:45:46,612][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:45:46,612][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:45:46,613][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:45:46,615][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:45:46,616][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:45:46,616][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:45:46,618][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:45:46,664][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre:    
                                                              0.063 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 13:47:15,351][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/1>
[2024-05-30 13:47:15,351][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:47:15,355][HYDRA] 	#112 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 13:47:15,652][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:47:15,655][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:47:15,656][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:47:15,657][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:47:15,660][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:47:15,661][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:47:15,662][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:47:15,662][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:47:15,664][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:47:15,667][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:47:15,667][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:47:15,669][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:47:15,715][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.10it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.444 val/mre:    
                                                              0.061 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 13:48:44,648][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/2>
[2024-05-30 13:48:44,649][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:48:44,652][HYDRA] 	#113 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 13:48:44,940][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:48:44,943][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:48:44,944][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:48:44,945][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:48:44,948][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:48:44,949][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:48:44,949][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:48:44,950][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:48:44,952][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:48:44,953][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:48:44,953][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:48:44,955][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:48:45,001][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.14it/s v_num: 1.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.556 val/mre:    
                                                              0.069 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-05-30 13:50:15,510][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/3>
[2024-05-30 13:50:15,511][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:50:15,515][HYDRA] 	#114 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 13:50:15,801][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:50:15,803][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:50:15,805][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:50:15,805][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:50:15,809][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:50:15,809][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:50:15,810][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:50:15,811][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:50:15,813][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:50:15,814][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:50:15,814][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:50:15,816][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:50:15,859][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.11it/s v_num: 1.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.074 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.026             
[2024-05-30 13:51:45,096][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/4>
[2024-05-30 13:51:45,097][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:51:45,100][HYDRA] 	#115 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 13:51:45,395][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:51:45,398][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:51:45,399][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:51:45,400][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:51:45,403][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:51:45,404][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:51:45,405][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:51:45,405][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:51:45,407][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:51:45,411][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:51:45,411][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:51:45,414][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:51:45,502][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.06it/s v_num: 1.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.061 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-05-30 13:53:15,426][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/5>
[2024-05-30 13:53:15,426][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:53:15,430][HYDRA] 	#116 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 13:53:15,719][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:53:15,721][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:53:15,723][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:53:15,723][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:53:15,727][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:53:15,728][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:53:15,728][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:53:15,729][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:53:15,731][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:53:15,732][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:53:15,732][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:53:15,734][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:53:15,778][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.14it/s v_num: 1.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.222 val/mre:    
                                                              0.065 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 13:54:45,219][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/6>
[2024-05-30 13:54:45,220][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:54:45,223][HYDRA] 	#117 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 13:54:45,510][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:54:45,513][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:54:45,514][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:54:45,515][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:54:45,518][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:54:45,519][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:54:45,520][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:54:45,520][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:54:45,522][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:54:45,523][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:54:45,523][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:54:45,525][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:54:45,570][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.19it/s v_num: 1.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.444 val/mre:    
                                                              0.064 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 13:56:14,737][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/7>
[2024-05-30 13:56:14,737][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:56:14,740][HYDRA] 	#118 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 13:56:15,026][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:56:15,028][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:56:15,030][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:56:15,030][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:56:15,034][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:56:15,035][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:56:15,035][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:56:15,036][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:56:15,038][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:56:15,040][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:56:15,040][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:56:15,042][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:56:15,086][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.10it/s v_num: 1.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.556 val/mre:    
                                                              0.065 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 13:57:45,453][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/8>
[2024-05-30 13:57:45,454][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:57:45,458][HYDRA] 	#119 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 13:57:45,747][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:57:45,750][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:57:45,751][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:57:45,752][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:57:45,755][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:57:45,756][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:57:45,757][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:57:45,757][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:57:45,759][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:57:45,760][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:57:45,760][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:57:45,763][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:57:45,806][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.29it/s v_num: 1.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.556 val/mre:    
                                                              0.076 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.026             
[2024-05-30 13:59:14,921][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.2/9>
[2024-05-30 13:59:14,922][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 13:59:14,925][HYDRA] 	#120 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 13:59:15,208][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 13:59:15,210][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 13:59:15,212][train.py][INFO] - Instantiating callbacks...
[2024-05-30 13:59:15,212][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 13:59:15,215][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 13:59:15,216][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 13:59:15,217][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 13:59:15,218][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 13:59:15,220][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 13:59:15,223][train.py][INFO] - Instantiating loggers...
[2024-05-30 13:59:15,223][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 13:59:15,225][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 13:59:15,313][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.15it/s v_num: 1.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre:    
                                                              0.067 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 14:00:44,090][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/0>
[2024-05-30 14:00:44,091][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:00:44,095][HYDRA] 	#121 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 14:00:44,391][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:00:44,394][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:00:44,395][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:00:44,396][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:00:44,399][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:00:44,400][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:00:44,401][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:00:44,402][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:00:44,403][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:00:44,404][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:00:44,405][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:00:44,407][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:00:44,451][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.12it/s v_num: 1.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre:    
                                                              0.066 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 14:02:13,593][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/1>
[2024-05-30 14:02:13,594][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:02:13,598][HYDRA] 	#122 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 14:02:13,890][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:02:13,892][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:02:13,894][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:02:13,894][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:02:13,898][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:02:13,899][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:02:13,900][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:02:13,900][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:02:13,902][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:02:13,904][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:02:13,904][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:02:13,906][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:02:13,959][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.09it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.444 val/mre:    
                                                              0.065 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 14:03:44,195][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/2>
[2024-05-30 14:03:44,196][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:03:44,200][HYDRA] 	#123 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 14:03:44,487][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:03:44,490][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:03:44,492][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:03:44,492][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:03:44,496][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:03:44,496][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:03:44,497][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:03:44,498][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:03:44,500][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:03:44,501][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:03:44,501][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:03:44,503][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:03:44,548][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.04it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.556 val/mre:    
                                                              0.060 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 14:05:14,325][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/3>
[2024-05-30 14:05:14,326][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:05:14,329][HYDRA] 	#124 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 14:05:14,619][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:05:14,622][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:05:14,623][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:05:14,624][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:05:14,627][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:05:14,628][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:05:14,629][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:05:14,630][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:05:14,632][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:05:14,633][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:05:14,633][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:05:14,635][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:05:14,679][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.11it/s v_num: 1.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.333 val/mre:    
                                                              0.079 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.030             
[2024-05-30 14:06:44,292][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/4>
[2024-05-30 14:06:44,292][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:06:44,296][HYDRA] 	#125 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 14:06:44,587][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:06:44,590][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:06:44,592][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:06:44,592][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:06:44,596][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:06:44,596][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:06:44,597][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:06:44,598][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:06:44,600][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:06:44,601][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:06:44,601][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:06:44,603][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:06:44,647][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.21it/s v_num: 1.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.060 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 14:08:13,924][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/5>
[2024-05-30 14:08:13,925][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:08:13,928][HYDRA] 	#126 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 14:08:14,218][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:08:14,220][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:08:14,222][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:08:14,222][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:08:14,226][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:08:14,227][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:08:14,228][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:08:14,228][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:08:14,230][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:08:14,231][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:08:14,231][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:08:14,234][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:08:14,276][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.08it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.065 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 14:09:44,203][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/6>
[2024-05-30 14:09:44,209][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:09:44,220][HYDRA] 	#127 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 14:09:44,708][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:09:44,713][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:09:44,715][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:09:44,715][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:09:44,719][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:09:44,719][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:09:44,720][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:09:44,721][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:09:44,723][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:09:44,724][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:09:44,724][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:09:44,726][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:09:44,867][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.12it/s v_num: 1.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre:    
                                                              0.063 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-05-30 14:11:16,474][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/7>
[2024-05-30 14:11:16,475][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:11:16,479][HYDRA] 	#128 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 14:11:16,769][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:11:16,772][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:11:16,773][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:11:16,774][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:11:16,777][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:11:16,778][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:11:16,779][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:11:16,779][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:11:16,781][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:11:16,782][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:11:16,782][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:11:16,784][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:11:16,829][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.13it/s v_num: 1.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.071 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 14:12:47,110][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/8>
[2024-05-30 14:12:47,111][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:12:47,115][HYDRA] 	#129 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 14:12:47,405][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:12:47,407][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:12:47,409][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:12:47,409][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:12:47,413][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:12:47,414][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:12:47,414][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:12:47,415][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:12:47,417][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:12:47,418][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:12:47,418][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:12:47,420][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:12:47,463][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.31it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.556 val/mre:    
                                                              0.075 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.026             
[2024-05-30 14:14:13,445][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.25/9>
[2024-05-30 14:14:13,446][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:14:13,450][HYDRA] 	#130 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 14:14:13,729][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:14:13,732][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:14:13,734][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:14:13,734][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:14:13,737][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:14:13,738][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:14:13,739][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:14:13,740][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:14:13,741][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:14:13,742][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:14:13,742][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:14:13,745][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:14:13,787][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.33it/s v_num: 1.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.556 val/mre:    
                                                              0.070 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-05-30 14:15:39,868][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/0>
[2024-05-30 14:15:39,868][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:15:39,871][HYDRA] 	#131 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 14:15:40,145][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:15:40,147][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:15:40,148][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:15:40,149][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:15:40,152][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:15:40,153][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:15:40,154][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:15:40,155][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:15:40,156][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:15:40,157][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:15:40,157][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:15:40,159][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:15:40,200][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre:    
                                                              0.067 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 14:17:06,946][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/1>
[2024-05-30 14:17:06,946][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:17:06,950][HYDRA] 	#132 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 14:17:07,249][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:17:07,252][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:17:07,253][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:17:07,254][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:17:07,258][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:17:07,258][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:17:07,259][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:17:07,260][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:17:07,262][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:17:07,266][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:17:07,267][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:17:07,269][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:17:07,387][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.14it/s v_num: 1.000      
                                                              val/auc: 0.333    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.222 val/mre:    
                                                              0.070 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 14:18:36,661][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/2>
[2024-05-30 14:18:36,663][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:18:36,667][HYDRA] 	#133 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 14:18:37,038][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:18:37,041][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:18:37,043][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:18:37,043][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:18:37,048][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:18:37,048][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:18:37,049][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:18:37,050][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:18:37,052][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:18:37,055][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:18:37,056][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:18:37,058][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:18:37,150][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.13it/s v_num: 1.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.444 val/mre:    
                                                              0.068 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 14:20:06,337][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/3>
[2024-05-30 14:20:06,337][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:20:06,341][HYDRA] 	#134 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 14:20:06,641][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:20:06,644][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:20:06,646][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:20:06,646][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:20:06,649][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:20:06,650][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:20:06,651][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:20:06,652][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:20:06,654][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:20:06,672][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:20:06,672][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:20:06,677][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:20:06,772][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.32it/s v_num: 1.000      
                                                              val/auc: 0.333    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.333 val/mre:    
                                                              0.066 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-05-30 14:21:33,864][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/4>
[2024-05-30 14:21:33,865][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:21:33,868][HYDRA] 	#135 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 14:21:34,163][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:21:34,165][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:21:34,167][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:21:34,167][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:21:34,171][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:21:34,171][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:21:34,172][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:21:34,173][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:21:34,175][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:21:34,178][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:21:34,178][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:21:34,180][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:21:34,271][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.27it/s v_num: 1.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 val/mre:    
                                                              0.063 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 14:23:00,806][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/5>
[2024-05-30 14:23:00,807][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:23:00,810][HYDRA] 	#136 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 14:23:01,107][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:23:01,110][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:23:01,111][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:23:01,112][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:23:01,115][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:23:01,116][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:23:01,117][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:23:01,117][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:23:01,119][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:23:01,122][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:23:01,123][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:23:01,125][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:23:01,212][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.22it/s v_num: 1.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.222 val/mre:    
                                                              0.068 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 14:24:29,162][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/6>
[2024-05-30 14:24:29,163][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:24:29,166][HYDRA] 	#137 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 14:24:29,457][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:24:29,460][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:24:29,461][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:24:29,462][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:24:29,465][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:24:29,466][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:24:29,467][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:24:29,468][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:24:29,469][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:24:29,474][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:24:29,475][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:24:29,477][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:24:29,587][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.29it/s v_num: 1.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.556 val/mre:    
                                                              0.072 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-05-30 14:25:57,677][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/7>
[2024-05-30 14:25:57,677][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:25:57,681][HYDRA] 	#138 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 14:25:57,961][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:25:57,963][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:25:57,965][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:25:57,965][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:25:57,969][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:25:57,970][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:25:57,970][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:25:57,971][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:25:57,973][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:25:57,974][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:25:57,974][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:25:57,976][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:25:58,018][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.23it/s v_num: 1.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre:    
                                                              0.064 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 14:27:26,076][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/8>
[2024-05-30 14:27:26,077][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:27:26,080][HYDRA] 	#139 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 14:27:26,357][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:27:26,360][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:27:26,361][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:27:26,362][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:27:26,365][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:27:26,366][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:27:26,366][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:27:26,367][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:27:26,369][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:27:26,370][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:27:26,370][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:27:26,372][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:27:26,414][train.py][INFO] - Starting training...
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.23it/s v_num: 1.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.067 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 14:28:53,531][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/anything_regret/0.3/9>
[2024-05-30 14:28:53,532][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:28:53,534][HYDRA] 	#140 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 14:28:53,828][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:28:53,831][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:28:53,832][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:28:53,833][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:28:53,836][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:28:53,837][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:28:53,837][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:28:53,838][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:28:53,840][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:28:53,843][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:28:53,843][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:28:53,846][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:28:53,938][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.13it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.024             
[2024-05-30 14:30:23,964][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/0>
[2024-05-30 14:30:23,965][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:30:23,968][HYDRA] 	#141 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 14:30:24,260][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:30:24,262][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:30:24,264][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:30:24,264][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:30:24,268][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:30:24,268][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:30:24,269][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:30:24,270][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:30:24,272][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:30:24,274][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:30:24,274][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:30:24,276][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:30:24,361][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.13it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-05-30 14:31:55,694][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/1>
[2024-05-30 14:31:55,705][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:31:55,722][HYDRA] 	#142 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 14:31:56,208][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:31:56,211][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:31:56,213][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:31:56,213][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:31:56,217][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:31:56,218][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:31:56,219][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:31:56,219][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:31:56,221][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:31:56,224][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:31:56,224][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:31:56,227][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:31:56,371][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.02it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 14:33:30,191][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/2>
[2024-05-30 14:33:30,191][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:33:30,195][HYDRA] 	#143 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 14:33:30,496][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:33:30,498][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:33:30,500][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:33:30,500][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:33:30,504][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:33:30,505][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:33:30,506][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:33:30,506][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:33:30,508][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:33:30,511][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:33:30,512][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:33:30,514][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:33:30,612][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.13it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 14:35:03,111][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/3>
[2024-05-30 14:35:03,111][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:35:03,115][HYDRA] 	#144 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 14:35:03,412][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:35:03,415][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:35:03,417][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:35:03,418][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:35:03,422][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:35:03,422][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:35:03,423][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:35:03,424][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:35:03,426][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:35:03,429][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:35:03,429][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:35:03,431][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:35:03,517][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.04it/s v_num: 1.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.833 val/recall: 
                                                              0.500 val/mre: nan
                                                              train/auc: 0.992  
                                                              train/f1: 0.992   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.016             
[2024-05-30 14:36:35,923][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/4>
[2024-05-30 14:36:35,924][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:36:35,927][HYDRA] 	#145 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 14:36:36,226][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:36:36,228][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:36:36,230][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:36:36,230][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:36:36,234][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:36:36,235][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:36:36,236][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:36:36,236][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:36:36,238][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:36:36,241][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:36:36,242][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:36:36,244][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:36:36,330][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.02it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 14:38:09,159][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/5>
[2024-05-30 14:38:09,160][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:38:09,163][HYDRA] 	#146 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 14:38:09,466][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:38:09,469][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:38:09,470][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:38:09,471][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:38:09,475][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:38:09,475][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:38:09,476][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:38:09,477][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:38:09,479][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:38:09,482][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:38:09,483][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:38:09,485][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:38:09,571][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.04it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-05-30 14:39:41,860][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/6>
[2024-05-30 14:39:41,861][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:39:41,864][HYDRA] 	#147 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 14:39:42,146][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:39:42,148][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:39:42,150][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:39:42,150][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:39:42,154][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:39:42,154][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:39:42,155][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:39:42,156][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:39:42,158][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:39:42,158][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:39:42,159][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:39:42,161][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:39:42,204][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.03it/s v_num: 1.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre: nan
                                                              train/auc: 0.992  
                                                              train/f1: 0.992   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.029             
[2024-05-30 14:41:14,386][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/7>
[2024-05-30 14:41:14,387][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:41:14,390][HYDRA] 	#148 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 14:41:14,677][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:41:14,680][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:41:14,681][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:41:14,682][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:41:14,685][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:41:14,686][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:41:14,687][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:41:14,688][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:41:14,690][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:41:14,690][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:41:14,691][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:41:14,693][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:41:14,736][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.17it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 14:42:44,840][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/8>
[2024-05-30 14:42:44,840][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:42:44,844][HYDRA] 	#149 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 14:42:45,120][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:42:45,122][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:42:45,124][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:42:45,124][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:42:45,128][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:42:45,128][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:42:45,129][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:42:45,130][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:42:45,132][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:42:45,133][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:42:45,133][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:42:45,135][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:42:45,176][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.09it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-05-30 14:44:15,676][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.0/9>
[2024-05-30 14:44:15,677][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:44:15,680][HYDRA] 	#150 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 14:44:15,982][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:44:15,985][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:44:15,986][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:44:15,987][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:44:15,991][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:44:15,992][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:44:15,992][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:44:15,993][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:44:15,995][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:44:15,998][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:44:15,999][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:44:16,001][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:44:16,089][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.15it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.074 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-05-30 14:45:46,478][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/0>
[2024-05-30 14:45:46,479][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:45:46,482][HYDRA] 	#151 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 14:45:46,779][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:45:46,782][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:45:46,784][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:45:46,784][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:45:46,788][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:45:46,788][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:45:46,789][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:45:46,790][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:45:46,792][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:45:46,795][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:45:46,795][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:45:46,798][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:45:46,882][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.13it/s v_num: 1.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.078 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-05-30 14:47:20,036][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/1>
[2024-05-30 14:47:20,036][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:47:20,040][HYDRA] 	#152 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 14:47:20,336][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:47:20,339][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:47:20,341][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:47:20,341][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:47:20,345][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:47:20,346][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:47:20,347][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:47:20,347][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:47:20,349][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:47:20,352][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:47:20,353][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:47:20,355][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:47:20,444][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.12it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.071 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 14:48:52,490][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/2>
[2024-05-30 14:48:52,491][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:48:52,516][HYDRA] 	#153 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 14:48:52,808][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:48:52,810][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:48:52,812][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:48:52,812][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:48:52,815][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:48:52,816][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:48:52,817][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:48:52,818][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:48:52,819][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:48:52,823][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:48:52,823][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:48:52,825][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:48:52,910][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.15it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.076 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-05-30 14:50:27,199][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/3>
[2024-05-30 14:50:27,200][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:50:27,203][HYDRA] 	#154 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 14:50:27,584][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:50:27,586][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:50:27,588][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:50:27,588][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:50:27,591][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:50:27,592][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:50:27,593][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:50:27,594][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:50:27,596][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:50:27,599][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:50:27,599][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:50:27,601][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:50:27,746][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.066 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 14:51:57,831][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/4>
[2024-05-30 14:51:57,832][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:51:57,836][HYDRA] 	#155 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 14:51:58,135][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:51:58,138][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:51:58,140][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:51:58,141][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:51:58,145][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:51:58,146][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:51:58,146][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:51:58,147][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:51:58,149][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:51:58,152][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:51:58,153][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:51:58,155][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:51:58,240][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.15it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.069 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 14:53:28,528][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/5>
[2024-05-30 14:53:28,529][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:53:28,532][HYDRA] 	#156 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 14:53:28,818][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:53:28,820][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:53:28,822][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:53:28,822][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:53:28,825][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:53:28,826][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:53:28,827][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:53:28,828][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:53:28,829][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:53:28,832][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:53:28,833][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:53:28,835][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:53:28,926][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.11it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.080 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-05-30 14:54:59,966][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/6>
[2024-05-30 14:54:59,967][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:54:59,971][HYDRA] 	#157 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 14:55:00,279][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:55:00,281][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:55:00,283][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:55:00,283][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:55:00,287][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:55:00,288][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:55:00,289][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:55:00,290][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:55:00,292][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:55:00,293][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:55:00,294][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:55:00,296][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:55:00,346][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.98it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.080 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-05-30 14:56:33,199][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/7>
[2024-05-30 14:56:33,200][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:56:33,203][HYDRA] 	#158 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 14:56:33,497][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:56:33,499][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:56:33,501][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:56:33,502][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:56:33,505][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:56:33,506][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:56:33,507][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:56:33,508][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:56:33,509][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:56:33,510][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:56:33,511][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:56:33,513][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:56:33,559][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.10it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.078 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-05-30 14:58:06,280][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/8>
[2024-05-30 14:58:06,281][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:58:06,284][HYDRA] 	#159 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 14:58:06,577][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:58:06,579][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:58:06,581][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:58:06,581][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:58:06,584][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:58:06,585][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:58:06,586][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:58:06,587][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:58:06,589][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:58:06,590][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:58:06,590][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:58:06,592][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:58:06,637][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.08it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.076 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-05-30 14:59:39,187][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.05/9>
[2024-05-30 14:59:39,188][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 14:59:39,191][HYDRA] 	#160 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 14:59:39,499][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 14:59:39,501][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 14:59:39,503][train.py][INFO] - Instantiating callbacks...
[2024-05-30 14:59:39,503][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 14:59:39,507][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 14:59:39,508][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 14:59:39,508][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 14:59:39,509][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 14:59:39,511][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 14:59:39,514][train.py][INFO] - Instantiating loggers...
[2024-05-30 14:59:39,515][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 14:59:39,517][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 14:59:39,608][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.05it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.081 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-05-30 15:01:12,135][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/0>
[2024-05-30 15:01:12,136][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:01:12,139][HYDRA] 	#161 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 15:01:12,445][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:01:12,448][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:01:12,449][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:01:12,450][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:01:12,453][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:01:12,454][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:01:12,455][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:01:12,456][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:01:12,457][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:01:12,461][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:01:12,462][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:01:12,464][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:01:12,561][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.01it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.090 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-05-30 15:02:45,657][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/1>
[2024-05-30 15:02:45,658][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:02:45,661][HYDRA] 	#162 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 15:02:45,970][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:02:45,972][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:02:45,974][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:02:45,974][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:02:45,978][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:02:45,979][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:02:45,979][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:02:45,980][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:02:45,982][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:02:45,986][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:02:45,986][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:02:45,988][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:02:46,085][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.14it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.085 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 15:04:19,050][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/2>
[2024-05-30 15:04:19,051][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:04:19,057][HYDRA] 	#163 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 15:04:19,368][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:04:19,371][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:04:19,373][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:04:19,373][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:04:19,378][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:04:19,378][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:04:19,379][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:04:19,380][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:04:19,382][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:04:19,385][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:04:19,385][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:04:19,387][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:04:19,485][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.091 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-05-30 15:05:52,037][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/3>
[2024-05-30 15:05:52,037][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:05:52,041][HYDRA] 	#164 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 15:05:52,333][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:05:52,336][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:05:52,338][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:05:52,339][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:05:52,342][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:05:52,343][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:05:52,344][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:05:52,345][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:05:52,347][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:05:52,350][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:05:52,350][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:05:52,352][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:05:52,438][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.01it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.083 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 15:07:24,948][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/4>
[2024-05-30 15:07:24,949][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:07:24,953][HYDRA] 	#165 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 15:07:25,255][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:07:25,258][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:07:25,260][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:07:25,261][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:07:25,265][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:07:25,266][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:07:25,267][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:07:25,268][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:07:25,269][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:07:25,273][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:07:25,273][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:07:25,276][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:07:25,370][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.98it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.100 val/mre:    
                                                              0.082 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 15:08:58,136][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/5>
[2024-05-30 15:08:58,136][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:08:58,139][HYDRA] 	#166 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 15:08:58,441][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:08:58,444][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:08:58,445][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:08:58,446][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:08:58,449][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:08:58,450][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:08:58,451][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:08:58,452][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:08:58,454][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:08:58,457][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:08:58,457][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:08:58,459][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:08:58,554][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.01it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.089 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-05-30 15:10:31,096][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/6>
[2024-05-30 15:10:31,099][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:10:31,118][HYDRA] 	#167 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 15:10:31,506][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:10:31,508][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:10:31,510][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:10:31,510][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:10:31,514][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:10:31,515][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:10:31,515][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:10:31,516][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:10:31,518][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:10:31,519][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:10:31,519][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:10:31,521][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:10:31,565][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.09it/s v_num: 1.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.100 val/mre:    
                                                              0.092 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-05-30 15:12:03,538][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/7>
[2024-05-30 15:12:03,539][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:12:03,542][HYDRA] 	#168 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 15:12:04,105][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:12:04,108][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:12:04,109][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:12:04,110][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:12:04,113][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:12:04,114][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:12:04,114][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:12:04,115][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:12:04,117][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:12:04,118][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:12:04,118][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:12:04,120][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:12:04,161][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.17it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.081 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-05-30 15:13:35,560][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/8>
[2024-05-30 15:13:35,561][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:13:35,564][HYDRA] 	#169 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 15:13:35,856][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:13:35,859][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:13:35,860][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:13:35,861][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:13:35,864][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:13:35,865][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:13:35,866][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:13:35,866][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:13:35,868][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:13:35,870][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:13:35,870][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:13:35,872][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:13:35,958][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.20it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.078 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 15:15:05,046][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.1/9>
[2024-05-30 15:15:05,047][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:15:05,050][HYDRA] 	#170 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 15:15:05,352][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:15:05,354][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:15:05,356][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:15:05,356][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:15:05,359][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:15:05,360][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:15:05,361][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:15:05,362][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:15:05,364][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:15:05,367][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:15:05,367][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:15:05,369][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:15:05,455][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.18it/s v_num: 1.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.081 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-05-30 15:16:35,036][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/0>
[2024-05-30 15:16:35,038][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:16:35,042][HYDRA] 	#171 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 15:16:35,337][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:16:35,339][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:16:35,341][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:16:35,341][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:16:35,345][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:16:35,345][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:16:35,346][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:16:35,347][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:16:35,349][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:16:35,352][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:16:35,353][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:16:35,355][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:16:35,445][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.18it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.090 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-05-30 15:18:04,978][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/1>
[2024-05-30 15:18:04,979][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:18:04,982][HYDRA] 	#172 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 15:18:05,292][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:18:05,296][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:18:05,298][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:18:05,299][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:18:05,302][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:18:05,303][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:18:05,304][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:18:05,305][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:18:05,307][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:18:05,310][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:18:05,310][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:18:05,312][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:18:05,482][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.17it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.084 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 15:19:35,094][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/2>
[2024-05-30 15:19:35,094][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:19:35,097][HYDRA] 	#173 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 15:19:35,386][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:19:35,389][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:19:35,391][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:19:35,392][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:19:35,396][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:19:35,396][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:19:35,397][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:19:35,398][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:19:35,400][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:19:35,403][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:19:35,403][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:19:35,405][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:19:35,501][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.13it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.092 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-05-30 15:21:05,122][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/3>
[2024-05-30 15:21:05,122][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:21:05,126][HYDRA] 	#174 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 15:21:05,424][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:21:05,426][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:21:05,428][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:21:05,428][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:21:05,432][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:21:05,432][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:21:05,433][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:21:05,434][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:21:05,436][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:21:05,438][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:21:05,438][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:21:05,441][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:21:05,540][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.18it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.086 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 15:22:35,641][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/4>
[2024-05-30 15:22:35,642][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:22:35,645][HYDRA] 	#175 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 15:22:35,925][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:22:35,927][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:22:35,929][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:22:35,929][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:22:35,933][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:22:35,933][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:22:35,934][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:22:35,935][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:22:35,937][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:22:35,937][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:22:35,938][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:22:35,940][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:22:35,990][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.17it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.082 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 15:24:05,807][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/5>
[2024-05-30 15:24:05,808][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:24:05,811][HYDRA] 	#176 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 15:24:06,106][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:24:06,108][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:24:06,110][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:24:06,110][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:24:06,113][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:24:06,114][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:24:06,115][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:24:06,116][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:24:06,117][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:24:06,120][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:24:06,121][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:24:06,123][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:24:06,211][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.18it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.087 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 15:25:36,379][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/6>
[2024-05-30 15:25:36,380][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:25:36,383][HYDRA] 	#177 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 15:25:36,679][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:25:36,682][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:25:36,684][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:25:36,685][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:25:36,689][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:25:36,690][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:25:36,690][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:25:36,691][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:25:36,693][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:25:36,697][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:25:36,697][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:25:36,699][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:25:36,789][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.14it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.090 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 15:27:07,024][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/7>
[2024-05-30 15:27:07,025][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:27:07,027][HYDRA] 	#178 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 15:27:07,320][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:27:07,322][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:27:07,324][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:27:07,324][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:27:07,328][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:27:07,328][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:27:07,329][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:27:07,330][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:27:07,332][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:27:07,335][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:27:07,336][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:27:07,338][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:27:07,424][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.19it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.104 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-05-30 15:28:36,945][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/8>
[2024-05-30 15:28:36,946][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:28:36,949][HYDRA] 	#179 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 15:28:37,243][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:28:37,246][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:28:37,248][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:28:37,249][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:28:37,253][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:28:37,253][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:28:37,254][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:28:37,255][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:28:37,257][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:28:37,260][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:28:37,261][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:28:37,263][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:28:37,352][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.19it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.089 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 15:30:07,921][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.15/9>
[2024-05-30 15:30:07,922][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:30:07,925][HYDRA] 	#180 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 15:30:08,209][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:30:08,212][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:30:08,213][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:30:08,214][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:30:08,217][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:30:08,218][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:30:08,219][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:30:08,219][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:30:08,221][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:30:08,225][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:30:08,225][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:30:08,227][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:30:08,313][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.084 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 15:31:39,156][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/0>
[2024-05-30 15:31:39,157][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:31:39,160][HYDRA] 	#181 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 15:31:39,445][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:31:39,447][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:31:39,449][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:31:39,449][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:31:39,452][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:31:39,453][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:31:39,454][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:31:39,455][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:31:39,456][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:31:39,457][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:31:39,458][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:31:39,460][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:31:39,552][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.18it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.092 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-05-30 15:33:09,020][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/1>
[2024-05-30 15:33:09,020][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:33:09,024][HYDRA] 	#182 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 15:33:09,304][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:33:09,307][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:33:09,308][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:33:09,309][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:33:09,312][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:33:09,313][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:33:09,314][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:33:09,314][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:33:09,316][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:33:09,317][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:33:09,317][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:33:09,319][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:33:09,365][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.17it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.090 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 15:34:40,916][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/2>
[2024-05-30 15:34:40,917][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:34:40,920][HYDRA] 	#183 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 15:34:41,200][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:34:41,202][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:34:41,204][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:34:41,204][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:34:41,208][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:34:41,208][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:34:41,209][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:34:41,210][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:34:41,212][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:34:41,212][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:34:41,213][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:34:41,215][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:34:41,257][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.11it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.583     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.700 val/mre:    
                                                              0.107 train/auc:  
                                                              0.984 train/f1:   
                                                              0.984             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.015             
[2024-05-30 15:36:11,873][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/3>
[2024-05-30 15:36:11,874][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:36:11,878][HYDRA] 	#184 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 15:36:12,168][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:36:12,170][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:36:12,172][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:36:12,172][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:36:12,176][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:36:12,176][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:36:12,177][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:36:12,178][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:36:12,180][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:36:12,183][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:36:12,183][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:36:12,185][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:36:12,272][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.102 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 15:37:41,656][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/4>
[2024-05-30 15:37:41,657][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:37:41,660][HYDRA] 	#185 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 15:37:41,954][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:37:41,956][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:37:41,958][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:37:41,958][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:37:41,961][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:37:41,962][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:37:41,963][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:37:41,964][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:37:41,965][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:37:41,972][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:37:41,972][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:37:41,974][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:37:42,064][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.17it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.086 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 15:39:12,030][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/5>
[2024-05-30 15:39:12,031][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:39:12,034][HYDRA] 	#186 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 15:39:12,326][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:39:12,329][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:39:12,331][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:39:12,331][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:39:12,335][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:39:12,336][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:39:12,337][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:39:12,338][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:39:12,339][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:39:12,343][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:39:12,343][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:39:12,345][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:39:12,434][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.086 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-05-30 15:40:42,490][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/6>
[2024-05-30 15:40:42,491][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:40:42,494][HYDRA] 	#187 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 15:40:42,785][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:40:42,787][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:40:42,789][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:40:42,789][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:40:42,792][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:40:42,793][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:40:42,794][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:40:42,795][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:40:42,797][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:40:42,800][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:40:42,800][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:40:42,802][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:40:42,886][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.096 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 15:42:13,242][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/7>
[2024-05-30 15:42:13,242][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:42:13,247][HYDRA] 	#188 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 15:42:13,576][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:42:13,579][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:42:13,580][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:42:13,581][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:42:13,584][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:42:13,585][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:42:13,586][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:42:13,587][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:42:13,588][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:42:13,609][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:42:13,609][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:42:13,614][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:42:13,751][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.19it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.093 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 15:43:44,469][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/8>
[2024-05-30 15:43:44,469][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:43:44,474][HYDRA] 	#189 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 15:43:44,758][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:43:44,760][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:43:44,762][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:43:44,762][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:43:44,766][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:43:44,766][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:43:44,767][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:43:44,768][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:43:44,770][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:43:44,771][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:43:44,771][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:43:44,773][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:43:44,815][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.23it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.098 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-05-30 15:45:14,570][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.2/9>
[2024-05-30 15:45:14,571][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:45:14,574][HYDRA] 	#190 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 15:45:15,145][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:45:15,147][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:45:15,148][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:45:15,149][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:45:15,152][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:45:15,153][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:45:15,154][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:45:15,154][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:45:15,156][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:45:15,157][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:45:15,157][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:45:15,159][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:45:15,201][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.19it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.092 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-05-30 15:46:45,392][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/0>
[2024-05-30 15:46:45,392][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:46:45,395][HYDRA] 	#191 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 15:46:45,689][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:46:45,691][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:46:45,693][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:46:45,693][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:46:45,697][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:46:45,697][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:46:45,698][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:46:45,699][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:46:45,701][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:46:45,704][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:46:45,704][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:46:45,706][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:46:45,790][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.19it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.099 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-05-30 15:48:15,298][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/1>
[2024-05-30 15:48:15,299][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:48:15,303][HYDRA] 	#192 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 15:48:15,595][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:48:15,597][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:48:15,599][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:48:15,599][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:48:15,603][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:48:15,603][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:48:15,604][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:48:15,605][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:48:15,607][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:48:15,610][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:48:15,610][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:48:15,612][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:48:15,695][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.15it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.092 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 15:49:45,740][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/2>
[2024-05-30 15:49:45,741][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:49:45,744][HYDRA] 	#193 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 15:49:46,047][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:49:46,050][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:49:46,052][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:49:46,052][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:49:46,056][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:49:46,057][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:49:46,058][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:49:46,058][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:49:46,060][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:49:46,063][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:49:46,064][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:49:46,066][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:49:46,154][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.19it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.098 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 15:51:16,312][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/3>
[2024-05-30 15:51:16,313][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:51:16,316][HYDRA] 	#194 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 15:51:16,603][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:51:16,606][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:51:16,608][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:51:16,609][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:51:16,615][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:51:16,616][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:51:16,617][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:51:16,617][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:51:16,619][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:51:16,622][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:51:16,623][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:51:16,625][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:51:16,716][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.17it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.116 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 15:52:46,174][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/4>
[2024-05-30 15:52:46,175][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:52:46,178][HYDRA] 	#195 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 15:52:46,471][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:52:46,474][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:52:46,476][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:52:46,476][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:52:46,479][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:52:46,480][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:52:46,481][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:52:46,482][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:52:46,483][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:52:46,487][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:52:46,487][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:52:46,489][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:52:46,577][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.18it/s v_num: 1.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.100 val/mre:    
                                                              0.102 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 15:54:16,693][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/5>
[2024-05-30 15:54:16,694][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:54:16,698][HYDRA] 	#196 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 15:54:16,995][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:54:16,997][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:54:16,999][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:54:16,999][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:54:17,003][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:54:17,004][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:54:17,004][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:54:17,005][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:54:17,007][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:54:17,011][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:54:17,011][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:54:17,013][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:54:17,101][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.21it/s v_num: 1.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.600 val/mre:    
                                                              0.091 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 15:55:46,515][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/6>
[2024-05-30 15:55:46,516][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:55:46,519][HYDRA] 	#197 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 15:55:46,803][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:55:46,806][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:55:46,808][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:55:46,809][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:55:46,813][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:55:46,813][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:55:46,814][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:55:46,815][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:55:46,817][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:55:46,818][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:55:46,818][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:55:46,820][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:55:46,867][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.15it/s v_num: 1.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.100 val/mre:    
                                                              0.100 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-05-30 15:57:16,760][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/7>
[2024-05-30 15:57:16,761][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:57:16,763][HYDRA] 	#198 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 15:57:17,042][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:57:17,044][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:57:17,046][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:57:17,046][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:57:17,049][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:57:17,050][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:57:17,051][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:57:17,051][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:57:17,053][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:57:17,054][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:57:17,054][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:57:17,056][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:57:17,098][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.092 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-05-30 15:58:47,507][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/8>
[2024-05-30 15:58:47,507][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 15:58:47,511][HYDRA] 	#199 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 15:58:47,813][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 15:58:47,816][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 15:58:47,817][train.py][INFO] - Instantiating callbacks...
[2024-05-30 15:58:47,818][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 15:58:47,821][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 15:58:47,822][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 15:58:47,823][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 15:58:47,823][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 15:58:47,825][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 15:58:47,830][train.py][INFO] - Instantiating loggers...
[2024-05-30 15:58:47,830][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 15:58:47,832][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 15:58:47,932][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.20it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.097 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 16:00:17,445][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.25/9>
[2024-05-30 16:00:17,446][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:00:17,452][HYDRA] 	#200 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 16:00:17,891][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:00:17,894][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:00:17,896][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:00:17,896][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:00:17,901][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:00:17,902][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:00:17,902][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:00:17,903][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:00:17,905][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:00:17,908][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:00:17,908][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:00:17,910][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:00:18,081][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.18it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.091 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 16:01:48,311][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/0>
[2024-05-30 16:01:48,311][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:01:48,316][HYDRA] 	#201 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 16:01:48,613][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:01:48,616][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:01:48,617][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:01:48,618][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:01:48,621][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:01:48,622][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:01:48,623][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:01:48,623][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:01:48,625][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:01:48,629][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:01:48,629][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:01:48,631][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:01:48,717][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.17it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.099 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-05-30 16:03:18,489][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/1>
[2024-05-30 16:03:18,490][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:03:18,493][HYDRA] 	#202 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 16:03:18,800][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:03:18,804][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:03:18,806][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:03:18,807][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:03:18,810][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:03:18,811][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:03:18,812][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:03:18,813][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:03:18,814][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:03:18,818][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:03:18,818][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:03:18,820][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:03:18,914][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.12it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.091 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 16:04:49,044][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/2>
[2024-05-30 16:04:49,045][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:04:49,048][HYDRA] 	#203 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 16:04:49,344][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:04:49,347][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:04:49,349][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:04:49,350][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:04:49,353][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:04:49,354][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:04:49,354][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:04:49,355][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:04:49,357][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:04:49,360][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:04:49,361][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:04:49,363][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:04:49,457][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.18it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.101 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 16:06:19,426][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/3>
[2024-05-30 16:06:19,427][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:06:19,430][HYDRA] 	#204 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 16:06:19,713][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:06:19,715][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:06:19,717][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:06:19,717][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:06:19,721][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:06:19,721][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:06:19,722][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:06:19,723][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:06:19,725][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:06:19,726][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:06:19,726][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:06:19,728][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:06:19,770][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.22it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.095 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 16:07:49,341][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/4>
[2024-05-30 16:07:49,342][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:07:49,345][HYDRA] 	#205 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 16:07:49,623][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:07:49,625][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:07:49,627][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:07:49,627][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:07:49,631][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:07:49,631][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:07:49,632][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:07:49,633][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:07:49,635][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:07:49,636][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:07:49,636][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:07:49,638][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:07:49,683][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.20it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.094 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 16:09:21,748][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/5>
[2024-05-30 16:09:21,749][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:09:21,752][HYDRA] 	#206 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 16:09:22,045][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:09:22,048][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:09:22,050][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:09:22,051][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:09:22,055][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:09:22,056][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:09:22,057][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:09:22,057][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:09:22,059][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:09:22,062][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:09:22,063][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:09:22,065][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:09:22,158][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.18it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.105 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-05-30 16:10:52,416][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/6>
[2024-05-30 16:10:52,417][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:10:52,420][HYDRA] 	#207 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 16:10:52,716][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:10:52,718][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:10:52,720][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:10:52,720][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:10:52,724][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:10:52,725][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:10:52,725][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:10:52,726][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:10:52,728][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:10:52,731][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:10:52,731][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:10:52,733][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:10:52,821][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.15it/s v_num: 1.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.300 val/mre:    
                                                              0.098 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 16:12:22,664][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/7>
[2024-05-30 16:12:22,665][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:12:22,667][HYDRA] 	#208 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 16:12:22,972][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:12:22,974][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:12:22,976][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:12:22,976][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:12:22,980][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:12:22,980][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:12:22,982][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:12:22,982][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:12:22,984][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:12:22,987][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:12:22,987][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:12:22,989][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:12:23,076][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.17it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.106 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-05-30 16:13:53,147][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/8>
[2024-05-30 16:13:53,148][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:13:53,151][HYDRA] 	#209 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 16:13:53,441][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:13:53,444][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:13:53,445][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:13:53,446][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:13:53,449][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:13:53,450][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:13:53,450][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:13:53,451][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:13:53,453][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:13:53,456][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:13:53,456][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:13:53,458][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:13:53,547][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.20it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.102 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 16:15:22,796][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/argued_someone/0.3/9>
[2024-05-30 16:15:22,796][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:15:22,799][HYDRA] 	#210 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 16:15:23,083][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:15:23,086][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:15:23,087][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:15:23,088][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:15:23,091][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:15:23,092][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:15:23,093][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:15:23,093][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:15:23,095][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:15:23,099][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:15:23,099][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:15:23,101][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:15:23,189][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.18it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 0.992  
                                                              train/f1: 0.991   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.017             
[2024-05-30 16:16:52,199][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/0>
[2024-05-30 16:16:52,199][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:16:52,203][HYDRA] 	#211 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 16:16:52,496][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:16:52,499][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:16:52,500][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:16:52,501][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:16:52,504][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:16:52,505][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:16:52,506][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:16:52,507][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:16:52,508][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:16:52,511][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:16:52,512][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:16:52,514][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:16:52,604][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.14it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre: nan
                                                              train/auc: 0.975  
                                                              train/f1: 0.975   
                                                              train/precision:  
                                                              0.967             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.029             
[2024-05-30 16:18:21,887][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/1>
[2024-05-30 16:18:21,888][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:18:21,891][HYDRA] 	#212 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 16:18:22,174][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:18:22,177][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:18:22,179][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:18:22,179][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:18:22,183][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:18:22,184][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:18:22,185][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:18:22,186][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:18:22,187][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:18:22,188][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:18:22,189][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:18:22,191][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:18:22,236][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.13it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.992  
                                                              train/f1: 0.991   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.021             
[2024-05-30 16:19:52,284][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/2>
[2024-05-30 16:19:52,285][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:19:52,289][HYDRA] 	#213 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 16:19:52,566][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:19:52,569][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:19:52,570][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:19:52,571][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:19:52,574][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:19:52,575][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:19:52,575][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:19:52,576][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:19:52,578][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:19:52,579][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:19:52,579][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:19:52,581][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:19:52,623][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.14it/s v_num: 1.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 16:21:22,555][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/3>
[2024-05-30 16:21:22,556][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:21:22,559][HYDRA] 	#214 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 16:21:22,866][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:21:22,869][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:21:22,871][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:21:22,872][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:21:22,875][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:21:22,876][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:21:22,877][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:21:22,877][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:21:22,879][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:21:22,883][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:21:22,883][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:21:22,885][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:21:22,982][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.18it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.992  
                                                              train/f1: 0.991   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.016             
[2024-05-30 16:22:53,122][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/4>
[2024-05-30 16:22:53,123][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:22:53,126][HYDRA] 	#215 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 16:22:53,421][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:22:53,423][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:22:53,425][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:22:53,425][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:22:53,428][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:22:53,429][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:22:53,430][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:22:53,431][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:22:53,432][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:22:53,436][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:22:53,436][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:22:53,438][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:22:53,527][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.01it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.983  
                                                              train/f1: 0.983   
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.019             
[2024-05-30 16:24:25,499][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/5>
[2024-05-30 16:24:25,500][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:24:25,503][HYDRA] 	#216 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 16:24:25,800][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:24:25,802][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:24:25,804][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:24:25,804][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:24:25,807][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:24:25,808][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:24:25,809][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:24:25,810][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:24:25,812][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:24:25,814][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:24:25,815][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:24:25,817][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:24:25,939][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.15it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 0.992  
                                                              train/f1: 0.992   
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.025             
[2024-05-30 16:25:55,246][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/6>
[2024-05-30 16:25:55,247][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:25:55,251][HYDRA] 	#217 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 16:25:55,547][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:25:55,551][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:25:55,552][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:25:55,553][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:25:55,556][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:25:55,557][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:25:55,558][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:25:55,559][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:25:55,560][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:25:55,563][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:25:55,564][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:25:55,566][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:25:55,652][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.14it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.975  
                                                              train/f1: 0.975   
                                                              train/precision:  
                                                              0.967             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.026             
[2024-05-30 16:27:24,955][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/7>
[2024-05-30 16:27:24,956][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:27:24,959][HYDRA] 	#218 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 16:27:25,247][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:27:25,250][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:27:25,252][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:27:25,252][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:27:25,256][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:27:25,257][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:27:25,258][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:27:25,259][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:27:25,260][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:27:25,266][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:27:25,267][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:27:25,269][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:27:25,366][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.06it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-05-30 16:28:57,255][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/8>
[2024-05-30 16:28:57,256][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:28:57,259][HYDRA] 	#219 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 16:28:57,545][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:28:57,547][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:28:57,549][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:28:57,549][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:28:57,552][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:28:57,553][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:28:57,554][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:28:57,555][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:28:57,556][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:28:57,557][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:28:57,558][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:28:57,560][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:28:57,603][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.14it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-05-30 16:30:27,200][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.0/9>
[2024-05-30 16:30:27,201][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:30:27,205][HYDRA] 	#220 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 16:30:27,484][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:30:27,487][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:30:27,488][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:30:27,489][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:30:27,492][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:30:27,493][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:30:27,493][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:30:27,494][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:30:27,496][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:30:27,497][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:30:27,497][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:30:27,499][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:30:27,542][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.059 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-05-30 16:31:56,647][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/0>
[2024-05-30 16:31:56,648][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:31:56,650][HYDRA] 	#221 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 16:31:56,957][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:31:56,959][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:31:56,961][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:31:56,961][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:31:56,964][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:31:56,965][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:31:56,966][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:31:56,967][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:31:56,969][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:31:56,972][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:31:56,972][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:31:56,975][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:31:57,076][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.14it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.055 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-05-30 16:33:26,276][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/1>
[2024-05-30 16:33:26,277][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:33:26,280][HYDRA] 	#222 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 16:33:26,572][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:33:26,574][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:33:26,576][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:33:26,576][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:33:26,579][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:33:26,580][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:33:26,581][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:33:26,582][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:33:26,583][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:33:26,587][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:33:26,587][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:33:26,589][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:33:26,673][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.07it/s v_num: 1.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.056 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-05-30 16:34:56,915][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/2>
[2024-05-30 16:34:56,919][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:34:56,929][HYDRA] 	#223 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 16:34:57,376][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:34:57,379][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:34:57,380][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:34:57,381][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:34:57,384][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:34:57,385][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:34:57,386][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:34:57,387][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:34:57,388][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:34:57,391][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:34:57,392][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:34:57,394][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:34:57,619][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.12it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.100 val/mre:    
                                                              0.061 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-05-30 16:36:28,047][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/3>
[2024-05-30 16:36:28,047][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:36:28,050][HYDRA] 	#224 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 16:36:28,349][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:36:28,351][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:36:28,353][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:36:28,353][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:36:28,356][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:36:28,357][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:36:28,358][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:36:28,359][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:36:28,360][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:36:28,364][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:36:28,364][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:36:28,366][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:36:28,451][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.17it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.055 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-05-30 16:37:57,784][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/4>
[2024-05-30 16:37:57,785][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:37:57,788][HYDRA] 	#225 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 16:37:58,076][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:37:58,079][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:37:58,080][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:37:58,081][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:37:58,084][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:37:58,085][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:37:58,086][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:37:58,086][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:37:58,088][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:37:58,091][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:37:58,092][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:37:58,094][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:37:58,208][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.14it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.061 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.022             
[2024-05-30 16:39:27,520][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/5>
[2024-05-30 16:39:27,521][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:39:27,524][HYDRA] 	#226 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 16:39:27,803][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:39:27,805][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:39:27,807][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:39:27,807][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:39:27,811][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:39:27,812][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:39:27,812][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:39:27,813][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:39:27,815][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:39:27,816][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:39:27,816][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:39:27,818][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:39:27,860][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.21it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.100 val/mre:    
                                                              0.073 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-05-30 16:40:56,836][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/6>
[2024-05-30 16:40:56,836][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:40:56,840][HYDRA] 	#227 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 16:40:57,120][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:40:57,123][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:40:57,124][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:40:57,125][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:40:57,128][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:40:57,129][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:40:57,130][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:40:57,130][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:40:57,132][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:40:57,133][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:40:57,133][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:40:57,135][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:40:57,178][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.14it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre:    
                                                              0.068 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.029             
[2024-05-30 16:42:26,616][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/7>
[2024-05-30 16:42:26,616][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:42:26,620][HYDRA] 	#228 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 16:42:26,913][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:42:26,916][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:42:26,917][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:42:26,918][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:42:26,921][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:42:26,922][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:42:26,923][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:42:26,923][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:42:26,925][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:42:26,929][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:42:26,929][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:42:26,931][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:42:27,033][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.11it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre:    
                                                              0.065 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.028             
[2024-05-30 16:43:57,234][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/8>
[2024-05-30 16:43:57,235][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:43:57,238][HYDRA] 	#229 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 16:43:57,532][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:43:57,535][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:43:57,536][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:43:57,537][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:43:57,540][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:43:57,541][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:43:57,541][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:43:57,542][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:43:57,544][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:43:57,547][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:43:57,547][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:43:57,549][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:43:57,644][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.19it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.054 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-05-30 16:45:26,121][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.05/9>
[2024-05-30 16:45:26,122][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:45:26,125][HYDRA] 	#230 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 16:45:26,416][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:45:26,419][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:45:26,420][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:45:26,421][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:45:26,424][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:45:26,425][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:45:26,426][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:45:26,426][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:45:26,428][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:45:26,431][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:45:26,432][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:45:26,434][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:45:26,522][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.19it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.100 val/mre:    
                                                              0.060 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 16:46:55,315][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/0>
[2024-05-30 16:46:55,316][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:46:55,319][HYDRA] 	#231 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 16:46:55,607][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:46:55,609][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:46:55,611][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:46:55,611][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:46:55,615][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:46:55,616][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:46:55,616][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:46:55,617][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:46:55,619][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:46:55,623][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:46:55,624][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:46:55,626][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:46:55,717][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.11it/s v_num: 1.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.050 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 16:48:26,311][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/1>
[2024-05-30 16:48:26,312][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:48:26,315][HYDRA] 	#232 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 16:48:26,712][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:48:26,715][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:48:26,717][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:48:26,717][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:48:26,720][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:48:26,721][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:48:26,722][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:48:26,723][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:48:26,725][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:48:26,728][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:48:26,729][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:48:26,731][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:48:26,903][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.18it/s v_num: 1.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.059 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.021             
[2024-05-30 16:49:56,788][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/2>
[2024-05-30 16:49:56,789][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:49:56,793][HYDRA] 	#233 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 16:49:57,078][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:49:57,080][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:49:57,082][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:49:57,082][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:49:57,086][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:49:57,086][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:49:57,087][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:49:57,088][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:49:57,090][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:49:57,091][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:49:57,091][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:49:57,093][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:49:57,141][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.19it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.057 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 16:51:27,380][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/3>
[2024-05-30 16:51:27,381][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:51:27,384][HYDRA] 	#234 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 16:51:27,662][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:51:27,664][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:51:27,666][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:51:27,666][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:51:27,669][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:51:27,670][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:51:27,671][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:51:27,672][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:51:27,673][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:51:27,674][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:51:27,675][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:51:27,677][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:51:27,719][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.058 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 16:52:57,415][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/4>
[2024-05-30 16:52:57,416][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:52:57,419][HYDRA] 	#235 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 16:52:57,854][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:52:57,859][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:52:57,862][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:52:57,863][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:52:57,868][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:52:57,869][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:52:57,870][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:52:57,870][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:52:57,872][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:52:57,875][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:52:57,876][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:52:57,878][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:52:58,176][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.17it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.060 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-05-30 16:54:26,923][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/5>
[2024-05-30 16:54:26,923][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:54:26,926][HYDRA] 	#236 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 16:54:27,219][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:54:27,222][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:54:27,223][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:54:27,224][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:54:27,228][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:54:27,229][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:54:27,229][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:54:27,230][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:54:27,232][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:54:27,235][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:54:27,235][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:54:27,237][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:54:27,322][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.058 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-05-30 16:55:55,953][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/6>
[2024-05-30 16:55:55,955][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:55:55,973][HYDRA] 	#237 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 16:55:56,371][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:55:56,376][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:55:56,379][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:55:56,379][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:55:56,384][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:55:56,385][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:55:56,386][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:55:56,386][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:55:56,388][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:55:56,391][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:55:56,392][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:55:56,394][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:55:56,719][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.21it/s v_num: 1.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.063 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.025             
[2024-05-30 16:57:26,093][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/7>
[2024-05-30 16:57:26,094][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:57:26,098][HYDRA] 	#238 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 16:57:26,391][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:57:26,394][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:57:26,395][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:57:26,396][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:57:26,399][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:57:26,400][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:57:26,400][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:57:26,401][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:57:26,403][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:57:26,406][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:57:26,406][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:57:26,409][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:57:26,511][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.18it/s v_num: 1.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.833 val/recall: 
                                                              0.500 val/mre:    
                                                              0.059 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-05-30 16:58:56,206][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/8>
[2024-05-30 16:58:56,207][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 16:58:56,210][HYDRA] 	#239 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 16:58:56,506][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 16:58:56,508][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 16:58:56,510][train.py][INFO] - Instantiating callbacks...
[2024-05-30 16:58:56,510][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 16:58:56,514][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 16:58:56,515][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 16:58:56,515][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 16:58:56,516][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 16:58:56,518][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 16:58:56,521][train.py][INFO] - Instantiating loggers...
[2024-05-30 16:58:56,521][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 16:58:56,523][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 16:58:56,615][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.18it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.400 val/mre:    
                                                              0.058 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-05-30 17:00:25,144][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.1/9>
[2024-05-30 17:00:25,144][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:00:25,147][HYDRA] 	#240 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 17:00:25,430][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:00:25,433][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:00:25,434][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:00:25,435][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:00:25,438][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:00:25,439][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:00:25,440][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:00:25,441][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:00:25,442][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:00:25,443][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:00:25,444][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:00:25,446][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:00:25,488][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.21it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.062 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 17:01:55,119][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/0>
[2024-05-30 17:01:55,120][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:01:55,124][HYDRA] 	#241 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 17:01:55,401][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:01:55,404][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:01:55,405][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:01:55,406][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:01:55,409][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:01:55,410][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:01:55,411][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:01:55,411][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:01:55,413][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:01:55,414][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:01:55,414][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:01:55,416][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:01:55,458][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.17it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.052 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 17:03:24,684][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/1>
[2024-05-30 17:03:24,684][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:03:24,692][HYDRA] 	#242 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 17:03:25,013][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:03:25,016][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:03:25,018][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:03:25,018][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:03:25,022][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:03:25,023][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:03:25,024][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:03:25,025][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:03:25,027][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:03:25,030][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:03:25,030][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:03:25,032][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:03:25,157][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.14it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.063 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-05-30 17:04:55,997][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/2>
[2024-05-30 17:04:55,998][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:04:56,001][HYDRA] 	#243 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 17:04:56,294][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:04:56,297][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:04:56,299][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:04:56,300][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:04:56,304][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:04:56,305][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:04:56,305][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:04:56,308][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:04:56,310][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:04:56,313][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:04:56,313][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:04:56,315][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:04:56,406][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.18it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.059 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 17:06:25,313][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/3>
[2024-05-30 17:06:25,314][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:06:25,317][HYDRA] 	#244 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 17:06:25,615][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:06:25,618][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:06:25,619][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:06:25,620][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:06:25,623][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:06:25,624][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:06:25,624][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:06:25,625][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:06:25,627][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:06:25,631][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:06:25,632][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:06:25,634][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:06:25,729][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.057 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 17:07:54,723][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/4>
[2024-05-30 17:07:54,724][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:07:54,727][HYDRA] 	#245 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 17:07:55,021][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:07:55,023][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:07:55,025][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:07:55,025][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:07:55,029][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:07:55,029][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:07:55,030][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:07:55,031][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:07:55,033][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:07:55,036][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:07:55,036][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:07:55,038][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:07:55,125][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.20it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.069 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-05-30 17:09:23,949][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/5>
[2024-05-30 17:09:23,950][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:09:23,954][HYDRA] 	#246 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 17:09:24,259][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:09:24,261][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:09:24,263][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:09:24,263][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:09:24,267][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:09:24,268][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:09:24,268][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:09:24,269][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:09:24,271][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:09:24,275][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:09:24,275][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:09:24,277][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:09:24,380][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.19it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre:    
                                                              0.051 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 17:10:52,902][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/6>
[2024-05-30 17:10:52,903][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:10:52,906][HYDRA] 	#247 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 17:10:53,211][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:10:53,213][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:10:53,215][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:10:53,215][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:10:53,219][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:10:53,220][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:10:53,220][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:10:53,221][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:10:53,223][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:10:53,227][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:10:53,227][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:10:53,229][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:10:53,316][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.27it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.064 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.024             
[2024-05-30 17:12:21,786][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/7>
[2024-05-30 17:12:21,787][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:12:21,790][HYDRA] 	#248 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 17:12:22,073][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:12:22,075][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:12:22,077][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:12:22,077][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:12:22,080][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:12:22,081][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:12:22,082][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:12:22,083][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:12:22,084][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:12:22,085][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:12:22,086][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:12:22,088][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:12:22,133][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.19it/s v_num: 1.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.077 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.032             
[2024-05-30 17:13:52,177][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/8>
[2024-05-30 17:13:52,178][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:13:52,182][HYDRA] 	#249 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 17:13:52,463][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:13:52,466][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:13:52,467][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:13:52,468][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:13:52,471][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:13:52,472][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:13:52,472][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:13:52,473][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:13:52,475][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:13:52,476][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:13:52,476][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:13:52,478][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:13:52,528][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.23it/s v_num: 1.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.055 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 17:15:20,953][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.15/9>
[2024-05-30 17:15:20,954][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:15:20,962][HYDRA] 	#250 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 17:15:21,439][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:15:21,444][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:15:21,446][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:15:21,446][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:15:21,450][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:15:21,451][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:15:21,451][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:15:21,452][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:15:21,454][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:15:21,457][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:15:21,458][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:15:21,460][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:15:21,578][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.17it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.059 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-05-30 17:16:50,642][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/0>
[2024-05-30 17:16:50,642][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:16:50,646][HYDRA] 	#251 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 17:16:51,281][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:16:51,284][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:16:51,286][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:16:51,286][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:16:51,290][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:16:51,291][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:16:51,291][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:16:51,292][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:16:51,294][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:16:51,297][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:16:51,297][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:16:51,300][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:16:51,393][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.21it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.062 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-05-30 17:18:20,662][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/1>
[2024-05-30 17:18:20,663][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:18:20,668][HYDRA] 	#252 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 17:18:21,055][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:18:21,060][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:18:21,062][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:18:21,062][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:18:21,068][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:18:21,069][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:18:21,070][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:18:21,071][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:18:21,073][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:18:21,084][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:18:21,084][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:18:21,087][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:18:21,698][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.08it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.064 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-05-30 17:19:52,312][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/2>
[2024-05-30 17:19:52,312][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:19:52,316][HYDRA] 	#253 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 17:19:52,607][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:19:52,609][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:19:52,611][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:19:52,611][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:19:52,614][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:19:52,615][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:19:52,616][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:19:52,617][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:19:52,618][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:19:52,623][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:19:52,623][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:19:52,625][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:19:52,712][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.18it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.059 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 17:21:21,502][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/3>
[2024-05-30 17:21:21,503][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:21:21,506][HYDRA] 	#254 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 17:21:21,793][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:21:21,796][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:21:21,797][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:21:21,798][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:21:21,801][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:21:21,802][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:21:21,803][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:21:21,803][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:21:21,805][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:21:21,808][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:21:21,808][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:21:21,810][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:21:21,902][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.15it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.056 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 17:22:50,845][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/4>
[2024-05-30 17:22:50,846][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:22:50,850][HYDRA] 	#255 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 17:22:51,130][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:22:51,132][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:22:51,134][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:22:51,134][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:22:51,138][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:22:51,138][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:22:51,139][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:22:51,140][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:22:51,142][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:22:51,143][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:22:51,143][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:22:51,145][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:22:51,188][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.27it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.064 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-05-30 17:24:18,801][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/5>
[2024-05-30 17:24:18,802][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:24:18,805][HYDRA] 	#256 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 17:24:19,082][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:24:19,084][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:24:19,086][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:24:19,086][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:24:19,089][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:24:19,090][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:24:19,091][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:24:19,092][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:24:19,093][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:24:19,094][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:24:19,095][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:24:19,097][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:24:19,140][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.15it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre:    
                                                              0.058 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 17:25:48,620][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/6>
[2024-05-30 17:25:48,620][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:25:48,623][HYDRA] 	#257 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 17:25:48,932][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:25:48,935][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:25:48,937][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:25:48,938][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:25:48,941][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:25:48,942][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:25:48,943][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:25:48,944][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:25:48,945][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:25:48,949][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:25:48,949][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:25:48,951][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:25:49,037][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.24it/s v_num: 1.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.058 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 17:27:17,542][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/7>
[2024-05-30 17:27:17,542][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:27:17,546][HYDRA] 	#258 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 17:27:17,839][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:27:17,842][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:27:17,844][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:27:17,844][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:27:17,848][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:27:17,849][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:27:17,850][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:27:17,851][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:27:17,852][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:27:17,855][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:27:17,856][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:27:17,858][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:27:17,942][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.11it/s v_num: 1.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.060 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-05-30 17:28:49,547][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/8>
[2024-05-30 17:28:49,548][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:28:49,551][HYDRA] 	#259 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 17:28:49,850][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:28:49,852][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:28:49,854][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:28:49,854][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:28:49,857][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:28:49,858][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:28:49,859][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:28:49,860][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:28:49,861][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:28:49,866][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:28:49,866][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:28:49,868][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:28:49,974][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.15it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.056 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 17:30:19,460][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.2/9>
[2024-05-30 17:30:19,460][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:30:19,464][HYDRA] 	#260 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 17:30:19,758][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:30:19,760][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:30:19,762][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:30:19,762][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:30:19,768][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:30:19,769][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:30:19,770][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:30:19,771][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:30:19,772][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:30:19,776][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:30:19,777][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:30:19,779][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:30:19,870][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.19it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.063 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 17:31:49,303][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/0>
[2024-05-30 17:31:49,303][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:31:49,307][HYDRA] 	#261 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 17:31:49,610][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:31:49,613][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:31:49,614][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:31:49,614][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:31:49,618][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:31:49,619][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:31:49,619][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:31:49,620][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:31:49,622][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:31:49,625][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:31:49,625][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:31:49,627][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:31:49,713][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.22it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.600 val/mre:    
                                                              0.073 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.020             
[2024-05-30 17:33:18,644][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/1>
[2024-05-30 17:33:18,645][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:33:18,648][HYDRA] 	#262 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 17:33:18,931][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:33:18,933][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:33:18,935][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:33:18,935][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:33:18,938][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:33:18,939][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:33:18,940][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:33:18,941][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:33:18,942][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:33:18,946][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:33:18,947][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:33:18,949][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:33:19,034][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.069 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.018             
[2024-05-30 17:34:48,526][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/2>
[2024-05-30 17:34:48,527][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:34:48,530][HYDRA] 	#263 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 17:34:48,816][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:34:48,818][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:34:48,820][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:34:48,820][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:34:48,823][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:34:48,824][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:34:48,825][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:34:48,826][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:34:48,827][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:34:48,828][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:34:48,829][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:34:48,831][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:34:48,874][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.22it/s v_num: 1.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.300 val/mre:    
                                                              0.060 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 17:36:18,364][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/3>
[2024-05-30 17:36:18,369][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:36:18,375][HYDRA] 	#264 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 17:36:18,814][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:36:18,817][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:36:18,818][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:36:18,819][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:36:18,822][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:36:18,823][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:36:18,824][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:36:18,824][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:36:18,826][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:36:18,827][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:36:18,827][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:36:18,829][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:36:18,874][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.14it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.062 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 17:37:48,786][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/4>
[2024-05-30 17:37:48,787][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:37:48,790][HYDRA] 	#265 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 17:37:49,082][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:37:49,085][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:37:49,086][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:37:49,087][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:37:49,090][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:37:49,091][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:37:49,091][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:37:49,092][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:37:49,094][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:37:49,097][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:37:49,097][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:37:49,099][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:37:49,188][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.21it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.062 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 17:39:17,801][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/5>
[2024-05-30 17:39:17,806][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:39:17,813][HYDRA] 	#266 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 17:39:18,172][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:39:18,175][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:39:18,176][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:39:18,177][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:39:18,180][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:39:18,181][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:39:18,182][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:39:18,182][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:39:18,184][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:39:18,189][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:39:18,189][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:39:18,191][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:39:18,398][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.21it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.057 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 17:40:47,083][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/6>
[2024-05-30 17:40:47,087][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:40:47,094][HYDRA] 	#267 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 17:40:47,482][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:40:47,485][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:40:47,487][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:40:47,487][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:40:47,490][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:40:47,491][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:40:47,492][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:40:47,493][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:40:47,495][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:40:47,498][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:40:47,498][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:40:47,500][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:40:47,641][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.18it/s v_num: 1.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.100 val/mre:    
                                                              0.066 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.016             
[2024-05-30 17:42:16,917][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/7>
[2024-05-30 17:42:16,918][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:42:16,921][HYDRA] 	#268 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 17:42:17,207][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:42:17,210][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:42:17,211][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:42:17,212][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:42:17,215][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:42:17,216][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:42:17,216][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:42:17,217][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:42:17,219][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:42:17,222][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:42:17,222][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:42:17,224][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:42:17,306][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.12it/s v_num: 1.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.067 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.017             
[2024-05-30 17:43:48,125][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/8>
[2024-05-30 17:43:48,126][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:43:48,129][HYDRA] 	#269 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 17:43:48,428][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:43:48,431][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:43:48,432][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:43:48,433][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:43:48,436][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:43:48,437][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:43:48,438][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:43:48,438][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:43:48,440][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:43:48,443][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:43:48,443][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:43:48,446][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:43:48,539][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.15it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.057 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 17:45:17,562][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.25/9>
[2024-05-30 17:45:17,563][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:45:17,566][HYDRA] 	#270 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 17:45:17,851][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:45:17,854][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:45:17,856][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:45:17,856][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:45:17,859][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:45:17,860][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:45:17,861][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:45:17,862][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:45:17,863][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:45:17,867][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:45:17,867][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:45:17,869][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:45:17,954][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.19it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre:    
                                                              0.061 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 17:46:46,395][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/0>
[2024-05-30 17:46:46,396][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:46:46,402][HYDRA] 	#271 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 17:46:46,688][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:46:46,691][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:46:46,692][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:46:46,693][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:46:46,696][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:46:46,697][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:46:46,697][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:46:46,698][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:46:46,700][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:46:46,701][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:46:46,701][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:46:46,703][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:46:46,751][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.30it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.070 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.023             
[2024-05-30 17:48:15,724][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/1>
[2024-05-30 17:48:15,725][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:48:15,728][HYDRA] 	#272 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 17:48:16,007][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:48:16,009][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:48:16,011][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:48:16,011][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:48:16,015][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:48:16,016][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:48:16,016][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:48:16,017][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:48:16,019][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:48:16,020][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:48:16,020][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:48:16,022][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:48:16,064][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.19it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.500 val/mre:    
                                                              0.064 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 17:49:45,348][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/2>
[2024-05-30 17:49:45,349][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:49:45,353][HYDRA] 	#273 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 17:49:45,645][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:49:45,647][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:49:45,649][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:49:45,649][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:49:45,652][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:49:45,653][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:49:45,654][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:49:45,655][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:49:45,656][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:49:45,659][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:49:45,660][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:49:45,662][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:49:45,748][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.17it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.057 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 17:51:15,032][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/3>
[2024-05-30 17:51:15,032][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:51:15,036][HYDRA] 	#274 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 17:51:15,332][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:51:15,335][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:51:15,337][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:51:15,337][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:51:15,341][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:51:15,342][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:51:15,342][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:51:15,343][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:51:15,345][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:51:15,349][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:51:15,349][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:51:15,351][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:51:15,437][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.18it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.059 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 17:52:44,653][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/4>
[2024-05-30 17:52:44,654][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:52:44,657][HYDRA] 	#275 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 17:52:44,985][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:52:44,990][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:52:44,992][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:52:44,992][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:52:44,997][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:52:44,998][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:52:44,998][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:52:44,999][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:52:45,001][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:52:45,004][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:52:45,005][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:52:45,007][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:52:45,173][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.059 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 17:54:13,053][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/5>
[2024-05-30 17:54:13,054][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:54:13,058][HYDRA] 	#276 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 17:54:13,347][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:54:13,350][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:54:13,352][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:54:13,352][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:54:13,355][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:54:13,356][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:54:13,357][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:54:13,358][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:54:13,359][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:54:13,384][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:54:13,385][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:54:13,389][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:54:13,487][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.13it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.070 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.019             
[2024-05-30 17:55:42,452][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/6>
[2024-05-30 17:55:42,453][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:55:42,456][HYDRA] 	#277 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 17:55:42,747][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:55:42,749][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:55:42,751][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:55:42,751][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:55:42,755][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:55:42,756][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:55:42,756][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:55:42,757][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:55:42,759][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:55:42,762][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:55:42,762][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:55:42,764][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:55:42,853][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.22it/s v_num: 1.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.600 val/mre:    
                                                              0.064 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 17:57:11,644][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/7>
[2024-05-30 17:57:11,645][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:57:11,648][HYDRA] 	#278 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 17:57:11,941][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:57:11,944][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:57:11,945][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:57:11,946][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:57:11,949][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:57:11,950][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:57:11,951][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:57:11,951][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:57:11,953][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:57:11,956][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:57:11,957][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:57:11,959][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:57:12,047][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.065 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 17:58:41,487][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/8>
[2024-05-30 17:58:41,488][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 17:58:41,491][HYDRA] 	#279 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 17:58:41,770][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 17:58:41,772][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 17:58:41,774][train.py][INFO] - Instantiating callbacks...
[2024-05-30 17:58:41,774][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 17:58:41,777][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 17:58:41,778][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 17:58:41,779][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 17:58:41,780][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 17:58:41,781][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 17:58:41,782][train.py][INFO] - Instantiating loggers...
[2024-05-30 17:58:41,783][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 17:58:41,785][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 17:58:41,828][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.21it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.061 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 18:00:10,832][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/controlling_temper/0.3/9>
[2024-05-30 18:00:10,833][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:00:10,838][HYDRA] 	#280 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 18:00:11,121][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:00:11,123][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:00:11,125][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:00:11,125][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:00:11,129][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:00:11,129][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:00:11,130][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:00:11,131][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:00:11,133][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:00:11,134][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:00:11,134][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:00:11,136][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:00:11,178][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.94it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 18:01:42,858][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/0>
[2024-05-30 18:01:42,859][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:01:42,862][HYDRA] 	#281 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 18:01:43,159][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:01:43,161][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:01:43,163][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:01:43,163][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:01:43,167][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:01:43,167][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:01:43,168][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:01:43,169][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:01:43,171][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:01:43,184][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:01:43,184][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:01:43,186][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:01:43,284][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.99it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 18:03:14,658][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/1>
[2024-05-30 18:03:14,658][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:03:14,662][HYDRA] 	#282 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 18:03:14,963][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:03:14,966][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:03:14,967][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:03:14,968][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:03:14,972][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:03:14,973][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:03:14,973][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:03:14,974][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:03:14,976][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:03:14,979][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:03:14,979][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:03:14,981][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:03:15,071][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.83it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 18:04:49,305][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/2>
[2024-05-30 18:04:49,306][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:04:49,309][HYDRA] 	#283 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 18:04:49,601][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:04:49,604][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:04:49,605][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:04:49,606][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:04:49,609][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:04:49,610][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:04:49,611][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:04:49,612][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:04:49,613][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:04:49,625][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:04:49,626][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:04:49,629][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:04:49,724][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.88it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 18:06:23,286][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/3>
[2024-05-30 18:06:23,287][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:06:23,290][HYDRA] 	#284 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 18:06:23,581][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:06:23,584][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:06:23,586][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:06:23,587][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:06:23,591][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:06:23,591][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:06:23,592][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:06:23,593][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:06:23,595][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:06:23,598][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:06:23,598][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:06:23,600][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:06:23,686][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.94it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 18:07:57,049][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/4>
[2024-05-30 18:07:57,050][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:07:57,053][HYDRA] 	#285 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 18:07:57,344][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:07:57,346][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:07:57,348][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:07:57,348][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:07:57,351][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:07:57,352][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:07:57,353][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:07:57,354][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:07:57,356][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:07:57,359][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:07:57,359][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:07:57,362][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:07:57,446][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.83it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 18:09:31,044][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/5>
[2024-05-30 18:09:31,044][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:09:31,048][HYDRA] 	#286 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 18:09:31,424][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:09:31,427][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:09:31,429][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:09:31,430][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:09:31,433][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:09:31,434][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:09:31,435][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:09:31,436][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:09:31,438][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:09:31,441][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:09:31,441][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:09:31,444][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:09:31,532][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.94it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 18:11:05,009][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/6>
[2024-05-30 18:11:05,010][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:11:05,013][HYDRA] 	#287 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 18:11:05,307][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:11:05,310][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:11:05,312][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:11:05,312][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:11:05,315][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:11:05,316][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:11:05,317][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:11:05,318][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:11:05,319][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:11:05,322][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:11:05,323][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:11:05,325][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:11:05,424][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.96it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 18:12:36,736][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/7>
[2024-05-30 18:12:36,737][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:12:36,740][HYDRA] 	#288 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 18:12:37,028][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:12:37,030][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:12:37,032][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:12:37,032][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:12:37,035][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:12:37,036][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:12:37,037][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:12:37,038][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:12:37,039][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:12:37,043][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:12:37,043][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:12:37,045][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:12:37,132][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.92it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 18:14:09,804][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/8>
[2024-05-30 18:14:09,805][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:14:09,808][HYDRA] 	#289 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 18:14:10,099][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:14:10,101][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:14:10,103][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:14:10,103][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:14:10,107][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:14:10,107][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:14:10,108][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:14:10,109][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:14:10,111][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:14:10,112][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:14:10,112][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:14:10,114][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:14:10,156][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.06it/s v_num: 1.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 18:15:40,595][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.0/9>
[2024-05-30 18:15:40,596][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:15:40,599][HYDRA] 	#290 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 18:15:40,877][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:15:40,879][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:15:40,881][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:15:40,881][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:15:40,885][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:15:40,885][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:15:40,886][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:15:40,887][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:15:40,889][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:15:40,890][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:15:40,890][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:15:40,892][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:15:40,935][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.07it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.027 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 18:17:11,360][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/0>
[2024-05-30 18:17:11,361][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:17:11,365][HYDRA] 	#291 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 18:17:11,643][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:17:11,645][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:17:11,647][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:17:11,647][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:17:11,651][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:17:11,651][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:17:11,652][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:17:11,653][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:17:11,655][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:17:11,656][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:17:11,656][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:17:11,658][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:17:11,702][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.95it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.200 val/mre:    
                                                              0.030 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 18:18:44,794][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/1>
[2024-05-30 18:18:44,795][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:18:44,798][HYDRA] 	#292 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 18:18:45,092][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:18:45,095][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:18:45,096][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:18:45,097][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:18:45,100][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:18:45,101][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:18:45,102][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:18:45,102][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:18:45,104][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:18:45,108][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:18:45,108][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:18:45,110][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:18:45,195][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.89it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.027 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 18:20:20,268][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/2>
[2024-05-30 18:20:20,269][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:20:20,272][HYDRA] 	#293 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 18:20:20,566][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:20:20,569][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:20:20,571][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:20:20,571][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:20:20,575][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:20:20,576][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:20:20,577][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:20:20,577][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:20:20,579][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:20:20,582][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:20:20,582][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:20:20,584][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:20:20,667][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.92it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.025 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 18:21:52,938][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/3>
[2024-05-30 18:21:52,939][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:21:52,943][HYDRA] 	#294 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 18:21:53,236][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:21:53,238][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:21:53,240][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:21:53,240][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:21:53,243][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:21:53,244][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:21:53,245][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:21:53,246][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:21:53,247][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:21:53,268][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:21:53,269][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:21:53,273][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:21:53,381][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.88it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.026 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 18:23:27,093][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/4>
[2024-05-30 18:23:27,093][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:23:27,097][HYDRA] 	#295 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 18:23:27,396][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:23:27,399][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:23:27,401][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:23:27,402][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:23:27,406][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:23:27,406][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:23:27,407][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:23:27,410][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:23:27,412][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:23:27,415][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:23:27,415][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:23:27,417][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:23:27,524][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.00it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.023 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 18:24:59,409][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/5>
[2024-05-30 18:24:59,410][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:24:59,413][HYDRA] 	#296 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 18:24:59,717][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:24:59,719][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:24:59,721][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:24:59,721][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:24:59,724][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:24:59,725][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:24:59,726][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:24:59,727][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:24:59,729][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:24:59,733][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:24:59,734][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:24:59,736][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:24:59,841][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.89it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.024 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 18:26:31,732][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/6>
[2024-05-30 18:26:31,733][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:26:31,736][HYDRA] 	#297 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 18:26:32,023][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:26:32,026][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:26:32,027][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:26:32,028][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:26:32,031][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:26:32,032][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:26:32,033][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:26:32,034][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:26:32,035][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:26:32,039][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:26:32,039][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:26:32,041][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:26:32,128][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.06it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.028 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 18:28:02,733][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/7>
[2024-05-30 18:28:02,734][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:28:02,737][HYDRA] 	#298 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 18:28:03,032][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:28:03,034][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:28:03,036][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:28:03,036][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:28:03,040][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:28:03,041][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:28:03,041][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:28:03,042][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:28:03,044][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:28:03,049][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:28:03,050][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:28:03,052][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:28:03,139][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.82it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.026 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 18:29:37,195][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/8>
[2024-05-30 18:29:37,196][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:29:37,206][HYDRA] 	#299 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 18:29:37,489][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:29:37,491][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:29:37,493][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:29:37,493][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:29:37,496][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:29:37,497][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:29:37,498][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:29:37,499][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:29:37,500][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:29:37,501][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:29:37,502][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:29:37,504][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:29:37,547][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.95it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.030 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 18:31:09,313][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.05/9>
[2024-05-30 18:31:09,314][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:31:09,318][HYDRA] 	#300 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 18:31:09,626][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:31:09,628][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:31:09,630][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:31:09,630][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:31:09,634][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:31:09,635][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:31:09,635][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:31:09,636][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:31:09,638][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:31:09,639][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:31:09,639][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:31:09,641][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:31:09,686][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.07it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.029 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 18:32:41,644][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/0>
[2024-05-30 18:32:41,645][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:32:41,648][HYDRA] 	#301 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 18:32:41,929][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:32:41,931][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:32:41,933][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:32:41,933][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:32:41,937][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:32:41,937][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:32:41,938][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:32:41,939][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:32:41,941][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:32:41,942][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:32:41,942][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:32:41,944][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:32:41,986][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.91it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.200 val/mre:    
                                                              0.029 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 18:34:15,527][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/1>
[2024-05-30 18:34:15,528][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:34:15,531][HYDRA] 	#302 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 18:34:15,824][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:34:15,826][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:34:15,828][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:34:15,828][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:34:15,832][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:34:15,833][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:34:15,833][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:34:15,834][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:34:15,836][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:34:15,839][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:34:15,840][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:34:15,842][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:34:15,930][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.86it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.026 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 18:35:49,279][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/2>
[2024-05-30 18:35:49,280][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:35:49,284][HYDRA] 	#303 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 18:35:49,575][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:35:49,577][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:35:49,579][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:35:49,579][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:35:49,583][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:35:49,583][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:35:49,584][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:35:49,585][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:35:49,587][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:35:49,590][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:35:49,590][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:35:49,592][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:35:49,676][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.97it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.026 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 18:37:22,281][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/3>
[2024-05-30 18:37:22,282][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:37:22,285][HYDRA] 	#304 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 18:37:22,591][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:37:22,593][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:37:22,595][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:37:22,596][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:37:22,600][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:37:22,601][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:37:22,602][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:37:22,603][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:37:22,605][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:37:22,608][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:37:22,608][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:37:22,610][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:37:22,705][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.87it/s v_num: 1.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.024 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.008             
[2024-05-30 18:38:57,025][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/4>
[2024-05-30 18:38:57,026][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:38:57,029][HYDRA] 	#305 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 18:38:57,324][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:38:57,326][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:38:57,328][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:38:57,328][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:38:57,332][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:38:57,333][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:38:57,333][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:38:57,334][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:38:57,336][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:38:57,341][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:38:57,342][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:38:57,344][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:38:57,432][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.98it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.200 val/mre:    
                                                              0.024 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 18:40:27,771][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/5>
[2024-05-30 18:40:27,772][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:40:27,775][HYDRA] 	#306 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 18:40:28,067][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:40:28,070][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:40:28,071][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:40:28,072][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:40:28,075][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:40:28,076][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:40:28,077][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:40:28,078][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:40:28,079][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:40:28,083][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:40:28,083][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:40:28,085][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:40:28,172][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.88it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.024 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 18:42:01,524][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/6>
[2024-05-30 18:42:01,525][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:42:01,528][HYDRA] 	#307 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 18:42:01,823][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:42:01,826][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:42:01,827][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:42:01,828][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:42:01,831][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:42:01,832][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:42:01,833][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:42:01,834][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:42:01,836][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:42:01,839][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:42:01,839][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:42:01,841][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:42:01,929][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.01it/s v_num: 1.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.026 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 18:43:33,539][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/7>
[2024-05-30 18:43:33,539][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:43:33,543][HYDRA] 	#308 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 18:43:33,940][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:43:33,943][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:43:33,945][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:43:33,945][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:43:33,949][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:43:33,950][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:43:33,951][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:43:33,952][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:43:33,954][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:43:33,957][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:43:33,958][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:43:33,960][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:43:34,050][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.89it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.025 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 18:45:07,094][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/8>
[2024-05-30 18:45:07,095][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:45:07,098][HYDRA] 	#309 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 18:45:07,383][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:45:07,386][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:45:07,387][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:45:07,388][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:45:07,391][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:45:07,392][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:45:07,393][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:45:07,394][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:45:07,395][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:45:07,397][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:45:07,397][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:45:07,399][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:45:07,443][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.89it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.029 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 18:46:39,921][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.1/9>
[2024-05-30 18:46:39,922][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:46:39,925][HYDRA] 	#310 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 18:46:40,256][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:46:40,259][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:46:40,260][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:46:40,261][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:46:40,264][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:46:40,265][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:46:40,266][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:46:40,266][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:46:40,268][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:46:40,269][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:46:40,269][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:46:40,271][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:46:40,365][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.05it/s v_num: 1.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.031 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 18:48:13,579][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/0>
[2024-05-30 18:48:13,579][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:48:13,583][HYDRA] 	#311 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 18:48:13,865][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:48:13,868][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:48:13,869][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:48:13,870][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:48:13,873][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:48:13,874][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:48:13,875][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:48:13,875][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:48:13,877][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:48:13,878][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:48:13,878][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:48:13,881][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:48:13,923][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.92it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.026 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 18:49:47,424][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/1>
[2024-05-30 18:49:47,425][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:49:47,428][HYDRA] 	#312 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 18:49:47,708][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:49:47,710][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:49:47,712][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:49:47,712][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:49:47,715][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:49:47,716][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:49:47,717][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:49:47,718][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:49:47,719][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:49:47,720][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:49:47,720][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:49:47,722][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:49:47,765][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.93it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.028 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 18:51:21,388][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/2>
[2024-05-30 18:51:21,388][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:51:21,393][HYDRA] 	#313 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 18:51:21,691][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:51:21,693][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:51:21,695][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:51:21,695][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:51:21,699][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:51:21,699][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:51:21,700][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:51:21,701][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:51:21,703][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:51:21,706][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:51:21,706][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:51:21,708][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:51:21,792][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.89it/s v_num: 1.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.027 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 18:52:54,933][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/3>
[2024-05-30 18:52:54,933][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:52:54,937][HYDRA] 	#314 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 18:52:55,232][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:52:55,235][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:52:55,237][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:52:55,238][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:52:55,241][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:52:55,242][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:52:55,243][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:52:55,243][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:52:55,245][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:52:55,248][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:52:55,248][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:52:55,250][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:52:55,337][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.01it/s v_num: 1.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.600 val/mre:    
                                                              0.025 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 18:54:26,697][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/4>
[2024-05-30 18:54:26,697][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:54:26,701][HYDRA] 	#315 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 18:54:26,992][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:54:26,995][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:54:26,996][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:54:26,997][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:54:27,000][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:54:27,001][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:54:27,002][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:54:27,002][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:54:27,004][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:54:27,008][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:54:27,008][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:54:27,010][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:54:27,098][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.96it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.026 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 18:55:58,772][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/5>
[2024-05-30 18:55:58,772][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:55:58,776][HYDRA] 	#316 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 18:55:59,468][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:55:59,473][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:55:59,475][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:55:59,475][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:55:59,479][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:55:59,480][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:55:59,481][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:55:59,482][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:55:59,484][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:55:59,487][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:55:59,488][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:55:59,490][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:55:59,576][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.98it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.031 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 18:57:32,027][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/6>
[2024-05-30 18:57:32,027][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:57:32,031][HYDRA] 	#317 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 18:57:32,346][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:57:32,348][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:57:32,350][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:57:32,350][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:57:32,354][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:57:32,355][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:57:32,355][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:57:32,356][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:57:32,358][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:57:32,361][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:57:32,361][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:57:32,363][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:57:32,478][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.05it/s v_num: 1.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.032 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 18:59:03,996][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/7>
[2024-05-30 18:59:03,997][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 18:59:04,000][HYDRA] 	#318 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 18:59:04,290][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 18:59:04,293][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 18:59:04,295][train.py][INFO] - Instantiating callbacks...
[2024-05-30 18:59:04,295][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 18:59:04,299][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 18:59:04,300][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 18:59:04,301][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 18:59:04,301][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 18:59:04,303][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 18:59:04,307][train.py][INFO] - Instantiating loggers...
[2024-05-30 18:59:04,307][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 18:59:04,309][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 18:59:04,406][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.90it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.200 val/mre:    
                                                              0.026 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.008             
[2024-05-30 19:00:37,887][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/8>
[2024-05-30 19:00:37,888][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:00:37,892][HYDRA] 	#319 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 19:00:38,181][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:00:38,183][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:00:38,185][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:00:38,185][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:00:38,189][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:00:38,189][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:00:38,190][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:00:38,191][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:00:38,193][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:00:38,196][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:00:38,196][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:00:38,198][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:00:38,283][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.97it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.200 val/mre:    
                                                              0.028 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 19:02:10,674][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.15/9>
[2024-05-30 19:02:10,674][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:02:10,678][HYDRA] 	#320 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 19:02:10,964][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:02:10,967][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:02:10,968][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:02:10,969][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:02:10,972][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:02:10,973][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:02:10,974][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:02:10,975][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:02:10,976][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:02:10,977][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:02:10,978][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:02:10,980][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:02:11,025][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.01it/s v_num: 1.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.600 val/mre:    
                                                              0.030 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 19:03:43,106][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/0>
[2024-05-30 19:03:43,107][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:03:43,110][HYDRA] 	#321 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 19:03:43,392][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:03:43,394][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:03:43,396][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:03:43,396][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:03:43,400][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:03:43,401][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:03:43,401][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:03:43,402][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:03:43,404][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:03:43,405][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:03:43,405][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:03:43,407][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:03:43,453][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.93it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.028 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 19:05:15,052][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/1>
[2024-05-30 19:05:15,053][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:05:15,056][HYDRA] 	#322 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 19:05:15,336][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:05:15,338][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:05:15,340][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:05:15,340][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:05:15,343][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:05:15,344][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:05:15,345][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:05:15,345][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:05:15,347][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:05:15,348][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:05:15,348][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:05:15,350][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:05:15,393][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.93it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.029 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 19:06:49,652][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/2>
[2024-05-30 19:06:49,652][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:06:49,656][HYDRA] 	#323 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 19:06:49,950][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:06:49,953][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:06:49,955][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:06:49,955][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:06:49,959][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:06:49,960][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:06:49,961][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:06:49,961][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:06:49,963][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:06:49,966][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:06:49,966][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:06:49,969][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:06:50,077][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.99it/s v_num: 1.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.028 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 19:08:22,179][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/3>
[2024-05-30 19:08:22,180][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:08:22,183][HYDRA] 	#324 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 19:08:22,473][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:08:22,475][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:08:22,477][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:08:22,477][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:08:22,481][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:08:22,481][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:08:22,482][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:08:22,483][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:08:22,485][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:08:22,488][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:08:22,488][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:08:22,490][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:08:22,575][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.04it/s v_num: 1.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.026 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 19:09:53,315][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/4>
[2024-05-30 19:09:53,316][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:09:53,320][HYDRA] 	#325 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 19:09:53,620][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:09:53,623][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:09:53,624][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:09:53,625][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:09:53,628][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:09:53,629][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:09:53,630][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:09:53,631][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:09:53,632][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:09:53,636][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:09:53,636][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:09:53,638][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:09:53,725][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.09it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.025 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.008             
[2024-05-30 19:11:24,112][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/5>
[2024-05-30 19:11:24,112][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:11:24,116][HYDRA] 	#326 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 19:11:24,412][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:11:24,414][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:11:24,416][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:11:24,416][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:11:24,420][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:11:24,421][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:11:24,421][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:11:24,422][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:11:24,424][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:11:24,427][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:11:24,427][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:11:24,429][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:11:24,516][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.95it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.200 val/mre:    
                                                              0.026 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 19:12:57,092][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/6>
[2024-05-30 19:12:57,093][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:12:57,096][HYDRA] 	#327 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 19:12:57,385][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:12:57,388][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:12:57,389][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:12:57,390][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:12:57,393][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:12:57,394][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:12:57,395][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:12:57,395][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:12:57,397][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:12:57,400][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:12:57,401][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:12:57,403][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:12:57,494][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.99it/s v_num: 1.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.030 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 19:14:28,886][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/7>
[2024-05-30 19:14:28,887][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:14:28,890][HYDRA] 	#328 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 19:14:29,181][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:14:29,183][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:14:29,185][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:14:29,185][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:14:29,188][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:14:29,189][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:14:29,190][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:14:29,191][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:14:29,192][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:14:29,195][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:14:29,196][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:14:29,198][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:14:29,285][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.87it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.029 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 19:16:03,692][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/8>
[2024-05-30 19:16:03,692][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:16:03,696][HYDRA] 	#329 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 19:16:03,981][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:16:03,984][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:16:03,985][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:16:03,986][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:16:03,989][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:16:03,990][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:16:03,991][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:16:03,991][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:16:03,993][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:16:03,994][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:16:03,994][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:16:03,996][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:16:04,039][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.01it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.028 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 19:17:35,707][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.2/9>
[2024-05-30 19:17:35,708][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:17:35,712][HYDRA] 	#330 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 19:17:35,997][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:17:36,000][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:17:36,002][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:17:36,002][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:17:36,006][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:17:36,006][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:17:36,007][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:17:36,008][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:17:36,010][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:17:36,011][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:17:36,011][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:17:36,013][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:17:36,056][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.04it/s v_num: 1.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.031 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 19:19:07,437][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/0>
[2024-05-30 19:19:07,438][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:19:07,441][HYDRA] 	#331 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 19:19:07,721][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:19:07,723][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:19:07,725][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:19:07,725][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:19:07,729][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:19:07,729][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:19:07,730][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:19:07,731][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:19:07,733][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:19:07,734][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:19:07,734][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:19:07,736][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:19:07,778][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.93it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.029 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 19:20:40,936][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/1>
[2024-05-30 19:20:40,938][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:20:40,941][HYDRA] 	#332 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 19:20:41,234][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:20:41,237][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:20:41,239][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:20:41,239][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:20:41,244][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:20:41,244][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:20:41,245][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:20:41,246][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:20:41,248][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:20:41,251][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:20:41,251][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:20:41,253][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:20:41,337][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.94it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.031 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 19:22:14,509][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/2>
[2024-05-30 19:22:14,510][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:22:14,514][HYDRA] 	#333 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 19:22:14,808][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:22:14,811][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:22:14,812][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:22:14,813][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:22:14,816][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:22:14,817][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:22:14,818][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:22:14,818][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:22:14,820][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:22:14,823][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:22:14,823][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:22:14,825][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:22:14,915][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.07it/s v_num: 1.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.028 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 19:23:46,921][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/3>
[2024-05-30 19:23:46,922][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:23:46,926][HYDRA] 	#334 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 19:23:47,245][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:23:47,248][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:23:47,249][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:23:47,250][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:23:47,253][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:23:47,254][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:23:47,255][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:23:47,256][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:23:47,257][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:23:47,261][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:23:47,261][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:23:47,263][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:23:47,348][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.01it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.027 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.008             
[2024-05-30 19:25:17,963][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/4>
[2024-05-30 19:25:17,963][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:25:17,966][HYDRA] 	#335 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 19:25:18,263][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:25:18,265][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:25:18,266][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:25:18,267][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:25:18,271][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:25:18,272][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:25:18,272][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:25:18,273][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:25:18,275][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:25:18,279][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:25:18,280][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:25:18,282][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:25:18,380][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.03it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.400 val/mre:    
                                                              0.025 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.008             
[2024-05-30 19:26:51,083][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/5>
[2024-05-30 19:26:51,084][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:26:51,087][HYDRA] 	#336 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 19:26:51,376][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:26:51,378][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:26:51,380][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:26:51,380][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:26:51,383][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:26:51,384][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:26:51,385][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:26:51,386][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:26:51,387][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:26:51,390][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:26:51,391][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:26:51,393][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:26:51,478][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.06it/s v_num: 1.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.200 val/mre:    
                                                              0.028 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 19:28:22,935][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/6>
[2024-05-30 19:28:22,936][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:28:22,939][HYDRA] 	#337 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 19:28:23,229][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:28:23,231][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:28:23,233][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:28:23,233][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:28:23,236][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:28:23,237][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:28:23,238][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:28:23,239][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:28:23,240][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:28:23,244][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:28:23,244][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:28:23,246][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:28:23,333][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.02it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.030 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 19:29:55,168][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/7>
[2024-05-30 19:29:55,169][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:29:55,173][HYDRA] 	#338 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 19:29:55,464][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:29:55,467][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:29:55,468][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:29:55,469][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:29:55,472][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:29:55,473][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:29:55,474][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:29:55,474][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:29:55,476][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:29:55,477][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:29:55,477][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:29:55,479][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:29:55,523][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.91it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.027 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 19:31:29,951][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/8>
[2024-05-30 19:31:29,952][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:31:29,955][HYDRA] 	#339 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 19:31:30,241][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:31:30,243][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:31:30,245][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:31:30,245][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:31:30,249][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:31:30,249][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:31:30,250][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:31:30,251][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:31:30,253][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:31:30,254][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:31:30,254][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:31:30,256][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:31:30,298][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.02it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.031 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 19:33:01,665][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.25/9>
[2024-05-30 19:33:01,665][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:33:01,669][HYDRA] 	#340 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 19:33:01,948][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:33:01,950][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:33:01,952][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:33:01,952][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:33:01,956][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:33:01,956][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:33:01,957][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:33:01,958][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:33:01,960][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:33:01,961][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:33:01,961][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:33:01,963][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:33:02,007][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.06it/s v_num: 1.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.600 val/mre:    
                                                              0.030 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 19:34:33,156][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/0>
[2024-05-30 19:34:33,156][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:34:33,160][HYDRA] 	#341 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 19:34:33,454][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:34:33,457][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:34:33,459][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:34:33,459][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:34:33,463][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:34:33,464][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:34:33,464][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:34:33,465][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:34:33,467][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:34:33,472][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:34:33,472][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:34:33,474][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:34:33,781][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.92it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.027 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 19:36:06,312][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/1>
[2024-05-30 19:36:06,313][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:36:06,316][HYDRA] 	#342 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 19:36:06,612][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:36:06,615][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:36:06,616][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:36:06,617][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:36:06,620][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:36:06,621][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:36:06,621][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:36:06,622][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:36:06,624][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:36:06,627][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:36:06,627][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:36:06,629][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:36:06,725][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.89it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.031 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 19:37:39,120][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/2>
[2024-05-30 19:37:39,121][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:37:39,124][HYDRA] 	#343 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 19:37:39,702][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:37:39,707][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:37:39,710][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:37:39,710][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:37:39,715][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:37:39,716][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:37:39,716][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:37:39,717][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:37:39,719][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:37:39,722][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:37:39,723][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:37:39,725][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:37:39,988][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.08it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.029 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 19:39:11,759][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/3>
[2024-05-30 19:39:11,760][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:39:11,763][HYDRA] 	#344 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 19:39:12,049][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:39:12,052][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:39:12,054][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:39:12,055][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:39:12,059][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:39:12,060][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:39:12,061][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:39:12,061][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:39:12,063][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:39:12,066][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:39:12,067][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:39:12,069][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:39:12,154][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.99it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.027 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.007             
[2024-05-30 19:40:44,975][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/4>
[2024-05-30 19:40:44,975][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:40:44,979][HYDRA] 	#345 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 19:40:45,283][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:40:45,286][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:40:45,288][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:40:45,289][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:40:45,294][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:40:45,295][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:40:45,295][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:40:45,298][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:40:45,300][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:40:45,304][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:40:45,304][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:40:45,306][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:40:45,410][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.07it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.600 val/mre:    
                                                              0.029 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 19:42:16,243][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/5>
[2024-05-30 19:42:16,244][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:42:16,247][HYDRA] 	#346 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 19:42:16,535][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:42:16,538][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:42:16,540][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:42:16,540][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:42:16,543][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:42:16,544][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:42:16,545][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:42:16,546][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:42:16,547][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:42:16,551][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:42:16,551][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:42:16,553][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:42:16,638][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.02it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.027 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.008             
[2024-05-30 19:43:48,561][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/6>
[2024-05-30 19:43:48,562][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:43:48,565][HYDRA] 	#347 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 19:43:48,852][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:43:48,855][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:43:48,857][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:43:48,857][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:43:48,860][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:43:48,861][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:43:48,862][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:43:48,863][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:43:48,865][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:43:48,866][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:43:48,866][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:43:48,868][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:43:48,914][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.00it/s v_num: 1.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.080 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.055             
[2024-05-30 19:45:21,540][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/7>
[2024-05-30 19:45:21,541][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:45:21,543][HYDRA] 	#348 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 19:45:21,842][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:45:21,844][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:45:21,846][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:45:21,846][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:45:21,849][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:45:21,850][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:45:21,851][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:45:21,852][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:45:21,854][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:45:21,854][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:45:21,855][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:45:21,857][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:45:21,900][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.91it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.028 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 19:46:54,871][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/8>
[2024-05-30 19:46:54,872][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:46:54,875][HYDRA] 	#349 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 19:46:55,283][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:46:55,286][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:46:55,288][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:46:55,289][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:46:55,292][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:46:55,293][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:46:55,294][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:46:55,294][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:46:55,296][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:46:55,300][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:46:55,300][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:46:55,302][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:46:55,401][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.96it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.030 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 19:48:27,036][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_depression/0.3/9>
[2024-05-30 19:48:27,037][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:48:27,040][HYDRA] 	#350 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 19:48:27,328][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:48:27,330][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:48:27,332][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:48:27,332][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:48:27,336][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:48:27,337][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:48:27,337][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:48:27,338][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:48:27,340][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:48:27,343][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:48:27,343][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:48:27,345][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:48:27,429][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.29it/s v_num: 1.000      
                                                              val/auc: 0.516    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.143 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 19:49:54,178][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/0>
[2024-05-30 19:49:54,179][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:49:54,182][HYDRA] 	#351 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 19:49:54,476][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:49:54,479][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:49:54,480][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:49:54,481][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:49:54,484][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:49:54,485][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:49:54,485][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:49:54,486][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:49:54,488][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:49:54,491][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:49:54,491][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:49:54,493][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:49:54,576][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.97it/s v_num: 1.000      
                                                              val/auc: 0.587    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.286 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 19:51:24,620][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/1>
[2024-05-30 19:51:24,621][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:51:24,624][HYDRA] 	#352 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 19:51:24,918][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:51:24,921][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:51:24,923][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:51:24,923][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:51:24,927][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:51:24,928][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:51:24,929][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:51:24,930][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:51:24,931][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:51:24,934][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:51:24,935][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:51:24,937][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:51:25,024][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.96it/s v_num: 1.000      
                                                              val/auc: 0.587    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.286 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 19:52:56,295][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/2>
[2024-05-30 19:52:56,296][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:52:56,300][HYDRA] 	#353 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 19:52:56,589][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:52:56,591][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:52:56,593][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:52:56,593][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:52:56,597][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:52:56,597][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:52:56,598][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:52:56,599][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:52:56,601][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:52:56,613][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:52:56,613][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:52:56,615][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:52:56,719][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.96it/s v_num: 1.000      
                                                              val/auc: 0.516    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.143 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 19:54:27,357][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/3>
[2024-05-30 19:54:27,361][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:54:27,379][HYDRA] 	#354 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 19:54:27,663][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:54:27,666][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:54:27,667][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:54:27,668][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:54:27,671][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:54:27,672][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:54:27,673][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:54:27,674][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:54:27,675][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:54:27,677][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:54:27,677][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:54:27,679][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:54:27,723][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.03it/s v_num: 1.000      
                                                              val/auc: 0.532    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.286 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 19:55:57,610][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/4>
[2024-05-30 19:55:57,613][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:55:57,617][HYDRA] 	#355 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 19:55:57,902][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:55:57,904][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:55:57,906][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:55:57,906][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:55:57,910][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:55:57,910][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:55:57,911][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:55:57,912][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:55:57,914][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:55:57,915][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:55:57,915][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:55:57,917][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:55:57,965][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.94it/s v_num: 1.000      
                                                              val/auc: 0.587    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.286 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 19:57:29,015][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/5>
[2024-05-30 19:57:29,016][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:57:29,019][HYDRA] 	#356 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 19:57:29,298][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:57:29,301][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:57:29,302][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:57:29,303][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:57:29,306][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:57:29,307][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:57:29,307][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:57:29,308][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:57:29,310][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:57:29,311][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:57:29,311][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:57:29,313][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:57:29,355][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.99it/s v_num: 1.000      
                                                              val/auc: 0.587    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.286 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 19:58:59,783][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/6>
[2024-05-30 19:58:59,783][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 19:58:59,787][HYDRA] 	#357 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 19:59:00,081][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 19:59:00,084][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 19:59:00,085][train.py][INFO] - Instantiating callbacks...
[2024-05-30 19:59:00,086][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 19:59:00,089][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 19:59:00,090][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 19:59:00,090][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 19:59:00,091][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 19:59:00,093][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 19:59:00,096][train.py][INFO] - Instantiating loggers...
[2024-05-30 19:59:00,096][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 19:59:00,098][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 19:59:00,185][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.04it/s v_num: 1.000      
                                                              val/auc: 0.587    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.286 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 20:00:30,292][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/7>
[2024-05-30 20:00:30,293][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:00:30,295][HYDRA] 	#358 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 20:00:30,588][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:00:30,590][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:00:30,591][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:00:30,592][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:00:30,595][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:00:30,596][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:00:30,597][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:00:30,598][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:00:30,599][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:00:30,602][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:00:30,603][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:00:30,605][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:00:30,689][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.00it/s v_num: 1.000      
                                                              val/auc: 0.643    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.286 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.045             
[2024-05-30 20:01:58,179][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/8>
[2024-05-30 20:01:58,180][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:01:58,183][HYDRA] 	#359 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 20:01:58,481][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:01:58,484][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:01:58,486][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:01:58,486][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:01:58,490][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:01:58,491][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:01:58,491][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:01:58,492][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:01:58,494][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:01:58,497][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:01:58,497][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:01:58,499][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:01:58,583][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.96it/s v_num: 1.000      
                                                              val/auc: 0.571    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.143 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 20:03:28,570][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.0/9>
[2024-05-30 20:03:28,570][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:03:28,573][HYDRA] 	#360 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 20:03:28,870][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:03:28,873][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:03:28,874][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:03:28,875][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:03:28,878][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:03:28,879][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:03:28,879][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:03:28,880][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:03:28,882][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:03:28,885][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:03:28,886][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:03:28,888][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:03:28,973][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.17it/s v_num: 1.000      
                                                              val/auc: 0.476    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.286 val/mre:    
                                                              0.024 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 20:04:55,402][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/0>
[2024-05-30 20:04:55,403][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:04:55,406][HYDRA] 	#361 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 20:04:55,692][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:04:55,695][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:04:55,696][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:04:55,697][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:04:55,700][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:04:55,701][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:04:55,702][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:04:55,703][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:04:55,704][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:04:55,708][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:04:55,708][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:04:55,710][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:04:55,798][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.03it/s v_num: 1.000      
                                                              val/auc: 0.532    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.286 val/mre:    
                                                              0.023 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 20:06:26,144][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/1>
[2024-05-30 20:06:26,145][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:06:26,148][HYDRA] 	#362 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 20:06:26,432][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:06:26,435][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:06:26,436][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:06:26,437][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:06:26,440][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:06:26,441][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:06:26,442][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:06:26,442][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:06:26,444][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:06:26,445][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:06:26,445][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:06:26,447][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:06:26,492][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.92it/s v_num: 1.000      
                                                              val/auc: 0.659    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.429 val/mre:    
                                                              0.019 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.007             
[2024-05-30 20:07:56,252][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/2>
[2024-05-30 20:07:56,253][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:07:56,256][HYDRA] 	#363 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 20:07:56,533][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:07:56,536][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:07:56,537][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:07:56,538][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:07:56,541][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:07:56,542][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:07:56,542][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:07:56,543][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:07:56,545][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:07:56,546][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:07:56,546][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:07:56,548][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:07:56,593][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.11it/s v_num: 1.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.027 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 20:09:25,999][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/3>
[2024-05-30 20:09:26,000][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:09:26,003][HYDRA] 	#364 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 20:09:26,300][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:09:26,303][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:09:26,304][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:09:26,305][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:09:26,308][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:09:26,309][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:09:26,310][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:09:26,310][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:09:26,312][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:09:26,315][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:09:26,315][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:09:26,317][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:09:26,405][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.05it/s v_num: 1.000      
                                                              val/auc: 0.476    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.286 val/mre:    
                                                              0.028 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 20:10:56,573][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/4>
[2024-05-30 20:10:56,574][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:10:56,577][HYDRA] 	#365 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 20:10:56,873][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:10:56,876][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:10:56,878][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:10:56,878][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:10:56,882][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:10:56,883][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:10:56,884][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:10:56,884][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:10:56,886][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:10:56,915][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:10:56,915][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:10:56,920][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:10:57,039][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.87it/s v_num: 1.000      
                                                              val/auc: 0.587    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.286 val/mre:    
                                                              0.026 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 20:12:28,017][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/5>
[2024-05-30 20:12:28,018][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:12:28,021][HYDRA] 	#366 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 20:12:28,313][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:12:28,316][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:12:28,318][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:12:28,318][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:12:28,322][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:12:28,323][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:12:28,324][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:12:28,325][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:12:28,326][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:12:28,329][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:12:28,330][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:12:28,332][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:12:28,418][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.13it/s v_num: 1.000      
                                                              val/auc: 0.460    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.143 val/mre:    
                                                              0.023 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 20:13:57,292][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/6>
[2024-05-30 20:13:57,292][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:13:57,296][HYDRA] 	#367 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 20:13:57,583][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:13:57,586][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:13:57,587][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:13:57,588][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:13:57,591][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:13:57,592][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:13:57,592][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:13:57,593][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:13:57,595][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:13:57,598][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:13:57,598][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:13:57,600][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:13:57,685][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.10it/s v_num: 1.000      
                                                              val/auc: 0.587    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.286 val/mre:    
                                                              0.022 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.008             
[2024-05-30 20:15:27,199][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/7>
[2024-05-30 20:15:27,200][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:15:27,203][HYDRA] 	#368 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 20:15:27,493][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:15:27,496][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:15:27,497][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:15:27,498][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:15:27,501][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:15:27,502][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:15:27,502][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:15:27,503][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:15:27,505][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:15:27,508][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:15:27,508][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:15:27,510][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:15:27,596][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.04it/s v_num: 1.000      
                                                              val/auc: 0.659    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.429 val/mre:    
                                                              0.023 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-05-30 20:16:57,353][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/8>
[2024-05-30 20:16:57,353][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:16:57,357][HYDRA] 	#369 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 20:16:57,678][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:16:57,680][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:16:57,682][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:16:57,682][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:16:57,686][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:16:57,686][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:16:57,687][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:16:57,688][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:16:57,690][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:16:57,691][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:16:57,691][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:16:57,693][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:16:57,789][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.12it/s v_num: 1.000      
                                                              val/auc: 0.587    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.286 val/mre:    
                                                              0.021 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 20:18:27,069][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.05/9>
[2024-05-30 20:18:27,070][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:18:27,073][HYDRA] 	#370 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 20:18:27,351][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:18:27,354][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:18:27,355][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:18:27,356][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:18:27,359][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:18:27,360][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:18:27,361][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:18:27,361][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:18:27,363][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:18:27,364][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:18:27,364][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:18:27,366][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:18:27,408][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.18it/s v_num: 1.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.024 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 20:19:54,643][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/0>
[2024-05-30 20:19:54,644][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:19:54,648][HYDRA] 	#371 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 20:19:54,939][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:19:54,941][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:19:54,943][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:19:54,943][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:19:54,946][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:19:54,947][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:19:54,948][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:19:54,949][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:19:54,951][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:19:54,955][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:19:54,955][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:19:54,957][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:19:55,070][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.03it/s v_num: 1.000      
                                                              val/auc: 0.532    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.286 val/mre:    
                                                              0.023 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 20:21:24,589][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/1>
[2024-05-30 20:21:24,590][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:21:24,594][HYDRA] 	#372 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 20:21:24,889][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:21:24,892][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:21:24,894][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:21:24,895][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:21:24,899][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:21:24,900][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:21:24,901][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:21:24,901][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:21:24,903][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:21:24,906][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:21:24,906][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:21:24,908][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:21:24,993][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.06it/s v_num: 1.000      
                                                              val/auc: 0.548    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.429 val/mre:    
                                                              0.023 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 20:22:54,411][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/2>
[2024-05-30 20:22:54,412][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:22:54,415][HYDRA] 	#373 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 20:22:54,707][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:22:54,710][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:22:54,712][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:22:54,712][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:22:54,716][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:22:54,717][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:22:54,718][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:22:54,718][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:22:54,720][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:22:54,724][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:22:54,724][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:22:54,726][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:22:54,813][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.07it/s v_num: 1.000      
                                                              val/auc: 0.548    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.429 val/mre:    
                                                              0.022 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 20:24:23,269][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/3>
[2024-05-30 20:24:23,270][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:24:23,273][HYDRA] 	#374 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 20:24:23,561][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:24:23,564][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:24:23,566][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:24:23,567][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:24:23,570][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:24:23,571][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:24:23,572][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:24:23,573][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:24:23,575][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:24:23,577][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:24:23,578][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:24:23,580][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:24:23,666][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.00it/s v_num: 1.000      
                                                              val/auc: 0.532    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.286 val/mre:    
                                                              0.024 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 20:25:53,052][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/4>
[2024-05-30 20:25:53,053][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:25:53,056][HYDRA] 	#375 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 20:25:53,352][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:25:53,354][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:25:53,356][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:25:53,356][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:25:53,360][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:25:53,361][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:25:53,361][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:25:53,362][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:25:53,364][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:25:53,367][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:25:53,368][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:25:53,370][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:25:53,457][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.03it/s v_num: 1.000      
                                                              val/auc: 0.587    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.286 val/mre:    
                                                              0.025 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 20:27:22,223][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/5>
[2024-05-30 20:27:22,223][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:27:22,226][HYDRA] 	#376 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 20:27:22,506][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:27:22,509][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:27:22,510][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:27:22,511][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:27:22,514][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:27:22,515][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:27:22,515][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:27:22,516][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:27:22,518][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:27:22,519][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:27:22,519][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:27:22,521][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:27:22,564][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.07it/s v_num: 1.000      
                                                              val/auc: 0.532    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.286 val/mre:    
                                                              0.027 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 20:28:51,546][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/6>
[2024-05-30 20:28:51,547][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:28:51,550][HYDRA] 	#377 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 20:28:51,828][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:28:51,830][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:28:51,832][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:28:51,832][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:28:51,836][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:28:51,836][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:28:51,837][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:28:51,838][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:28:51,840][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:28:51,841][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:28:51,841][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:28:51,843][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:28:51,887][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.12it/s v_num: 1.000      
                                                              val/auc: 0.690    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.714 val/mre:    
                                                              0.023 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 20:30:20,375][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/7>
[2024-05-30 20:30:20,376][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:30:20,379][HYDRA] 	#378 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 20:30:20,675][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:30:20,678][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:30:20,680][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:30:20,680][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:30:20,684][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:30:20,685][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:30:20,685][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:30:20,686][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:30:20,688][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:30:20,691][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:30:20,691][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:30:20,694][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:30:20,781][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.92it/s v_num: 1.000      
                                                              val/auc: 0.548    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.429 val/mre:    
                                                              0.060 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.049             
[2024-05-30 20:31:50,553][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/8>
[2024-05-30 20:31:50,554][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:31:50,557][HYDRA] 	#379 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 20:31:50,853][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:31:50,855][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:31:50,857][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:31:50,857][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:31:50,861][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:31:50,861][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:31:50,862][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:31:50,863][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:31:50,865][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:31:50,868][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:31:50,868][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:31:50,870][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:31:50,958][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.02it/s v_num: 1.000      
                                                              val/auc: 0.587    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.286 val/mre:    
                                                              0.021 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 20:33:19,824][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.1/9>
[2024-05-30 20:33:19,825][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:33:19,828][HYDRA] 	#380 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 20:33:20,120][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:33:20,122][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:33:20,124][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:33:20,124][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:33:20,128][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:33:20,129][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:33:20,129][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:33:20,130][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:33:20,132][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:33:20,135][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:33:20,135][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:33:20,137][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:33:20,225][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.21it/s v_num: 1.000      
                                                              val/auc: 0.476    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.286 val/mre:    
                                                              0.061 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.037             
[2024-05-30 20:34:46,453][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/0>
[2024-05-30 20:34:46,453][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:34:46,457][HYDRA] 	#381 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 20:34:46,745][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:34:46,748][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:34:46,749][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:34:46,750][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:34:46,753][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:34:46,754][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:34:46,755][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:34:46,756][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:34:46,757][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:34:46,761][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:34:46,761][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:34:46,763][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:34:46,849][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.03it/s v_num: 1.000      
                                                              val/auc: 0.476    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.286 val/mre:    
                                                              0.033 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.015             
[2024-05-30 20:36:16,237][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/1>
[2024-05-30 20:36:16,237][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:36:16,241][HYDRA] 	#382 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 20:36:16,527][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:36:16,529][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:36:16,531][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:36:16,531][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:36:16,534][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:36:16,535][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:36:16,536][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:36:16,537][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:36:16,539][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:36:16,542][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:36:16,542][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:36:16,544][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:36:16,631][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.07it/s v_num: 1.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.024 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 20:37:46,295][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/2>
[2024-05-30 20:37:46,295][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:37:46,299][HYDRA] 	#383 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 20:37:46,584][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:37:46,587][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:37:46,588][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:37:46,589][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:37:46,592][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:37:46,593][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:37:46,594][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:37:46,594][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:37:46,596][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:37:46,597][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:37:46,597][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:37:46,599][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:37:46,643][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.09it/s v_num: 1.000      
                                                              val/auc: 0.675    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.571 val/mre:    
                                                              0.022 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 20:39:16,390][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/3>
[2024-05-30 20:39:16,391][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:39:16,396][HYDRA] 	#384 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 20:39:16,674][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:39:16,677][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:39:16,678][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:39:16,679][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:39:16,682][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:39:16,683][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:39:16,683][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:39:16,684][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:39:16,686][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:39:16,687][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:39:16,687][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:39:16,689][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:39:16,731][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.00it/s v_num: 1.000      
                                                              val/auc: 0.532    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.286 val/mre:    
                                                              0.075 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.058             
[2024-05-30 20:40:45,572][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/4>
[2024-05-30 20:40:45,572][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:40:45,576][HYDRA] 	#385 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 20:40:45,874][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:40:45,876][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:40:45,878][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:40:45,878][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:40:45,882][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:40:45,883][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:40:45,883][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:40:45,884][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:40:45,886][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:40:45,889][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:40:45,889][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:40:45,891][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:40:45,979][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.91it/s v_num: 1.000      
                                                              val/auc: 0.730    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.571 val/mre:    
                                                              0.025 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.014             
[2024-05-30 20:42:18,415][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/5>
[2024-05-30 20:42:18,416][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:42:18,419][HYDRA] 	#386 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 20:42:18,716][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:42:18,719][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:42:18,720][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:42:18,721][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:42:18,725][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:42:18,726][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:42:18,726][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:42:18,727][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:42:18,729][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:42:18,732][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:42:18,733][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:42:18,735][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:42:18,821][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.08it/s v_num: 1.000      
                                                              val/auc: 0.476    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.286 val/mre:    
                                                              0.021 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.008             
[2024-05-30 20:43:49,006][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/6>
[2024-05-30 20:43:49,007][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:43:49,010][HYDRA] 	#387 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 20:43:49,308][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:43:49,311][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:43:49,312][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:43:49,313][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:43:49,316][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:43:49,317][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:43:49,318][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:43:49,318][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:43:49,320][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:43:49,324][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:43:49,324][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:43:49,326][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:43:49,417][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.19it/s v_num: 1.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.024 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 20:45:19,591][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/7>
[2024-05-30 20:45:19,592][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:45:19,595][HYDRA] 	#388 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 20:45:19,888][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:45:19,891][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:45:19,893][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:45:19,893][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:45:19,897][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:45:19,898][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:45:19,899][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:45:19,899][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:45:19,901][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:45:19,904][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:45:19,905][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:45:19,907][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:45:19,991][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.99it/s v_num: 1.000      
                                                              val/auc: 0.619    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.571 val/mre:    
                                                              0.024 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 20:46:50,759][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/8>
[2024-05-30 20:46:50,760][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:46:50,763][HYDRA] 	#389 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 20:46:51,086][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:46:51,090][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:46:51,093][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:46:51,093][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:46:51,097][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:46:51,098][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:46:51,098][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:46:51,099][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:46:51,101][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:46:51,105][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:46:51,105][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:46:51,107][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:46:51,248][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.04it/s v_num: 1.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.022 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 20:48:19,931][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.15/9>
[2024-05-30 20:48:19,932][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:48:19,935][HYDRA] 	#390 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 20:48:20,214][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:48:20,217][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:48:20,218][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:48:20,219][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:48:20,222][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:48:20,223][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:48:20,223][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:48:20,224][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:48:20,226][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:48:20,227][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:48:20,227][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:48:20,229][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:48:20,273][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.24it/s v_num: 1.000      
                                                              val/auc: 0.405    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.143 val/mre:    
                                                              0.026 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 20:49:47,086][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/0>
[2024-05-30 20:49:47,093][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:49:47,109][HYDRA] 	#391 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 20:49:47,429][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:49:47,431][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:49:47,433][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:49:47,433][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:49:47,438][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:49:47,439][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:49:47,440][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:49:47,441][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:49:47,443][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:49:47,444][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:49:47,444][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:49:47,446][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:49:47,545][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.96it/s v_num: 1.000      
                                                              val/auc: 0.532    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.286 val/mre:    
                                                              0.022 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 20:51:18,413][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/1>
[2024-05-30 20:51:18,414][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:51:18,418][HYDRA] 	#392 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 20:51:18,707][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:51:18,710][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:51:18,711][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:51:18,712][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:51:18,715][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:51:18,716][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:51:18,717][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:51:18,718][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:51:18,719][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:51:18,722][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:51:18,723][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:51:18,725][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:51:18,811][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.91it/s v_num: 1.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.023 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 20:52:50,017][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/2>
[2024-05-30 20:52:50,018][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:52:50,021][HYDRA] 	#393 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 20:52:50,315][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:52:50,318][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:52:50,320][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:52:50,320][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:52:50,325][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:52:50,326][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:52:50,326][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:52:50,327][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:52:50,329][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:52:50,332][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:52:50,332][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:52:50,334][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:52:50,420][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.02it/s v_num: 1.000      
                                                              val/auc: 0.619    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.571 val/mre:    
                                                              0.025 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 20:54:20,187][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/3>
[2024-05-30 20:54:20,188][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:54:20,192][HYDRA] 	#394 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 20:54:20,503][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:54:20,506][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:54:20,507][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:54:20,508][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:54:20,511][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:54:20,512][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:54:20,513][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:54:20,513][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:54:20,515][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:54:20,520][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:54:20,521][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:54:20,523][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:54:20,611][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.11it/s v_num: 1.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.024 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 20:55:48,628][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/4>
[2024-05-30 20:55:48,629][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:55:48,633][HYDRA] 	#395 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 20:55:48,927][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:55:48,930][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:55:48,932][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:55:48,932][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:55:48,936][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:55:48,937][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:55:48,938][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:55:48,939][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:55:48,940][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:55:48,943][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:55:48,944][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:55:48,946][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:55:49,037][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.90it/s v_num: 1.000      
                                                              val/auc: 0.619    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.571 val/mre:    
                                                              0.086 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.069             
[2024-05-30 20:57:19,275][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/5>
[2024-05-30 20:57:19,275][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:57:19,278][HYDRA] 	#396 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 20:57:19,569][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:57:19,571][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:57:19,573][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:57:19,573][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:57:19,577][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:57:19,578][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:57:19,578][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:57:19,579][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:57:19,581][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:57:19,604][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:57:19,604][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:57:19,609][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:57:19,718][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.02it/s v_num: 1.000      
                                                              val/auc: 0.476    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.286 val/mre:    
                                                              0.024 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 20:58:50,045][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/6>
[2024-05-30 20:58:50,046][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 20:58:50,049][HYDRA] 	#397 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 20:58:50,331][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 20:58:50,334][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 20:58:50,335][train.py][INFO] - Instantiating callbacks...
[2024-05-30 20:58:50,336][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 20:58:50,339][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 20:58:50,340][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 20:58:50,341][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 20:58:50,341][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 20:58:50,343][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 20:58:50,344][train.py][INFO] - Instantiating loggers...
[2024-05-30 20:58:50,344][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 20:58:50,346][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 20:58:50,390][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.22it/s v_num: 1.000      
                                                              val/auc: 0.746    
                                                              val/f1: 0.714     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.714 val/mre:    
                                                              0.025 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 21:00:18,844][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/7>
[2024-05-30 21:00:18,845][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:00:18,848][HYDRA] 	#398 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 21:00:19,126][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:00:19,129][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:00:19,131][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:00:19,131][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:00:19,134][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:00:19,135][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:00:19,136][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:00:19,137][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:00:19,138][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:00:19,139][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:00:19,140][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:00:19,142][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:00:19,185][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.96it/s v_num: 1.000      
                                                              val/auc: 0.437    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.429 val/mre:    
                                                              0.087 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.070             
[2024-05-30 21:01:49,232][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/8>
[2024-05-30 21:01:49,232][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:01:49,236][HYDRA] 	#399 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 21:01:49,533][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:01:49,536][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:01:49,537][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:01:49,538][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:01:49,541][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:01:49,542][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:01:49,543][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:01:49,543][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:01:49,545][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:01:49,549][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:01:49,549][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:01:49,551][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:01:49,659][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.02it/s v_num: 1.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.028 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 21:03:19,061][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.2/9>
[2024-05-30 21:03:19,062][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:03:19,065][HYDRA] 	#400 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 21:03:19,354][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:03:19,356][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:03:19,358][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:03:19,358][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:03:19,362][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:03:19,362][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:03:19,363][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:03:19,364][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:03:19,366][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:03:19,369][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:03:19,369][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:03:19,371][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:03:19,457][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.20it/s v_num: 1.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.027 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 21:04:45,884][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/0>
[2024-05-30 21:04:45,884][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:04:45,888][HYDRA] 	#401 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 21:04:46,193][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:04:46,195][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:04:46,197][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:04:46,197][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:04:46,201][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:04:46,202][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:04:46,202][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:04:46,203][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:04:46,205][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:04:46,208][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:04:46,208][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:04:46,210][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:04:46,299][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.98it/s v_num: 1.000      
                                                              val/auc: 0.476    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.286 val/mre:    
                                                              0.023 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 21:06:17,216][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/1>
[2024-05-30 21:06:17,217][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:06:17,220][HYDRA] 	#402 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 21:06:17,512][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:06:17,514][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:06:17,516][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:06:17,516][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:06:17,519][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:06:17,520][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:06:17,521][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:06:17,522][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:06:17,524][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:06:17,527][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:06:17,527][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:06:17,529][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:06:17,617][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.97it/s v_num: 1.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.028 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 21:07:47,826][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/2>
[2024-05-30 21:07:47,826][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:07:47,830][HYDRA] 	#403 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 21:07:48,124][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:07:48,127][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:07:48,128][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:07:48,129][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:07:48,133][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:07:48,134][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:07:48,135][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:07:48,135][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:07:48,137][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:07:48,141][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:07:48,141][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:07:48,143][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:07:48,232][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.08it/s v_num: 1.000      
                                                              val/auc: 0.619    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.571 val/mre:    
                                                              0.025 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 21:09:17,054][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/3>
[2024-05-30 21:09:17,054][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:09:17,058][HYDRA] 	#404 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 21:09:17,339][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:09:17,341][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:09:17,343][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:09:17,343][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:09:17,347][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:09:17,347][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:09:17,348][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:09:17,349][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:09:17,351][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:09:17,352][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:09:17,352][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:09:17,354][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:09:17,397][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.06it/s v_num: 1.000      
                                                              val/auc: 0.548    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.429 val/mre:    
                                                              0.024 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 21:10:46,558][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/4>
[2024-05-30 21:10:46,558][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:10:46,562][HYDRA] 	#405 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 21:10:46,840][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:10:46,842][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:10:46,844][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:10:46,844][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:10:46,847][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:10:46,848][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:10:46,849][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:10:46,849][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:10:46,851][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:10:46,852][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:10:46,852][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:10:46,854][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:10:46,897][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.94it/s v_num: 1.000      
                                                              val/auc: 0.675    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.571 val/mre:    
                                                              0.028 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 21:12:16,623][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/5>
[2024-05-30 21:12:16,623][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:12:16,627][HYDRA] 	#406 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 21:12:16,919][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:12:16,922][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:12:16,924][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:12:16,925][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:12:16,928][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:12:16,929][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:12:16,930][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:12:16,931][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:12:16,932][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:12:16,936][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:12:16,936][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:12:16,938][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:12:17,027][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.07it/s v_num: 1.000      
                                                              val/auc: 0.421    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.286 val/mre:    
                                                              0.024 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 21:13:47,503][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/6>
[2024-05-30 21:13:47,503][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:13:47,507][HYDRA] 	#407 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 21:13:47,805][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:13:47,807][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:13:47,809][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:13:47,809][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:13:47,813][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:13:47,814][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:13:47,814][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:13:47,815][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:13:47,817][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:13:47,820][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:13:47,820][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:13:47,823][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:13:47,909][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.06it/s v_num: 1.000      
                                                              val/auc: 0.746    
                                                              val/f1: 0.714     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.714 val/mre:    
                                                              0.025 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 21:15:16,995][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/7>
[2024-05-30 21:15:16,996][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:15:16,999][HYDRA] 	#408 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 21:15:17,291][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:15:17,294][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:15:17,295][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:15:17,296][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:15:17,299][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:15:17,300][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:15:17,300][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:15:17,301][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:15:17,303][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:15:17,309][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:15:17,309][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:15:17,311][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:15:17,404][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.07it/s v_num: 1.000      
                                                              val/auc: 0.548    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.429 val/mre:    
                                                              0.090 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.074             
[2024-05-30 21:16:46,030][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/8>
[2024-05-30 21:16:46,031][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:16:46,034][HYDRA] 	#409 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 21:16:46,326][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:16:46,329][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:16:46,330][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:16:46,331][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:16:46,334][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:16:46,335][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:16:46,336][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:16:46,336][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:16:46,338][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:16:46,341][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:16:46,342][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:16:46,344][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:16:46,444][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.95it/s v_num: 1.000      
                                                              val/auc: 0.476    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.286 val/mre:    
                                                              0.091 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.075             
[2024-05-30 21:18:16,655][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.25/9>
[2024-05-30 21:18:16,655][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:18:16,659][HYDRA] 	#410 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 21:18:16,944][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:18:16,947][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:18:16,948][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:18:16,949][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:18:16,952][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:18:16,953][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:18:16,954][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:18:16,954][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:18:16,956][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:18:16,959][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:18:16,960][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:18:16,962][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:18:17,044][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.23it/s v_num: 1.000      
                                                              val/auc: 0.492    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.429 val/mre:    
                                                              0.026 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 21:19:44,435][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/0>
[2024-05-30 21:19:44,436][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:19:44,440][HYDRA] 	#411 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 21:19:44,739][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:19:44,742][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:19:44,744][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:19:44,744][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:19:44,747][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:19:44,748][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:19:44,749][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:19:44,750][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:19:44,752][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:19:44,755][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:19:44,755][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:19:44,757][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:19:44,874][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.05it/s v_num: 1.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.023 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 21:21:13,951][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/1>
[2024-05-30 21:21:13,952][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:21:13,956][HYDRA] 	#412 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 21:21:14,245][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:21:14,248][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:21:14,249][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:21:14,250][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:21:14,253][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:21:14,254][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:21:14,255][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:21:14,255][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:21:14,257][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:21:14,259][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:21:14,259][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:21:14,261][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:21:14,306][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.01it/s v_num: 1.000      
                                                              val/auc: 0.508    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.571 val/mre:    
                                                              0.035 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.012             
[2024-05-30 21:22:44,705][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/2>
[2024-05-30 21:22:44,706][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:22:44,709][HYDRA] 	#413 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 21:22:44,988][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:22:44,990][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:22:44,992][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:22:44,992][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:22:44,996][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:22:44,996][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:22:44,997][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:22:44,998][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:22:45,000][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:22:45,001][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:22:45,001][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:22:45,003][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:22:45,045][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.10it/s v_num: 1.000      
                                                              val/auc: 0.548    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.429 val/mre:    
                                                              0.057 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.041             
[2024-05-30 21:24:15,218][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/3>
[2024-05-30 21:24:15,218][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:24:15,222][HYDRA] 	#414 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 21:24:15,618][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:24:15,621][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:24:15,622][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:24:15,623][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:24:15,636][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:24:15,637][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:24:15,639][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:24:15,640][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:24:15,644][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:24:15,645][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:24:15,646][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:24:15,648][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:24:15,739][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.07it/s v_num: 1.000      
                                                              val/auc: 0.619    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.571 val/mre:    
                                                              0.026 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 21:25:45,894][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/4>
[2024-05-30 21:25:45,894][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:25:45,899][HYDRA] 	#415 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 21:25:46,196][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:25:46,199][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:25:46,201][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:25:46,201][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:25:46,205][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:25:46,205][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:25:46,206][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:25:46,207][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:25:46,209][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:25:46,212][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:25:46,212][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:25:46,214][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:25:46,303][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.15it/s v_num: 1.000      
                                                              val/auc: 0.619    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.571 val/mre:    
                                                              0.027 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 21:27:14,449][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/5>
[2024-05-30 21:27:14,450][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:27:14,453][HYDRA] 	#416 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 21:27:14,747][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:27:14,750][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:27:14,751][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:27:14,752][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:27:14,755][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:27:14,756][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:27:14,756][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:27:14,757][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:27:14,759][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:27:14,762][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:27:14,762][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:27:14,764][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:27:14,852][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.12it/s v_num: 1.000      
                                                              val/auc: 0.675    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.571 val/mre:    
                                                              0.024 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 21:28:43,891][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/6>
[2024-05-30 21:28:43,892][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:28:43,895][HYDRA] 	#417 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 21:28:44,187][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:28:44,190][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:28:44,191][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:28:44,192][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:28:44,195][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:28:44,196][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:28:44,196][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:28:44,197][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:28:44,199][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:28:44,202][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:28:44,202][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:28:44,205][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:28:44,293][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.11it/s v_num: 1.000      
                                                              val/auc: 0.762    
                                                              val/f1: 0.750     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.857 val/mre:    
                                                              0.025 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 21:30:14,153][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/7>
[2024-05-30 21:30:14,154][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:30:14,157][HYDRA] 	#418 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 21:30:14,455][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:30:14,458][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:30:14,459][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:30:14,460][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:30:14,463][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:30:14,464][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:30:14,465][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:30:14,465][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:30:14,467][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:30:14,470][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:30:14,471][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:30:14,473][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:30:14,559][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.12it/s v_num: 1.000      
                                                              val/auc: 0.492    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.429 val/mre:    
                                                              0.028 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 21:31:43,723][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/8>
[2024-05-30 21:31:43,724][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:31:43,727][HYDRA] 	#419 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 21:31:44,023][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:31:44,026][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:31:44,028][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:31:44,028][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:31:44,033][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:31:44,033][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:31:44,034][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:31:44,035][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:31:44,037][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:31:44,040][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:31:44,040][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:31:44,042][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:31:44,148][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.08it/s v_num: 1.000      
                                                              val/auc: 0.532    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.286 val/mre:    
                                                              0.027 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 21:33:12,357][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/diagnosed_p_t_s_d/0.3/9>
[2024-05-30 21:33:12,358][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:33:12,362][HYDRA] 	#420 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 21:33:12,645][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:33:12,647][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:33:12,649][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:33:12,649][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:33:12,652][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:33:12,653][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:33:12,654][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:33:12,655][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:33:12,657][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:33:12,660][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:33:12,660][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:33:12,662][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:33:12,752][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.10it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 21:34:44,107][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/0>
[2024-05-30 21:34:44,108][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:34:44,111][HYDRA] 	#421 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 21:34:44,391][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:34:44,394][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:34:44,395][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:34:44,396][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:34:44,399][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:34:44,400][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:34:44,401][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:34:44,401][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:34:44,403][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:34:44,404][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:34:44,404][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:34:44,406][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:34:44,450][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.96it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 21:36:18,153][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/1>
[2024-05-30 21:36:18,153][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:36:18,157][HYDRA] 	#422 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 21:36:18,436][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:36:18,439][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:36:18,440][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:36:18,441][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:36:18,444][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:36:18,445][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:36:18,446][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:36:18,446][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:36:18,448][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:36:18,449][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:36:18,449][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:36:18,451][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:36:18,494][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.00it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 21:37:51,807][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/2>
[2024-05-30 21:37:51,808][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:37:51,811][HYDRA] 	#423 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 21:37:52,093][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:37:52,096][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:37:52,097][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:37:52,098][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:37:52,101][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:37:52,102][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:37:52,103][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:37:52,103][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:37:52,105][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:37:52,106][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:37:52,106][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:37:52,108][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:37:52,151][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.93it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 21:39:25,325][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/3>
[2024-05-30 21:39:25,326][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:39:25,334][HYDRA] 	#424 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 21:39:25,625][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:39:25,627][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:39:25,629][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:39:25,629][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:39:25,632][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:39:25,633][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:39:25,634][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:39:25,635][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:39:25,636][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:39:25,637][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:39:25,638][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:39:25,640][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:39:26,151][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.93it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 21:40:59,836][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/4>
[2024-05-30 21:40:59,836][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:40:59,839][HYDRA] 	#425 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 21:41:00,132][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:41:00,135][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:41:00,136][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:41:00,137][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:41:00,140][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:41:00,141][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:41:00,142][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:41:00,142][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:41:00,144][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:41:00,148][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:41:00,148][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:41:00,150][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:41:00,237][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.86it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 21:42:33,757][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/5>
[2024-05-30 21:42:33,758][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:42:33,762][HYDRA] 	#426 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 21:42:34,149][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:42:34,151][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:42:34,153][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:42:34,153][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:42:34,157][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:42:34,158][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:42:34,158][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:42:34,159][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:42:34,161][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:42:34,165][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:42:34,165][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:42:34,167][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:42:34,378][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.90it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 21:44:09,718][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/6>
[2024-05-30 21:44:09,719][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:44:09,722][HYDRA] 	#427 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 21:44:10,016][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:44:10,019][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:44:10,021][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:44:10,021][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:44:10,025][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:44:10,026][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:44:10,026][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:44:10,027][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:44:10,029][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:44:10,033][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:44:10,033][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:44:10,035][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:44:10,123][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.02it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 21:45:43,044][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/7>
[2024-05-30 21:45:43,045][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:45:43,048][HYDRA] 	#428 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 21:45:43,346][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:45:43,348][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:45:43,350][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:45:43,350][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:45:43,353][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:45:43,354][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:45:43,355][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:45:43,356][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:45:43,358][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:45:43,361][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:45:43,361][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:45:43,364][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:45:43,455][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.98it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 21:47:15,698][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/8>
[2024-05-30 21:47:15,698][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:47:15,702][HYDRA] 	#429 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 21:47:15,997][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:47:15,999][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:47:16,001][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:47:16,001][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:47:16,004][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:47:16,005][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:47:16,006][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:47:16,007][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:47:16,009][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:47:16,012][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:47:16,012][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:47:16,014][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:47:16,104][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.13it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 1.000  
                                                              train/f1: 1.000   
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 21:48:47,641][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.0/9>
[2024-05-30 21:48:47,641][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:48:47,645][HYDRA] 	#430 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 21:48:47,931][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:48:47,933][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:48:47,935][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:48:47,935][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:48:47,938][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:48:47,939][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:48:47,940][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:48:47,941][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:48:47,943][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:48:47,946][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:48:47,946][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:48:47,948][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:48:48,034][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.07it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.037 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 21:50:18,813][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/0>
[2024-05-30 21:50:18,813][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:50:18,817][HYDRA] 	#431 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 21:50:19,101][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:50:19,104][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:50:19,105][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:50:19,106][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:50:19,109][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:50:19,110][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:50:19,111][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:50:19,111][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:50:19,113][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:50:19,116][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:50:19,116][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:50:19,118][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:50:19,207][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.87it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.037 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 21:51:53,864][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/1>
[2024-05-30 21:51:53,865][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:51:53,868][HYDRA] 	#432 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 21:51:54,158][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:51:54,161][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:51:54,162][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:51:54,163][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:51:54,166][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:51:54,167][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:51:54,167][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:51:54,168][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:51:54,170][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:51:54,173][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:51:54,174][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:51:54,176][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:51:54,268][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.84it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.035 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 21:53:29,565][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/2>
[2024-05-30 21:53:29,565][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:53:29,569][HYDRA] 	#433 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 21:53:29,851][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:53:29,854][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:53:29,855][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:53:29,856][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:53:29,859][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:53:29,860][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:53:29,861][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:53:29,862][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:53:29,863][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:53:29,865][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:53:29,866][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:53:29,868][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:53:29,950][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.94it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.032 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 21:55:03,162][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/3>
[2024-05-30 21:55:03,162][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:55:03,166][HYDRA] 	#434 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 21:55:03,446][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:55:03,448][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:55:03,450][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:55:03,450][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:55:03,454][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:55:03,454][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:55:03,455][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:55:03,456][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:55:03,458][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:55:03,458][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:55:03,459][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:55:03,461][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:55:03,504][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.98it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.033 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 21:56:35,885][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/4>
[2024-05-30 21:56:35,886][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:56:35,889][HYDRA] 	#435 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 21:56:36,166][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:56:36,168][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:56:36,170][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:56:36,170][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:56:36,174][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:56:36,174][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:56:36,175][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:56:36,176][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:56:36,178][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:56:36,179][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:56:36,179][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:56:36,181][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:56:36,224][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.88it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.035 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 21:58:10,048][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/5>
[2024-05-30 21:58:10,049][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:58:10,052][HYDRA] 	#436 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 21:58:10,332][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:58:10,335][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:58:10,336][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:58:10,337][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:58:10,340][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:58:10,341][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:58:10,342][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:58:10,343][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:58:10,344][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:58:10,345][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:58:10,346][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:58:10,348][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:58:10,390][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.98it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.033 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 21:59:44,275][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/6>
[2024-05-30 21:59:44,275][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 21:59:44,279][HYDRA] 	#437 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 21:59:44,557][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 21:59:44,560][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 21:59:44,561][train.py][INFO] - Instantiating callbacks...
[2024-05-30 21:59:44,562][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 21:59:44,565][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 21:59:44,566][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 21:59:44,567][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 21:59:44,567][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 21:59:44,569][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 21:59:44,570][train.py][INFO] - Instantiating loggers...
[2024-05-30 21:59:44,570][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 21:59:44,572][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 21:59:44,614][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.96it/s v_num: 1.000      
                                                              val/auc: 0.300    
                                                              val/f1: 0.125     
                                                              val/precision:    
                                                              0.167 val/recall: 
                                                              0.100 val/mre:    
                                                              0.036 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 22:01:17,509][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/7>
[2024-05-30 22:01:17,510][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:01:17,514][HYDRA] 	#438 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 22:01:17,790][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:01:17,792][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:01:17,794][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:01:17,794][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:01:17,797][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:01:17,798][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:01:17,799][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:01:17,800][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:01:17,801][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:01:17,802][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:01:17,803][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:01:17,805][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:01:17,846][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.95it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.036 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 22:02:51,969][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/8>
[2024-05-30 22:02:51,970][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:02:51,972][HYDRA] 	#439 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 22:02:52,268][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:02:52,271][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:02:52,273][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:02:52,273][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:02:52,276][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:02:52,277][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:02:52,278][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:02:52,279][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:02:52,281][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:02:52,284][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:02:52,284][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:02:52,286][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:02:52,372][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.04it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.041 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 22:04:24,681][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.05/9>
[2024-05-30 22:04:24,682][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:04:24,686][HYDRA] 	#440 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 22:04:25,056][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:04:25,058][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:04:25,060][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:04:25,060][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:04:25,064][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:04:25,065][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:04:25,065][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:04:25,066][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:04:25,068][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:04:25,071][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:04:25,071][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:04:25,073][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:04:25,274][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.11it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.036 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 22:05:57,058][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/0>
[2024-05-30 22:05:57,058][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:05:57,063][HYDRA] 	#441 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 22:05:57,358][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:05:57,361][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:05:57,362][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:05:57,363][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:05:57,366][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:05:57,367][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:05:57,368][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:05:57,368][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:05:57,370][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:05:57,374][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:05:57,374][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:05:57,376][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:05:57,469][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.99it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.041 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 22:07:30,901][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/1>
[2024-05-30 22:07:30,902][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:07:30,905][HYDRA] 	#442 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 22:07:31,197][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:07:31,200][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:07:31,201][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:07:31,202][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:07:31,205][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:07:31,206][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:07:31,206][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:07:31,207][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:07:31,209][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:07:31,226][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:07:31,227][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:07:31,230][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:07:31,326][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.89it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.034 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 22:09:06,637][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/2>
[2024-05-30 22:09:06,641][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:09:06,659][HYDRA] 	#443 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 22:09:06,995][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:09:06,997][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:09:06,999][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:09:06,999][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:09:07,003][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:09:07,004][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:09:07,004][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:09:07,005][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:09:07,007][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:09:07,010][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:09:07,010][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:09:07,012][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:09:07,101][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.02it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.033 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 22:10:40,015][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/3>
[2024-05-30 22:10:40,016][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:10:40,019][HYDRA] 	#444 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 22:10:40,310][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:10:40,313][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:10:40,314][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:10:40,315][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:10:40,320][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:10:40,321][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:10:40,321][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:10:40,322][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:10:40,324][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:10:40,327][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:10:40,327][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:10:40,329][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:10:40,420][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.99it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.036 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 22:12:13,851][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/4>
[2024-05-30 22:12:13,851][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:12:13,859][HYDRA] 	#445 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 22:12:14,154][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:12:14,157][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:12:14,159][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:12:14,159][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:12:14,163][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:12:14,164][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:12:14,165][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:12:14,165][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:12:14,167][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:12:14,171][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:12:14,172][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:12:14,174][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:12:14,279][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.01it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.038 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 22:13:47,917][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/5>
[2024-05-30 22:13:47,918][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:13:47,921][HYDRA] 	#446 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 22:13:48,211][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:13:48,214][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:13:48,216][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:13:48,216][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:13:48,219][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:13:48,220][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:13:48,221][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:13:48,222][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:13:48,223][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:13:48,227][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:13:48,227][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:13:48,229][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:13:48,316][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.96it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.035 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 22:15:22,519][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/6>
[2024-05-30 22:15:22,520][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:15:22,531][HYDRA] 	#447 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 22:15:22,847][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:15:22,849][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:15:22,851][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:15:22,851][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:15:22,855][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:15:22,856][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:15:22,857][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:15:22,857][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:15:22,859][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:15:22,860][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:15:22,860][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:15:22,862][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:15:22,904][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.01it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.038 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 22:16:55,921][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/7>
[2024-05-30 22:16:55,921][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:16:55,925][HYDRA] 	#448 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 22:16:56,205][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:16:56,207][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:16:56,209][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:16:56,209][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:16:56,212][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:16:56,213][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:16:56,214][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:16:56,215][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:16:56,216][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:16:56,217][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:16:56,218][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:16:56,220][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:16:56,263][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.97it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.038 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 22:18:29,702][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/8>
[2024-05-30 22:18:29,702][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:18:29,706][HYDRA] 	#449 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 22:18:29,985][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:18:29,987][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:18:29,989][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:18:29,989][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:18:29,992][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:18:29,993][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:18:29,994][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:18:29,995][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:18:29,996][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:18:29,997][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:18:29,998][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:18:30,000][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:18:30,042][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.11it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.034 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 22:20:00,994][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.1/9>
[2024-05-30 22:20:00,995][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:20:00,997][HYDRA] 	#450 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 22:20:01,276][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:20:01,278][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:20:01,280][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:20:01,280][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:20:01,283][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:20:01,284][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:20:01,285][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:20:01,286][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:20:01,288][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:20:01,288][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:20:01,289][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:20:01,291][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:20:01,332][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.16it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.037 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 22:21:31,602][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/0>
[2024-05-30 22:21:31,603][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:21:31,606][HYDRA] 	#451 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 22:21:31,901][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:21:31,903][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:21:31,905][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:21:31,905][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:21:31,909][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:21:31,910][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:21:31,910][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:21:31,911][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:21:31,913][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:21:31,918][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:21:31,918][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:21:31,920][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:21:32,006][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.01it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.037 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 22:23:05,677][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/1>
[2024-05-30 22:23:05,678][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:23:05,682][HYDRA] 	#452 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 22:23:05,974][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:23:05,977][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:23:05,978][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:23:05,979][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:23:05,983][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:23:05,984][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:23:05,985][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:23:05,985][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:23:05,987][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:23:05,990][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:23:05,990][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:23:05,992][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:23:06,096][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.78it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.035 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 22:24:41,210][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/2>
[2024-05-30 22:24:41,211][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:24:41,215][HYDRA] 	#453 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 22:24:41,512][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:24:41,514][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:24:41,516][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:24:41,516][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:24:41,520][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:24:41,521][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:24:41,521][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:24:41,522][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:24:41,524][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:24:41,527][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:24:41,527][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:24:41,529][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:24:41,612][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.10it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.035 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 22:26:13,746][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/3>
[2024-05-30 22:26:13,747][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:26:13,750][HYDRA] 	#454 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 22:26:14,043][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:26:14,046][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:26:14,048][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:26:14,048][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:26:14,052][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:26:14,053][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:26:14,054][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:26:14,056][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:26:14,058][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:26:14,083][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:26:14,083][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:26:14,087][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:26:14,188][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.00it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.038 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 22:27:47,811][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/4>
[2024-05-30 22:27:47,812][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:27:47,815][HYDRA] 	#455 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 22:27:48,110][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:27:48,112][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:27:48,114][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:27:48,114][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:27:48,118][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:27:48,119][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:27:48,119][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:27:48,120][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:27:48,122][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:27:48,125][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:27:48,126][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:27:48,128][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:27:48,216][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.99it/s v_num: 1.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.300 val/mre:    
                                                              0.038 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 22:29:21,798][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/5>
[2024-05-30 22:29:21,798][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:29:21,802][HYDRA] 	#456 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 22:29:22,094][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:29:22,096][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:29:22,098][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:29:22,098][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:29:22,101][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:29:22,102][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:29:22,103][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:29:22,104][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:29:22,105][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:29:22,114][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:29:22,115][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:29:22,117][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:29:22,207][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.96it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.038 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 22:30:55,196][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/6>
[2024-05-30 22:30:55,197][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:30:55,202][HYDRA] 	#457 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 22:30:55,489][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:30:55,492][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:30:55,493][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:30:55,494][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:30:55,497][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:30:55,498][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:30:55,498][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:30:55,499][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:30:55,501][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:30:55,505][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:30:55,505][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:30:55,507][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:30:55,594][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.03it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.037 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 22:32:29,122][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/7>
[2024-05-30 22:32:29,123][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:32:29,127][HYDRA] 	#458 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 22:32:29,615][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:32:29,620][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:32:29,623][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:32:29,623][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:32:29,629][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:32:29,629][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:32:29,630][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:32:29,631][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:32:29,633][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:32:29,636][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:32:29,637][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:32:29,639][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:32:29,934][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.97it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.040 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.012             
[2024-05-30 22:34:03,391][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/8>
[2024-05-30 22:34:03,391][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:34:03,395][HYDRA] 	#459 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 22:34:03,684][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:34:03,686][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:34:03,688][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:34:03,688][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:34:03,692][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:34:03,693][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:34:03,693][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:34:03,694][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:34:03,696][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:34:03,699][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:34:03,699][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:34:03,701][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:34:03,788][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.07it/s v_num: 1.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.040 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 22:35:35,480][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.15/9>
[2024-05-30 22:35:35,480][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:35:35,484][HYDRA] 	#460 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 22:35:35,767][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:35:35,770][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:35:35,771][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:35:35,772][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:35:35,775][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:35:35,776][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:35:35,777][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:35:35,777][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:35:35,779][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:35:35,783][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:35:35,783][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:35:35,785][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:35:35,883][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.04it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.037 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 22:37:08,483][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/0>
[2024-05-30 22:37:08,483][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:37:08,487][HYDRA] 	#461 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 22:37:08,770][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:37:08,772][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:37:08,774][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:37:08,774][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:37:08,777][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:37:08,778][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:37:08,779][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:37:08,780][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:37:08,781][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:37:08,782][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:37:08,783][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:37:08,785][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:37:08,827][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.98it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.040 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.008             
[2024-05-30 22:38:43,048][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/1>
[2024-05-30 22:38:43,048][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:38:43,052][HYDRA] 	#462 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 22:38:43,336][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:38:43,339][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:38:43,340][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:38:43,341][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:38:43,344][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:38:43,345][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:38:43,346][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:38:43,347][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:38:43,348][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:38:43,349][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:38:43,350][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:38:43,352][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:38:43,397][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.93it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.040 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 22:40:17,486][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/2>
[2024-05-30 22:40:17,486][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:40:17,489][HYDRA] 	#463 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 22:40:17,771][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:40:17,773][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:40:17,775][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:40:17,775][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:40:17,778][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:40:17,779][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:40:17,780][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:40:17,781][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:40:17,782][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:40:17,783][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:40:17,784][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:40:17,786][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:40:17,828][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.03it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.037 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.008             
[2024-05-30 22:41:50,403][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/3>
[2024-05-30 22:41:50,403][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:41:50,407][HYDRA] 	#464 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 22:41:50,700][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:41:50,703][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:41:50,704][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:41:50,705][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:41:50,708][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:41:50,709][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:41:50,710][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:41:50,710][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:41:50,712][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:41:50,715][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:41:50,716][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:41:50,718][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:41:50,806][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.12it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.039 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 22:43:22,729][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/4>
[2024-05-30 22:43:22,730][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:43:22,733][HYDRA] 	#465 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 22:43:23,478][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:43:23,481][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:43:23,482][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:43:23,483][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:43:23,486][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:43:23,487][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:43:23,488][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:43:23,488][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:43:23,490][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:43:23,493][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:43:23,493][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:43:23,495][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:43:23,581][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.05it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.400 val/mre:    
                                                              0.040 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 22:44:58,383][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/5>
[2024-05-30 22:44:58,383][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:44:58,386][HYDRA] 	#466 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 22:44:58,686][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:44:58,688][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:44:58,690][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:44:58,690][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:44:58,694][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:44:58,695][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:44:58,695][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:44:58,696][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:44:58,698][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:44:58,701][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:44:58,701][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:44:58,703][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:44:58,789][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.95it/s v_num: 1.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.040 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 22:46:31,422][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/6>
[2024-05-30 22:46:31,423][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:46:31,426][HYDRA] 	#467 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 22:46:31,717][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:46:31,720][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:46:31,722][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:46:31,722][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:46:31,726][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:46:31,727][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:46:31,728][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:46:31,728][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:46:31,730][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:46:31,733][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:46:31,734][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:46:31,736][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:46:31,823][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.09it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.041 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 22:48:03,325][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/7>
[2024-05-30 22:48:03,325][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:48:03,329][HYDRA] 	#468 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 22:48:03,624][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:48:03,627][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:48:03,629][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:48:03,629][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:48:03,634][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:48:03,635][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:48:03,636][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:48:03,636][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:48:03,638][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:48:03,641][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:48:03,642][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:48:03,644][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:48:03,734][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.93it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.040 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 22:49:37,785][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/8>
[2024-05-30 22:49:37,794][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:49:37,808][HYDRA] 	#469 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 22:49:38,256][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:49:38,258][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:49:38,260][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:49:38,260][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:49:38,264][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:49:38,264][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:49:38,265][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:49:38,266][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:49:38,268][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:49:38,271][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:49:38,271][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:49:38,273][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:49:38,417][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.05it/s v_num: 1.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.036 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 22:51:11,615][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.2/9>
[2024-05-30 22:51:11,616][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:51:11,619][HYDRA] 	#470 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 22:51:11,903][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:51:11,905][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:51:11,907][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:51:11,907][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:51:11,911][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:51:11,911][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:51:11,912][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:51:11,913][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:51:11,915][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:51:11,918][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:51:11,918][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:51:11,920][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:51:12,006][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.12it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.038 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 22:52:43,104][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/0>
[2024-05-30 22:52:43,105][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:52:43,108][HYDRA] 	#471 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 22:52:43,392][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:52:43,395][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:52:43,396][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:52:43,397][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:52:43,400][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:52:43,401][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:52:43,402][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:52:43,402][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:52:43,404][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:52:43,408][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:52:43,408][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:52:43,410][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:52:43,499][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.96it/s v_num: 1.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.833 val/recall: 
                                                              0.500 val/mre:    
                                                              0.038 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.008             
[2024-05-30 22:54:17,773][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/1>
[2024-05-30 22:54:17,774][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:54:17,777][HYDRA] 	#472 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 22:54:18,064][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:54:18,066][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:54:18,068][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:54:18,068][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:54:18,071][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:54:18,072][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:54:18,073][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:54:18,074][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:54:18,076][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:54:18,079][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:54:18,079][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:54:18,081][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:54:18,167][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.87it/s v_num: 1.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.040 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 22:55:52,223][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/2>
[2024-05-30 22:55:52,224][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:55:52,228][HYDRA] 	#473 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 22:55:52,509][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:55:52,512][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:55:52,513][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:55:52,514][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:55:52,517][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:55:52,518][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:55:52,518][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:55:52,519][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:55:52,521][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:55:52,522][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:55:52,522][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:55:52,525][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:55:52,568][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.09it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.039 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.008             
[2024-05-30 22:57:24,523][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/3>
[2024-05-30 22:57:24,524][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:57:24,529][HYDRA] 	#474 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 22:57:24,812][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:57:24,815][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:57:24,816][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:57:24,817][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:57:24,820][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:57:24,821][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:57:24,822][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:57:24,822][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:57:24,824][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:57:24,825][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:57:24,825][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:57:24,827][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:57:24,871][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.11it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.040 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 22:58:57,256][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/4>
[2024-05-30 22:58:57,257][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 22:58:57,261][HYDRA] 	#475 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 22:58:57,539][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 22:58:57,542][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 22:58:57,543][train.py][INFO] - Instantiating callbacks...
[2024-05-30 22:58:57,544][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 22:58:57,547][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 22:58:57,548][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 22:58:57,548][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 22:58:57,549][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 22:58:57,551][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 22:58:57,552][train.py][INFO] - Instantiating loggers...
[2024-05-30 22:58:57,552][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 22:58:57,554][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 22:58:57,596][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.93it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.476     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.500 val/mre:    
                                                              0.041 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 23:00:31,958][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/5>
[2024-05-30 23:00:31,959][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:00:31,963][HYDRA] 	#476 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 23:00:32,243][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:00:32,246][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 23:00:32,247][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:00:32,248][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:00:32,251][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:00:32,252][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:00:32,252][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:00:32,253][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:00:32,255][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:00:32,256][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:00:32,256][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:00:32,258][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:00:32,300][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.98it/s v_num: 1.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.042 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 23:02:06,382][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/6>
[2024-05-30 23:02:06,383][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:02:06,385][HYDRA] 	#477 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 23:02:06,662][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:02:06,665][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 23:02:06,666][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:02:06,667][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:02:06,670][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:02:06,671][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:02:06,672][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:02:06,672][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:02:06,674][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:02:06,675][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:02:06,675][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:02:06,677][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:02:06,721][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.10it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.042 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 23:03:38,426][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/7>
[2024-05-30 23:03:38,427][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:03:38,430][HYDRA] 	#478 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 23:03:38,726][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:03:38,729][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 23:03:38,730][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:03:38,731][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:03:38,734][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:03:38,735][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:03:38,735][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:03:38,736][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:03:38,738][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:03:38,741][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:03:38,741][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:03:38,743][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:03:38,831][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.01it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.043 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 23:05:10,518][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/8>
[2024-05-30 23:05:10,519][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:05:10,522][HYDRA] 	#479 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 23:05:10,833][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:05:10,835][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 23:05:10,837][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:05:10,837][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:05:10,843][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:05:10,844][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:05:10,845][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:05:10,845][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:05:10,847][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:05:10,856][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:05:10,856][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:05:10,858][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:05:11,000][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.01it/s v_num: 1.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.038 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 23:06:43,703][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.25/9>
[2024-05-30 23:06:43,704][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:06:43,707][HYDRA] 	#480 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-05-30 23:06:43,992][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:06:43,995][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 23:06:43,997][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:06:43,997][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:06:44,000][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:06:44,001][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:06:44,002][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:06:44,003][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:06:44,004][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:06:44,007][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:06:44,008][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:06:44,010][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:06:44,094][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/0/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.00it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.042 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 23:08:16,554][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/0>
[2024-05-30 23:08:16,555][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:08:16,558][HYDRA] 	#481 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-05-30 23:08:16,856][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:08:16,858][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 23:08:16,860][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:08:16,860][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:08:16,864][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:08:16,864][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:08:16,865][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:08:16,866][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:08:16,868][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:08:16,871][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:08:16,872][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:08:16,874][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:08:16,962][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/1/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.93it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.040 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.008             
[2024-05-30 23:09:51,333][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/1>
[2024-05-30 23:09:51,334][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:09:51,338][HYDRA] 	#482 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-05-30 23:09:51,630][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:09:51,633][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 23:09:51,635][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:09:51,635][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:09:51,639][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:09:51,640][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:09:51,641][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:09:51,641][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:09:51,643][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:09:51,646][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:09:51,647][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:09:51,649][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:09:51,738][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/2/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.95it/s v_num: 1.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.235     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.200 val/mre:    
                                                              0.042 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.010             
[2024-05-30 23:11:25,091][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/2>
[2024-05-30 23:11:25,092][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:11:25,096][HYDRA] 	#483 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-05-30 23:11:25,384][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:11:25,387][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 23:11:25,389][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:11:25,389][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:11:25,393][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:11:25,394][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:11:25,395][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:11:25,396][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:11:25,397][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:11:25,401][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:11:25,401][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:11:25,403][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:11:25,490][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/3/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.09it/s v_num: 1.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.038 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.008             
[2024-05-30 23:12:56,497][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/3>
[2024-05-30 23:12:56,497][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:12:56,501][HYDRA] 	#484 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-05-30 23:12:56,789][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:12:56,791][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 23:12:56,793][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:12:56,793][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:12:56,797][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:12:56,797][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:12:56,798][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:12:56,799][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:12:56,801][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:12:56,804][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:12:56,804][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:12:56,806][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:12:56,908][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/4/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.15it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.040 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 23:14:28,402][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/4>
[2024-05-30 23:14:28,403][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:14:28,407][HYDRA] 	#485 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-05-30 23:14:28,693][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:14:28,695][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 23:14:28,697][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:14:28,697][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:14:28,701][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:14:28,702][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:14:28,702][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:14:28,703][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:14:28,705][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:14:28,706][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:14:28,706][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:14:28,708][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:14:28,751][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/5/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.01it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.500 val/mre:    
                                                              0.045 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.013             
[2024-05-30 23:16:02,263][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/5>
[2024-05-30 23:16:02,264][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:16:02,267][HYDRA] 	#486 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-05-30 23:16:02,546][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:16:02,549][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 23:16:02,550][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:16:02,551][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:16:02,554][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:16:02,555][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:16:02,556][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:16:02,557][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:16:02,558][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:16:02,559][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:16:02,560][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:16:02,562][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:16:02,604][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/6/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.98it/s v_num: 1.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.042 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 23:17:35,368][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/6>
[2024-05-30 23:17:35,369][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:17:35,372][HYDRA] 	#487 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-05-30 23:17:35,666][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:17:35,669][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 23:17:35,670][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:17:35,671][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:17:35,674][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:17:35,675][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:17:35,676][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:17:35,676][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:17:35,678][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:17:35,681][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:17:35,682][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:17:35,684][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:17:35,772][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/7/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 3.06it/s v_num: 1.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.042 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.011             
[2024-05-30 23:19:07,759][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/7>
[2024-05-30 23:19:07,759][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:19:07,763][HYDRA] 	#488 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-05-30 23:19:08,057][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:19:08,060][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 23:19:08,061][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:19:08,062][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:19:08,065][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:19:08,066][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:19:08,067][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:19:08,067][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:19:08,069][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:19:08,073][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:19:08,073][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:19:08,075][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:19:08,162][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/8/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.96it/s v_num: 1.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.400 val/mre:    
                                                              0.077 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.032             
[2024-05-30 23:20:41,282][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/8>
[2024-05-30 23:20:41,283][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-05-30 23:20:41,286][HYDRA] 	#489 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=brits model.lightningmodule.rnn_hidden_size=32 callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-05-30 23:20:41,583][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-05-30 23:20:41,586][train.py][INFO] - Instantiating model <src.methods.brits.lightningmodule.BRITSLightningModule>
[2024-05-30 23:20:41,587][train.py][INFO] - Instantiating callbacks...
[2024-05-30 23:20:41,588][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-05-30 23:20:41,591][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-05-30 23:20:41,592][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-05-30 23:20:41,593][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-05-30 23:20:41,594][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-05-30 23:20:41,595][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-05-30 23:20:41,598][train.py][INFO] - Instantiating loggers...
[2024-05-30 23:20:41,599][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-05-30 23:20:41,601][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-05-30 23:20:41,690][train.py][INFO] - Starting training...
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/9/checkpoints exists and is not empty.
┏━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃    ┃ Name                                ┃ Type              ┃ Params ┃
┡━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0  │ model                               │ MultiTaskBRITS    │  327 K │
│ 1  │ model.model                         │ MyBackboneBRITS   │  323 K │
│ 2  │ model.model.rits_f                  │ MyBackboneRITS    │  161 K │
│ 3  │ model.model.rits_f.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 4  │ model.model.rits_f.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 5  │ model.model.rits_f.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 6  │ model.model.rits_f.hist_reg         │ Linear            │  5.3 K │
│ 7  │ model.model.rits_f.feat_reg         │ FeatureRegression │ 26.4 K │
│ 8  │ model.model.rits_f.combining_weight │ Linear            │ 52.6 K │
│ 9  │ model.model.rits_b                  │ MyBackboneRITS    │  161 K │
│ 10 │ model.model.rits_b.rnn_cell         │ LSTMCell          │ 45.8 K │
│ 11 │ model.model.rits_b.temp_decay_h     │ TemporalDecay     │  5.2 K │
│ 12 │ model.model.rits_b.temp_decay_x     │ TemporalDecay     │ 26.4 K │
│ 13 │ model.model.rits_b.hist_reg         │ Linear            │  5.3 K │
│ 14 │ model.model.rits_b.feat_reg         │ FeatureRegression │ 26.4 K │
│ 15 │ model.model.rits_b.combining_weight │ Linear            │ 52.6 K │
│ 16 │ model.out                           │ Linear            │  1.9 K │
│ 17 │ model.W_s1                          │ Linear            │    990 │
│ 18 │ model.W_s2                          │ Linear            │    930 │
└────┴─────────────────────────────────────┴───────────────────┴────────┘
Trainable params: 327 K                                                         
Non-trainable params: 0                                                         
Total params: 327 K                                                             
Total estimated model params size (MB): 1                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━━ 4/4 0:00:01 • 0:00:00 2.96it/s v_num: 1.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.039 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.009             
[2024-05-30 23:22:14,894][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/05-30-10-05-07/doing_today/0.3/9>
[2024-05-30 23:22:14,895][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
