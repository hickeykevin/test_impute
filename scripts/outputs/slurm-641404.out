GRU base line results for ricardo style evaluation
[2024-06-06 16:16:31,475][HYDRA] Launching 70 jobs locally
[2024-06-06 16:16:31,476][HYDRA] 	#0 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-06 16:16:31,650][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:16:36,091][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:16:36,104][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:16:36,105][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:16:36,153][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:16:36,153][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:16:36,163][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:16:36,163][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:16:36,163][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:16:36,165][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
/home/khickey/test_impute/env/lib/python3.12/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:16:36,581][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/advice_yourself/0.0/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.42it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 16:17:00,780][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/advice_yourself/0.0/0>
[2024-06-06 16:17:00,781][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:17:00,784][HYDRA] 	#1 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-06 16:17:01,009][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:17:01,011][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:17:01,012][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:17:01,012][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:17:01,015][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:17:01,015][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:17:01,016][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:17:01,016][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:17:01,016][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:17:01,018][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:17:01,051][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/advice_yourself/0.0/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.21it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 16:17:21,457][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/advice_yourself/0.0/1>
[2024-06-06 16:17:21,457][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:17:21,460][HYDRA] 	#2 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-06 16:17:21,650][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:17:21,652][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:17:21,653][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:17:21,653][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:17:21,656][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:17:21,656][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:17:21,657][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:17:21,659][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:17:21,659][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:17:21,660][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:17:21,773][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/advice_yourself/0.0/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.52it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 16:17:43,886][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/advice_yourself/0.0/2>
[2024-06-06 16:17:43,889][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:17:43,899][HYDRA] 	#3 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-06 16:17:44,207][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:17:44,208][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:17:44,209][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:17:44,209][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:17:44,217][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:17:44,217][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:17:44,218][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:17:44,220][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:17:44,221][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:17:44,222][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:17:44,324][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/advice_yourself/0.0/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.28it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 16:18:06,907][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/advice_yourself/0.0/3>
[2024-06-06 16:18:06,907][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:18:06,910][HYDRA] 	#4 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-06 16:18:07,154][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:18:07,156][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:18:07,157][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:18:07,157][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:18:07,159][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:18:07,160][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:18:07,160][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:18:07,161][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:18:07,161][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:18:07,162][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:18:07,242][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/advice_yourself/0.0/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.55it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 16:18:28,888][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/advice_yourself/0.0/4>
[2024-06-06 16:18:28,890][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:18:28,892][HYDRA] 	#5 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-06 16:18:29,099][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:18:29,101][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:18:29,102][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:18:29,102][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:18:29,104][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:18:29,105][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:18:29,105][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:18:29,107][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:18:29,107][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:18:29,109][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:18:29,219][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/advice_yourself/0.0/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.21it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 16:18:50,532][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/advice_yourself/0.0/5>
[2024-06-06 16:18:50,532][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:18:50,535][HYDRA] 	#6 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-06 16:18:50,866][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:18:50,867][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:18:50,868][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:18:50,868][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:18:50,871][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:18:50,871][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:18:50,871][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:18:50,873][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:18:50,873][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:18:50,874][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:18:50,905][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/advice_yourself/0.0/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.54it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 16:19:11,376][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/advice_yourself/0.0/6>
[2024-06-06 16:19:11,377][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:19:11,380][HYDRA] 	#7 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-06 16:19:11,563][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:19:11,564][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:19:11,565][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:19:11,565][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:19:11,567][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:19:11,568][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:19:11,568][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:19:11,569][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:19:11,569][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:19:11,570][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:19:11,603][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/advice_yourself/0.0/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.29it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 16:19:31,782][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/advice_yourself/0.0/7>
[2024-06-06 16:19:31,783][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:19:31,785][HYDRA] 	#8 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-06 16:19:31,960][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:19:31,961][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:19:31,962][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:19:31,963][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:19:31,965][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:19:31,965][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:19:31,966][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:19:31,968][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:19:31,968][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:19:31,969][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:19:32,054][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/advice_yourself/0.0/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.35it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 16:19:52,345][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/advice_yourself/0.0/8>
[2024-06-06 16:19:52,345][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:19:52,348][HYDRA] 	#9 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-06 16:19:52,523][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:19:52,525][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:19:52,526][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:19:52,526][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:19:52,528][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:19:52,528][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:19:52,529][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:19:52,530][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:19:52,530][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:19:52,531][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:19:52,565][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/advice_yourself/0.0/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.43it/s v_num: 0.000      
                                                              val/auc: 0.461    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.222 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 16:20:12,924][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/advice_yourself/0.0/9>
[2024-06-06 16:20:12,924][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:20:12,930][HYDRA] 	#10 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-06 16:20:13,118][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:20:13,119][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:20:13,120][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:20:13,120][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:20:13,122][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:20:13,123][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:20:13,123][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:20:13,124][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:20:13,124][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:20:13,125][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:20:13,158][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/anything_regret/0.0/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 24.83it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.556 train/auc:  
                                                              0.759 train/f1:   
                                                              0.748             
                                                              train/precision:  
                                                              0.784             
                                                              train/recall:     
                                                              0.714             
[2024-06-06 16:20:32,618][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/anything_regret/0.0/0>
[2024-06-06 16:20:32,618][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:20:32,621][HYDRA] 	#11 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-06 16:20:32,805][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:20:32,806][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:20:32,807][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:20:32,807][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:20:32,809][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:20:32,810][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:20:32,810][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:20:32,813][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:20:32,813][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:20:32,814][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:20:32,890][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/anything_regret/0.0/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 24.78it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 train/auc:  
                                                              0.705 train/f1:   
                                                              0.769             
                                                              train/precision:  
                                                              0.632             
                                                              train/recall:     
                                                              0.982             
[2024-06-06 16:20:51,824][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/anything_regret/0.0/1>
[2024-06-06 16:20:51,825][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:20:51,827][HYDRA] 	#12 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-06 16:20:51,999][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:20:52,000][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:20:52,001][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:20:52,001][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:20:52,003][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:20:52,004][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:20:52,004][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:20:52,005][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:20:52,005][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:20:52,006][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:20:52,034][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/anything_regret/0.0/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 24.65it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 train/auc:  
                                                              0.893 train/f1:   
                                                              0.897             
                                                              train/precision:  
                                                              0.867             
                                                              train/recall:     
                                                              0.929             
[2024-06-06 16:21:11,051][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/anything_regret/0.0/2>
[2024-06-06 16:21:11,052][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:21:11,054][HYDRA] 	#13 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-06 16:21:11,234][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:21:11,236][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:21:11,237][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:21:11,237][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:21:11,239][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:21:11,240][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:21:11,240][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:21:11,241][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:21:11,241][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:21:11,243][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:21:11,278][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/anything_regret/0.0/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 24.48it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 train/auc:  
                                                              0.902 train/f1:   
                                                              0.909             
                                                              train/precision:  
                                                              0.846             
                                                              train/recall:     
                                                              0.982             
[2024-06-06 16:21:30,344][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/anything_regret/0.0/3>
[2024-06-06 16:21:30,345][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:21:30,349][HYDRA] 	#14 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-06 16:21:30,528][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:21:30,530][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:21:30,531][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:21:30,531][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:21:30,534][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:21:30,534][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:21:30,534][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:21:30,535][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:21:30,535][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:21:30,536][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:21:30,576][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/anything_regret/0.0/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 24.62it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.111 train/auc:  
                                                              0.920 train/f1:   
                                                              0.924             
                                                              train/precision:  
                                                              0.873             
                                                              train/recall:     
                                                              0.982             
[2024-06-06 16:21:49,572][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/anything_regret/0.0/4>
[2024-06-06 16:21:49,572][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:21:49,575][HYDRA] 	#15 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-06 16:21:49,912][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:21:49,913][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:21:49,914][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:21:49,914][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:21:49,917][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:21:49,917][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:21:49,918][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:21:49,920][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:21:49,920][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:21:49,921][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:21:50,007][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/anything_regret/0.0/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 24.41it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.556 train/auc:  
                                                              0.723 train/f1:   
                                                              0.774             
                                                              train/precision:  
                                                              0.654             
                                                              train/recall:     
                                                              0.946             
[2024-06-06 16:22:09,000][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/anything_regret/0.0/5>
[2024-06-06 16:22:09,001][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:22:09,003][HYDRA] 	#16 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-06 16:22:09,177][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:22:09,179][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:22:09,180][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:22:09,180][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:22:09,182][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:22:09,182][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:22:09,183][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:22:09,183][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:22:09,183][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:22:09,185][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:22:09,220][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/anything_regret/0.0/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 24.75it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 train/auc:  
                                                              0.714 train/f1:   
                                                              0.771             
                                                              train/precision:  
                                                              0.643             
                                                              train/recall:     
                                                              0.964             
[2024-06-06 16:22:28,181][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/anything_regret/0.0/6>
[2024-06-06 16:22:28,182][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:22:28,185][HYDRA] 	#17 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-06 16:22:28,361][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:22:28,362][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:22:28,363][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:22:28,364][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:22:28,366][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:22:28,366][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:22:28,366][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:22:28,368][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:22:28,368][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:22:28,369][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:22:28,400][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/anything_regret/0.0/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 24.65it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 train/auc:  
                                                              0.750 train/f1:   
                                                              0.781             
                                                              train/precision:  
                                                              0.694             
                                                              train/recall:     
                                                              0.893             
[2024-06-06 16:22:47,480][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/anything_regret/0.0/7>
[2024-06-06 16:22:47,481][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:22:47,486][HYDRA] 	#18 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-06 16:22:47,697][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:22:47,698][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:22:47,699][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:22:47,699][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:22:47,702][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:22:47,702][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:22:47,703][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:22:47,703][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:22:47,704][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:22:47,705][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:22:47,747][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/anything_regret/0.0/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 24.68it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 train/auc:  
                                                              0.938 train/f1:   
                                                              0.940             
                                                              train/precision:  
                                                              0.902             
                                                              train/recall:     
                                                              0.982             
[2024-06-06 16:23:07,526][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/anything_regret/0.0/8>
[2024-06-06 16:23:07,527][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:23:07,530][HYDRA] 	#19 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-06 16:23:07,738][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:23:07,739][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:23:07,740][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:23:07,740][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:23:07,743][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:23:07,743][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:23:07,744][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:23:07,746][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:23:07,746][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:23:07,747][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:23:07,865][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/anything_regret/0.0/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 24.82it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 train/auc:  
                                                              0.804 train/f1:   
                                                              0.817             
                                                              train/precision:  
                                                              0.766             
                                                              train/recall:     
                                                              0.875             
[2024-06-06 16:23:26,956][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/anything_regret/0.0/9>
[2024-06-06 16:23:26,957][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:23:26,959][HYDRA] 	#20 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-06 16:23:27,147][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:23:27,149][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:23:27,150][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:23:27,150][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:23:27,152][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:23:27,153][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:23:27,153][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:23:27,154][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:23:27,154][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:23:27,155][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:23:27,185][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/argued_someone/0.0/0/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.32it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 train/auc:  
                                                              0.960 train/f1:   
                                                              0.960             
                                                              train/precision:  
                                                              0.952             
                                                              train/recall:     
                                                              0.968             
[2024-06-06 16:23:48,411][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/argued_someone/0.0/0>
[2024-06-06 16:23:48,411][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:23:48,413][HYDRA] 	#21 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-06 16:23:48,614][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:23:48,615][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:23:48,616][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:23:48,617][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:23:48,618][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:23:48,619][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:23:48,619][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:23:48,621][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:23:48,621][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:23:48,622][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:23:48,655][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/argued_someone/0.0/1/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.22it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.100 train/auc:  
                                                              0.919 train/f1:   
                                                              0.921             
                                                              train/precision:  
                                                              0.906             
                                                              train/recall:     
                                                              0.935             
[2024-06-06 16:24:09,198][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/argued_someone/0.0/1>
[2024-06-06 16:24:09,199][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:24:09,202][HYDRA] 	#22 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-06 16:24:09,385][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:24:09,386][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:24:09,387][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:24:09,387][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:24:09,389][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:24:09,390][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:24:09,390][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:24:09,392][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:24:09,392][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:24:09,394][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:24:09,480][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/argued_someone/0.0/2/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.26it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 train/auc:  
                                                              0.968 train/f1:   
                                                              0.967             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.952             
[2024-06-06 16:24:29,962][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/argued_someone/0.0/2>
[2024-06-06 16:24:29,963][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:24:29,966][HYDRA] 	#23 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-06 16:24:30,138][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:24:30,140][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:24:30,140][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:24:30,141][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:24:30,143][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:24:30,143][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:24:30,144][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:24:30,144][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:24:30,144][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:24:30,145][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:24:30,176][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/argued_someone/0.0/3/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.30it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 train/auc:  
                                                              0.935 train/f1:   
                                                              0.931             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.871             
[2024-06-06 16:24:50,539][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/argued_someone/0.0/3>
[2024-06-06 16:24:50,539][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:24:50,541][HYDRA] 	#24 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-06 16:24:50,719][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:24:50,720][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:24:50,721][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:24:50,721][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:24:50,723][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:24:50,724][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:24:50,724][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:24:50,725][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:24:50,725][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:24:50,727][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:24:50,763][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/argued_someone/0.0/4/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.21it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 train/auc:  
                                                              0.935 train/f1:   
                                                              0.931             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.871             
[2024-06-06 16:25:11,081][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/argued_someone/0.0/4>
[2024-06-06 16:25:11,082][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:25:11,084][HYDRA] 	#25 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-06 16:25:11,260][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:25:11,261][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:25:11,262][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:25:11,262][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:25:11,264][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:25:11,265][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:25:11,265][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:25:11,267][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:25:11,268][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:25:11,269][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:25:11,343][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/argued_someone/0.0/5/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 22.87it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.400 train/auc:  
                                                              0.968 train/f1:   
                                                              0.968             
                                                              train/precision:  
                                                              0.968             
                                                              train/recall:     
                                                              0.968             
[2024-06-06 16:25:31,989][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/argued_someone/0.0/5>
[2024-06-06 16:25:31,990][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:25:31,993][HYDRA] 	#26 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-06 16:25:32,165][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:25:32,166][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:25:32,167][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:25:32,168][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:25:32,170][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:25:32,170][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:25:32,170][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:25:32,171][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:25:32,171][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:25:32,172][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:25:32,203][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/argued_someone/0.0/6/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.41it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 train/auc:  
                                                              0.960 train/f1:   
                                                              0.959             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.935             
[2024-06-06 16:25:52,739][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/argued_someone/0.0/6>
[2024-06-06 16:25:52,739][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:25:52,741][HYDRA] 	#27 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-06 16:25:52,919][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:25:52,921][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:25:52,922][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:25:52,922][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:25:52,924][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:25:52,924][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:25:52,925][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:25:52,926][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:25:52,926][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:25:52,928][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:25:52,962][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/argued_someone/0.0/7/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.48it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.100 train/auc:  
                                                              0.984 train/f1:   
                                                              0.984             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.968             
[2024-06-06 16:26:13,508][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/argued_someone/0.0/7>
[2024-06-06 16:26:13,509][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:26:13,512][HYDRA] 	#28 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-06 16:26:13,700][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:26:13,702][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:26:13,703][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:26:13,703][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:26:13,705][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:26:13,706][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:26:13,706][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:26:13,708][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:26:13,709][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:26:13,710][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:26:13,811][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/argued_someone/0.0/8/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.30it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.100 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 16:26:34,435][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/argued_someone/0.0/8>
[2024-06-06 16:26:34,436][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:26:34,438][HYDRA] 	#29 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-06 16:26:34,616][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:26:34,617][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:26:34,618][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:26:34,618][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:26:34,621][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:26:34,621][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:26:34,622][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:26:34,622][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:26:34,622][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:26:34,623][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:26:34,654][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/argued_someone/0.0/9/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.19it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 train/auc:  
                                                              0.960 train/f1:   
                                                              0.959             
                                                              train/precision:  
                                                              0.967             
                                                              train/recall:     
                                                              0.952             
[2024-06-06 16:26:55,119][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/argued_someone/0.0/9>
[2024-06-06 16:26:55,120][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:26:55,122][HYDRA] 	#30 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-06 16:26:55,313][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:26:55,314][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:26:55,315][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:26:55,315][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:26:55,317][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:26:55,318][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:26:55,318][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:26:55,320][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:26:55,320][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:26:55,321][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:26:55,360][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/controlling_temper/0.0/0/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 24.00it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 train/auc:  
                                                              0.941 train/f1:   
                                                              0.943             
                                                              train/precision:  
                                                              0.906             
                                                              train/recall:     
                                                              0.983             
[2024-06-06 16:27:16,775][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/controlling_temper/0.0/0>
[2024-06-06 16:27:16,776][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:27:16,781][HYDRA] 	#31 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-06 16:27:16,990][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:27:16,992][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:27:16,993][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:27:16,993][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:27:16,995][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:27:16,996][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:27:16,996][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:27:16,999][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:27:16,999][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:27:17,000][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:27:17,133][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/controlling_temper/0.0/1/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.79it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 train/auc:  
                                                              0.966 train/f1:   
                                                              0.967             
                                                              train/precision:  
                                                              0.951             
                                                              train/recall:     
                                                              0.983             
[2024-06-06 16:27:37,319][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/controlling_temper/0.0/1>
[2024-06-06 16:27:37,319][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:27:37,321][HYDRA] 	#32 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-06 16:27:37,499][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:27:37,501][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:27:37,502][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:27:37,502][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:27:37,504][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:27:37,505][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:27:37,505][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:27:37,506][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:27:37,506][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:27:37,507][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:27:37,537][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/controlling_temper/0.0/2/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.91it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.133     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.100 train/auc:  
                                                              0.898 train/f1:   
                                                              0.905             
                                                              train/precision:  
                                                              0.851             
                                                              train/recall:     
                                                              0.966             
[2024-06-06 16:27:57,363][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/controlling_temper/0.0/2>
[2024-06-06 16:27:57,364][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:27:57,366][HYDRA] 	#33 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-06 16:27:57,555][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:27:57,557][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:27:57,558][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:27:57,559][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:27:57,561][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:27:57,561][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:27:57,562][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:27:57,563][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:27:57,563][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:27:57,565][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:27:57,603][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/controlling_temper/0.0/3/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.92it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.235     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.200 train/auc:  
                                                              0.924 train/f1:   
                                                              0.928             
                                                              train/precision:  
                                                              0.879             
                                                              train/recall:     
                                                              0.983             
[2024-06-06 16:28:17,509][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/controlling_temper/0.0/3>
[2024-06-06 16:28:17,509][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:28:17,512][HYDRA] 	#34 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-06 16:28:17,694][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:28:17,696][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:28:17,697][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:28:17,697][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:28:17,699][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:28:17,699][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:28:17,700][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:28:17,702][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:28:17,702][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:28:17,703][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:28:17,785][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/controlling_temper/0.0/4/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.58it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.316     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.300 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.966             
[2024-06-06 16:28:37,591][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/controlling_temper/0.0/4>
[2024-06-06 16:28:37,591][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:28:37,594][HYDRA] 	#35 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-06 16:28:37,773][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:28:37,774][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:28:37,775][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:28:37,775][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:28:37,777][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:28:37,778][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:28:37,778][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:28:37,779][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:28:37,779][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:28:37,780][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:28:37,810][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/controlling_temper/0.0/5/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.81it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.500 train/auc:  
                                                              0.788 train/f1:   
                                                              0.815             
                                                              train/precision:  
                                                              0.724             
                                                              train/recall:     
                                                              0.932             
[2024-06-06 16:28:57,675][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/controlling_temper/0.0/5>
[2024-06-06 16:28:57,676][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:28:57,679][HYDRA] 	#36 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-06 16:28:57,862][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:28:57,864][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:28:57,865][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:28:57,865][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:28:57,867][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:28:57,867][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:28:57,868][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:28:57,869][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:28:57,869][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:28:57,870][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:28:57,907][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/controlling_temper/0.0/6/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.33it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.400 train/auc:  
                                                              0.915 train/f1:   
                                                              0.921             
                                                              train/precision:  
                                                              0.866             
                                                              train/recall:     
                                                              0.983             
[2024-06-06 16:29:17,920][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/controlling_temper/0.0/6>
[2024-06-06 16:29:17,920][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:29:17,923][HYDRA] 	#37 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-06 16:29:18,103][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:29:18,104][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:29:18,105][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:29:18,105][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:29:18,108][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:29:18,108][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:29:18,108][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:29:18,110][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:29:18,111][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:29:18,112][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:29:18,191][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/controlling_temper/0.0/7/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.77it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 train/auc:  
                                                              0.966 train/f1:   
                                                              0.966             
                                                              train/precision:  
                                                              0.966             
                                                              train/recall:     
                                                              0.966             
[2024-06-06 16:29:38,135][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/controlling_temper/0.0/7>
[2024-06-06 16:29:38,135][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:29:38,137][HYDRA] 	#38 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-06 16:29:38,320][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:29:38,322][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:29:38,323][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:29:38,323][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:29:38,325][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:29:38,325][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:29:38,326][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:29:38,327][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:29:38,327][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:29:38,328][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:29:38,362][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/controlling_temper/0.0/8/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.81it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 train/auc:  
                                                              0.975 train/f1:   
                                                              0.975             
                                                              train/precision:  
                                                              0.967             
                                                              train/recall:     
                                                              0.983             
[2024-06-06 16:29:58,175][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/controlling_temper/0.0/8>
[2024-06-06 16:29:58,175][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:29:58,178][HYDRA] 	#39 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-06 16:29:58,362][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:29:58,364][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:29:58,365][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:29:58,365][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:29:58,367][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:29:58,368][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:29:58,368][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:29:58,369][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:29:58,369][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:29:58,371][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:29:58,407][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/controlling_temper/0.0/9/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 24.08it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 train/auc:  
                                                              0.975 train/f1:   
                                                              0.974             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.966             
[2024-06-06 16:30:18,271][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/controlling_temper/0.0/9>
[2024-06-06 16:30:18,272][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:30:18,274][HYDRA] 	#40 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-06 16:30:18,460][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:30:18,461][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:30:18,462][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:30:18,462][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:30:18,465][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:30:18,466][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:30:18,466][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:30:18,468][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:30:18,468][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:30:18,469][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:30:18,555][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_depression/0.0/0/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 22.79it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 train/auc:  
                                                              0.508 train/f1:   
                                                              0.279             
                                                              train/precision:  
                                                              0.522             
                                                              train/recall:     
                                                              0.190             
[2024-06-06 16:30:39,243][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_depression/0.0/0>
[2024-06-06 16:30:39,244][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:30:39,246][HYDRA] 	#41 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-06 16:30:39,433][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:30:39,435][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:30:39,435][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:30:39,436][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:30:39,438][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:30:39,438][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:30:39,439][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:30:39,439][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:30:39,439][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:30:39,441][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:30:39,475][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_depression/0.0/1/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 22.98it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 train/auc:  
                                                              0.548 train/f1:   
                                                              0.360             
                                                              train/precision:  
                                                              0.615             
                                                              train/recall:     
                                                              0.254             
[2024-06-06 16:30:59,433][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_depression/0.0/1>
[2024-06-06 16:30:59,434][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:30:59,436][HYDRA] 	#42 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-06 16:30:59,613][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:30:59,615][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:30:59,616][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:30:59,616][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:30:59,618][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:30:59,618][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:30:59,619][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:30:59,620][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:30:59,620][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:30:59,621][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:30:59,662][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_depression/0.0/2/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 22.76it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 train/auc:  
                                                              0.532 train/f1:   
                                                              0.272             
                                                              train/precision:  
                                                              0.611             
                                                              train/recall:     
                                                              0.175             
[2024-06-06 16:31:19,968][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_depression/0.0/2>
[2024-06-06 16:31:19,969][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:31:19,971][HYDRA] 	#43 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-06 16:31:20,183][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:31:20,184][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:31:20,185][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:31:20,185][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:31:20,188][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:31:20,189][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:31:20,189][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:31:20,192][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:31:20,192][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:31:20,193][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:31:20,307][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_depression/0.0/3/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 22.93it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 train/auc:  
                                                              0.540 train/f1:   
                                                              0.171             
                                                              train/precision:  
                                                              0.857             
                                                              train/recall:     
                                                              0.095             
[2024-06-06 16:31:40,688][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_depression/0.0/3>
[2024-06-06 16:31:40,688][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:31:40,691][HYDRA] 	#44 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-06 16:31:40,881][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:31:40,883][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:31:40,884][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:31:40,884][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:31:40,887][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:31:40,887][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:31:40,887][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:31:40,888][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:31:40,888][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:31:40,889][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:31:40,922][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_depression/0.0/4/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 22.81it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 train/auc:  
                                                              0.548 train/f1:   
                                                              0.197             
                                                              train/precision:  
                                                              0.875             
                                                              train/recall:     
                                                              0.111             
[2024-06-06 16:32:00,964][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_depression/0.0/4>
[2024-06-06 16:32:00,965][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:32:00,967][HYDRA] 	#45 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-06 16:32:01,145][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:32:01,147][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:32:01,147][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:32:01,148][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:32:01,150][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:32:01,150][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:32:01,151][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:32:01,152][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:32:01,152][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:32:01,153][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:32:01,188][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_depression/0.0/5/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 22.99it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 train/auc:  
                                                              0.516 train/f1:   
                                                              0.344             
                                                              train/precision:  
                                                              0.533             
                                                              train/recall:     
                                                              0.254             
[2024-06-06 16:32:21,518][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_depression/0.0/5>
[2024-06-06 16:32:21,518][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:32:21,521][HYDRA] 	#46 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-06 16:32:21,704][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:32:21,706][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:32:21,706][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:32:21,707][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:32:21,709][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:32:21,709][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:32:21,710][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:32:21,712][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:32:21,712][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:32:21,713][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:32:21,800][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_depression/0.0/6/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 22.74it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 train/auc:  
                                                              0.532 train/f1:   
                                                              0.169             
                                                              train/precision:  
                                                              0.750             
                                                              train/recall:     
                                                              0.095             
[2024-06-06 16:32:41,896][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_depression/0.0/6>
[2024-06-06 16:32:41,896][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:32:41,899][HYDRA] 	#47 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-06 16:32:42,081][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:32:42,082][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:32:42,083][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:32:42,083][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:32:42,085][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:32:42,086][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:32:42,086][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:32:42,087][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:32:42,087][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:32:42,088][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:32:42,117][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_depression/0.0/7/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 22.39it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 train/auc:  
                                                              0.532 train/f1:   
                                                              0.306             
                                                              train/precision:  
                                                              0.591             
                                                              train/recall:     
                                                              0.206             
[2024-06-06 16:33:02,540][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_depression/0.0/7>
[2024-06-06 16:33:02,540][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:33:02,543][HYDRA] 	#48 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-06 16:33:02,724][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:33:02,725][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:33:02,726][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:33:02,726][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:33:02,728][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:33:02,729][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:33:02,729][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:33:02,731][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:33:02,731][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:33:02,732][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:33:02,772][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_depression/0.0/8/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 22.97it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 train/auc:  
                                                              0.548 train/f1:   
                                                              0.197             
                                                              train/precision:  
                                                              0.875             
                                                              train/recall:     
                                                              0.111             
[2024-06-06 16:33:22,794][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_depression/0.0/8>
[2024-06-06 16:33:22,795][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:33:22,797][HYDRA] 	#49 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=9
[2024-06-06 16:33:22,969][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:33:22,970][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:33:22,971][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:33:22,971][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[rank: 0] Seed set to 9
[2024-06-06 16:33:22,974][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:33:22,974][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:33:22,974][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:33:22,976][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:33:22,977][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:33:22,978][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-06-06 16:33:23,065][train.py][INFO] - Starting training...
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_depression/0.0/9/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.22it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 train/auc:  
                                                              0.563 train/f1:   
                                                              0.247             
                                                              train/precision:  
                                                              0.900             
                                                              train/recall:     
                                                              0.143             
[2024-06-06 16:33:43,139][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_depression/0.0/9>
[2024-06-06 16:33:43,140][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:33:43,143][HYDRA] 	#50 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-06 16:33:43,326][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:33:43,327][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:33:43,328][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:33:43,328][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:33:43,330][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:33:43,331][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:33:43,331][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:33:43,332][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:33:43,332][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:33:43,333][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:33:43,371][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_p_t_s_d/0.0/0/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 24.49it/s v_num: 0.000      
                                                              val/auc: 0.516    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.143 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982             
[2024-06-06 16:34:02,905][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_p_t_s_d/0.0/0>
[2024-06-06 16:34:02,905][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:34:02,908][HYDRA] 	#51 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-06 16:34:03,097][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:34:03,099][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:34:03,100][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:34:03,100][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:34:03,102][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:34:03,103][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:34:03,103][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:34:03,104][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:34:03,104][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:34:03,105][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:34:03,141][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_p_t_s_d/0.0/1/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 24.25it/s v_num: 0.000      
                                                              val/auc: 0.587    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.286 train/auc:  
                                                              0.956 train/f1:   
                                                              0.955             
                                                              train/precision:  
                                                              0.981             
                                                              train/recall:     
                                                              0.930             
[2024-06-06 16:34:22,044][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_p_t_s_d/0.0/1>
[2024-06-06 16:34:22,045][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:34:22,048][HYDRA] 	#52 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-06 16:34:22,230][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:34:22,231][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:34:22,232][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:34:22,232][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:34:22,234][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:34:22,235][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:34:22,235][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:34:22,236][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:34:22,236][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:34:22,237][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:34:22,269][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_p_t_s_d/0.0/2/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 24.22it/s v_num: 0.000      
                                                              val/auc: 0.516    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.143 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 16:34:41,008][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_p_t_s_d/0.0/2>
[2024-06-06 16:34:41,008][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:34:41,010][HYDRA] 	#53 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-06 16:34:41,186][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:34:41,188][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:34:41,189][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:34:41,189][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:34:41,191][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:34:41,191][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:34:41,192][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:34:41,194][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:34:41,194][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:34:41,196][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:34:41,274][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_p_t_s_d/0.0/3/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 24.36it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.609     
                                                              val/precision:    
                                                              0.438 val/recall: 
                                                              1.000 train/auc:  
                                                              0.456 train/f1:   
                                                              0.569             
                                                              train/precision:  
                                                              0.471             
                                                              train/recall:     
                                                              0.719             
[2024-06-06 16:35:00,113][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_p_t_s_d/0.0/3>
[2024-06-06 16:35:00,114][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:35:00,117][HYDRA] 	#54 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-06 16:35:00,304][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:35:00,306][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:35:00,307][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:35:00,307][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:35:00,309][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:35:00,310][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:35:00,310][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:35:00,312][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:35:00,313][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:35:00,314][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:35:00,407][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_p_t_s_d/0.0/4/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 24.12it/s v_num: 0.000      
                                                              val/auc: 0.516    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.143 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 16:35:19,783][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_p_t_s_d/0.0/4>
[2024-06-06 16:35:19,784][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:35:19,786][HYDRA] 	#55 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-06 16:35:19,959][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:35:19,961][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:35:19,961][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:35:19,962][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:35:19,964][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:35:19,964][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:35:19,964][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:35:19,965][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:35:19,965][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:35:19,967][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-06-06 16:35:19,999][train.py][INFO] - Starting training...
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_p_t_s_d/0.0/5/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 24.28it/s v_num: 0.000      
                                                              val/auc: 0.516    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.143 train/auc:  
                                                              0.939 train/f1:   
                                                              0.938             
                                                              train/precision:  
                                                              0.946             
                                                              train/recall:     
                                                              0.930             
[2024-06-06 16:35:38,842][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_p_t_s_d/0.0/5>
[2024-06-06 16:35:38,842][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:35:38,845][HYDRA] 	#56 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-06 16:35:39,024][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:35:39,026][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:35:39,027][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:35:39,027][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:35:39,029][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:35:39,029][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:35:39,030][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:35:39,030][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:35:39,031][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:35:39,032][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:35:39,063][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_p_t_s_d/0.0/6/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 24.32it/s v_num: 0.000      
                                                              val/auc: 0.516    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.143 train/auc:  
                                                              0.991 train/f1:   
                                                              0.991             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.982             
[2024-06-06 16:35:57,982][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_p_t_s_d/0.0/6>
[2024-06-06 16:35:57,983][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:35:57,986][HYDRA] 	#57 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-06 16:35:58,163][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:35:58,164][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:35:58,165][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:35:58,165][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:35:58,167][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:35:58,168][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:35:58,168][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:35:58,169][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:35:58,169][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:35:58,170][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:35:58,203][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_p_t_s_d/0.0/7/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 24.44it/s v_num: 0.000      
                                                              val/auc: 0.516    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.143 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 16:36:17,135][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_p_t_s_d/0.0/7>
[2024-06-06 16:36:17,135][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:36:17,138][HYDRA] 	#58 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-06 16:36:17,321][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:36:17,322][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:36:17,323][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:36:17,323][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:36:17,325][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:36:17,326][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:36:17,326][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:36:17,328][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:36:17,329][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:36:17,330][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:36:17,417][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_p_t_s_d/0.0/8/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 24.25it/s v_num: 0.000      
                                                              val/auc: 0.516    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.143 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 16:36:37,260][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_p_t_s_d/0.0/8>
[2024-06-06 16:36:37,261][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:36:37,264][HYDRA] 	#59 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-06 16:36:37,469][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:36:37,471][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:36:37,472][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:36:37,472][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:36:37,474][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:36:37,475][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:36:37,475][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:36:37,476][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:36:37,476][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:36:37,477][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:36:37,704][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_p_t_s_d/0.0/9/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 24.57it/s v_num: 0.000      
                                                              val/auc: 0.516    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.143 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 16:36:56,816][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/diagnosed_p_t_s_d/0.0/9>
[2024-06-06 16:36:56,817][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:36:56,819][HYDRA] 	#60 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-06 16:36:57,001][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:36:57,003][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:36:57,004][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:36:57,004][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:36:57,006][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:36:57,007][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:36:57,007][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:36:57,008][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:36:57,008][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:36:57,009][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:36:57,039][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/doing_today/0.0/0/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 22.90it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.100 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.985             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 16:37:18,476][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/doing_today/0.0/0>
[2024-06-06 16:37:18,477][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:37:18,485][HYDRA] 	#61 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-06 16:37:18,695][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:37:18,697][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:37:18,697][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:37:18,698][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:37:18,702][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:37:18,703][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:37:18,704][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:37:18,706][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:37:18,706][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:37:18,707][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:37:18,839][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/doing_today/0.0/1/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 23.10it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.100 train/auc:  
                                                              0.961 train/f1:   
                                                              0.962             
                                                              train/precision:  
                                                              0.928             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 16:37:40,424][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/doing_today/0.0/1>
[2024-06-06 16:37:40,424][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:37:40,426][HYDRA] 	#62 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-06 16:37:40,629][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:37:40,630][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:37:40,631][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:37:40,631][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:37:40,633][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:37:40,634][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:37:40,634][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:37:40,635][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:37:40,635][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:37:40,637][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:37:40,681][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/doing_today/0.0/2/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 22.99it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.100 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.985             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 16:38:01,408][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/doing_today/0.0/2>
[2024-06-06 16:38:01,409][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:38:01,412][HYDRA] 	#63 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-06 16:38:01,592][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:38:01,594][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:38:01,595][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:38:01,595][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:38:01,597][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:38:01,597][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:38:01,598][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:38:01,598][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:38:01,599][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:38:01,600][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:38:01,636][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/doing_today/0.0/3/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 22.77it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.100 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 16:38:22,840][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/doing_today/0.0/3>
[2024-06-06 16:38:22,841][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:38:22,844][HYDRA] 	#64 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-06 16:38:23,045][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:38:23,046][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:38:23,047][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:38:23,047][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:38:23,049][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:38:23,050][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:38:23,050][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:38:23,053][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:38:23,053][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:38:23,054][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:38:23,157][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/doing_today/0.0/4/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 22.74it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.100 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.985             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 16:38:44,422][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/doing_today/0.0/4>
[2024-06-06 16:38:44,423][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:38:44,425][HYDRA] 	#65 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-06 16:38:44,608][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:38:44,609][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:38:44,610][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:38:44,611][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:38:44,613][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:38:44,613][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:38:44,614][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:38:44,614][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:38:44,615][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:38:44,616][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:38:44,654][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/doing_today/0.0/5/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 22.74it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.985             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 16:39:05,596][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/doing_today/0.0/5>
[2024-06-06 16:39:05,596][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:39:05,599][HYDRA] 	#66 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-06 16:39:05,778][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:39:05,779][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:39:05,780][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:39:05,781][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:39:05,783][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:39:05,784][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:39:05,784][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:39:05,785][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:39:05,785][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:39:05,786][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:39:05,818][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/doing_today/0.0/6/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 22.63it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.100 train/auc:  
                                                              0.969 train/f1:   
                                                              0.969             
                                                              train/precision:  
                                                              0.955             
                                                              train/recall:     
                                                              0.984             
[2024-06-06 16:39:26,594][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/doing_today/0.0/6>
[2024-06-06 16:39:26,595][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:39:26,597][HYDRA] 	#67 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-06 16:39:26,789][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:39:26,790][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:39:26,791][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:39:26,792][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:39:26,794][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:39:26,794][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:39:26,795][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:39:26,797][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:39:26,797][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:39:26,798][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:39:26,892][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/doing_today/0.0/7/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 22.83it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.100 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.985             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 16:39:47,983][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/doing_today/0.0/7>
[2024-06-06 16:39:47,984][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:39:47,987][HYDRA] 	#68 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-06 16:39:48,181][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:39:48,182][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:39:48,183][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:39:48,183][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:39:48,185][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:39:48,186][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:39:48,186][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:39:48,187][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:39:48,187][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:39:48,188][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:39:48,233][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/doing_today/0.0/8/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 22.84it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.100 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.984             
[2024-06-06 16:40:09,368][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/doing_today/0.0/8>
[2024-06-06 16:40:09,369][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-06 16:40:09,372][HYDRA] 	#69 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=gru callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-06 16:40:09,557][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-06 16:40:09,558][train.py][INFO] - Instantiating model <src.methods.gru.lightningmodule.GRULightningModule>
[2024-06-06 16:40:09,559][train.py][INFO] - Instantiating callbacks...
[2024-06-06 16:40:09,560][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-06 16:40:09,562][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-06 16:40:09,562][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-06 16:40:09,563][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-06 16:40:09,563][train.py][INFO] - Instantiating loggers...
[2024-06-06 16:40:09,563][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-06 16:40:09,565][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-06 16:40:09,599][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/doing_today/0.0/9/csv_artifacts/
┏━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓
┃   ┃ Name          ┃ Type             ┃ Params ┃
┡━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩
│ 0 │ loss          │ CrossEntropyLoss │      0 │
│ 1 │ model         │ GRU_base         │  162 K │
│ 2 │ model.RNNCell │ GRU              │  162 K │
│ 3 │ model.predict │ Linear           │    258 │
│ 4 │ model.act     │ Sigmoid          │      0 │
└───┴───────────────┴──────────────────┴────────┘
Trainable params: 162 K                                                         
Non-trainable params: 0                                                         
Total params: 162 K                                                             
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 22.69it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.100 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.985             
                                                              train/recall:     
                                                              1.000             
[2024-06-06 16:40:30,678][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-06-16-06-23/doing_today/0.0/9>
[2024-06-06 16:40:30,678][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
