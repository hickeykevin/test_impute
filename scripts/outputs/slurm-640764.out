[2024-06-05 16:57:39,365][HYDRA] Launching 490 jobs locally
[2024-06-05 16:57:39,365][HYDRA] 	#0 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 16:57:39,592][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 16:57:49,472][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
2024-06-05 16:57:59 [ERROR]: ❌ No module named 'torch_geometric'
Note torch_geometric is missing, please install it with 'pip install torch_geometric torch_scatter torch_sparse' or 'conda install -c pyg pyg pytorch-scatter pytorch-sparse'
2024-06-05 16:57:59 [ERROR]: ❌ name 'MessagePassing' is not defined
Note torch_geometric is missing, please install it with 'pip install torch_geometric torch_scatter torch_sparse' or 'conda install -c pyg pyg pytorch-scatter pytorch-sparse'
[2024-06-05 16:57:59,723][train.py][INFO] - Instantiating callbacks...
[2024-06-05 16:57:59,724][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 16:57:59,777][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 16:57:59,777][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 16:57:59,808][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 16:57:59,809][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 16:57:59,812][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 16:57:59,813][train.py][INFO] - Instantiating loggers...
[2024-06-05 16:57:59,813][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 16:57:59,815][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
/home/khickey/test_impute/env/lib/python3.12/site-packages/torch/cuda/__init__.py:628: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 16:58:00,697][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.0/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.96it/s v_num: 0.000      
                                                              val/auc: 0.683    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.667 val/mre: nan
                                                              train/auc: 0.895  
                                                              train/f1: 0.901   
                                                              train/precision:  
                                                              0.855             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              nan               
[2024-06-05 16:58:21,559][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.0/0>
[2024-06-05 16:58:21,559][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 16:58:21,563][HYDRA] 	#1 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 16:58:21,941][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 16:58:21,943][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 16:58:21,944][train.py][INFO] - Instantiating callbacks...
[2024-06-05 16:58:21,945][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 16:58:21,948][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 16:58:21,948][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 16:58:21,949][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 16:58:21,949][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 16:58:21,950][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 16:58:21,951][train.py][INFO] - Instantiating loggers...
[2024-06-05 16:58:21,951][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 16:58:21,952][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 16:58:22,024][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.0/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.71it/s v_num: 0.000      
                                                              val/auc: 0.467    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 0.823  
                                                              train/f1: 0.810   
                                                              train/precision:  
                                                              0.870             
                                                              train/recall:     
                                                              0.758 train/mre:  
                                                              nan               
[2024-06-05 16:58:38,603][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.0/1>
[2024-06-05 16:58:38,604][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 16:58:38,610][HYDRA] 	#2 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 16:58:38,838][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 16:58:38,839][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 16:58:38,841][train.py][INFO] - Instantiating callbacks...
[2024-06-05 16:58:38,841][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 16:58:38,843][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 16:58:38,843][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 16:58:38,844][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 16:58:38,844][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 16:58:38,845][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 16:58:38,846][train.py][INFO] - Instantiating loggers...
[2024-06-05 16:58:38,846][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 16:58:38,847][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 16:58:38,891][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.0/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.54it/s v_num: 0.000      
                                                              val/auc: 0.417    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 0.669  
                                                              train/f1: 0.655   
                                                              train/precision:  
                                                              0.684             
                                                              train/recall:     
                                                              0.629 train/mre:  
                                                              nan               
[2024-06-05 16:58:54,159][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.0/2>
[2024-06-05 16:58:54,160][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 16:58:54,163][HYDRA] 	#3 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 16:58:54,358][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 16:58:54,360][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 16:58:54,361][train.py][INFO] - Instantiating callbacks...
[2024-06-05 16:58:54,361][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 16:58:54,363][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 16:58:54,364][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 16:58:54,364][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 16:58:54,365][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 16:58:54,366][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 16:58:54,369][train.py][INFO] - Instantiating loggers...
[2024-06-05 16:58:54,369][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 16:58:54,370][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 16:58:54,414][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.0/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.48it/s v_num: 0.000      
                                                              val/auc: 0.667    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 0.806  
                                                              train/f1: 0.818   
                                                              train/precision:  
                                                              0.771             
                                                              train/recall:     
                                                              0.871 train/mre:  
                                                              nan               
[2024-06-05 16:59:09,511][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.0/3>
[2024-06-05 16:59:09,512][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 16:59:09,515][HYDRA] 	#4 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 16:59:09,717][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 16:59:09,719][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 16:59:09,720][train.py][INFO] - Instantiating callbacks...
[2024-06-05 16:59:09,720][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 16:59:09,723][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 16:59:09,723][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 16:59:09,723][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 16:59:09,724][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 16:59:09,725][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 16:59:09,726][train.py][INFO] - Instantiating loggers...
[2024-06-05 16:59:09,726][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 16:59:09,728][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 16:59:09,840][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.0/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.60it/s v_num: 0.000      
                                                              val/auc: 0.428    
                                                              val/f1: 0.476     
                                                              val/precision:    
                                                              0.417 val/recall: 
                                                              0.556 val/mre: nan
                                                              train/auc: 0.653  
                                                              train/f1: 0.656   
                                                              train/precision:  
                                                              0.651             
                                                              train/recall:     
                                                              0.661 train/mre:  
                                                              nan               
[2024-06-05 16:59:25,142][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.0/4>
[2024-06-05 16:59:25,142][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 16:59:25,145][HYDRA] 	#5 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 16:59:25,353][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 16:59:25,354][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 16:59:25,356][train.py][INFO] - Instantiating callbacks...
[2024-06-05 16:59:25,356][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 16:59:25,358][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 16:59:25,359][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 16:59:25,359][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 16:59:25,360][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 16:59:25,361][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 16:59:25,361][train.py][INFO] - Instantiating loggers...
[2024-06-05 16:59:25,361][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 16:59:25,363][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 16:59:25,400][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.0/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 58.89it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre: nan
                                                              train/auc: 0.653  
                                                              train/f1: 0.733   
                                                              train/precision:  
                                                              0.596             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              nan               
[2024-06-05 16:59:40,737][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.0/5>
[2024-06-05 16:59:40,738][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 16:59:40,741][HYDRA] 	#6 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 16:59:40,966][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 16:59:40,968][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 16:59:40,969][train.py][INFO] - Instantiating callbacks...
[2024-06-05 16:59:40,969][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 16:59:40,973][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 16:59:40,974][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 16:59:40,974][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 16:59:40,975][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 16:59:40,976][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 16:59:40,976][train.py][INFO] - Instantiating loggers...
[2024-06-05 16:59:40,976][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 16:59:40,978][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 16:59:41,013][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.0/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.70it/s v_num: 0.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.222 val/mre: nan
                                                              train/auc: 0.726  
                                                              train/f1: 0.746   
                                                              train/precision:  
                                                              0.694             
                                                              train/recall:     
                                                              0.806 train/mre:  
                                                              nan               
[2024-06-05 16:59:56,168][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.0/6>
[2024-06-05 16:59:56,168][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 16:59:56,171][HYDRA] 	#7 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 16:59:56,371][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 16:59:56,372][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 16:59:56,374][train.py][INFO] - Instantiating callbacks...
[2024-06-05 16:59:56,374][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 16:59:56,376][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 16:59:56,376][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 16:59:56,377][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 16:59:56,377][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 16:59:56,378][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 16:59:56,381][train.py][INFO] - Instantiating loggers...
[2024-06-05 16:59:56,381][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 16:59:56,382][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 16:59:56,434][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.0/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 66.06it/s v_num: 0.000      
                                                              val/auc: 0.722    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.444 val/mre: nan
                                                              train/auc: 0.847  
                                                              train/f1: 0.826   
                                                              train/precision:  
                                                              0.957             
                                                              train/recall:     
                                                              0.726 train/mre:  
                                                              nan               
[2024-06-05 17:00:11,969][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.0/7>
[2024-06-05 17:00:11,969][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:00:11,975][HYDRA] 	#8 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 17:00:12,202][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:00:12,203][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:00:12,205][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:00:12,205][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:00:12,207][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:00:12,208][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:00:12,208][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:00:12,209][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:00:12,210][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:00:12,211][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:00:12,211][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:00:12,212][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:00:12,355][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.0/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.13it/s v_num: 0.000      
                                                              val/auc: 0.594    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.533 val/recall: 
                                                              0.889 val/mre: nan
                                                              train/auc: 0.669  
                                                              train/f1: 0.735   
                                                              train/precision:  
                                                              0.613             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              nan               
[2024-06-05 17:00:28,230][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.0/8>
[2024-06-05 17:00:28,231][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:00:28,235][HYDRA] 	#9 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 17:00:28,454][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:00:28,455][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:00:28,456][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:00:28,457][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:00:28,459][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:00:28,459][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:00:28,460][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:00:28,460][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:00:28,461][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:00:28,462][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:00:28,462][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:00:28,463][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:00:28,503][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.0/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.99it/s v_num: 0.000      
                                                              val/auc: 0.633    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.667 val/mre: nan
                                                              train/auc: 0.855  
                                                              train/f1: 0.855   
                                                              train/precision:  
                                                              0.855             
                                                              train/recall:     
                                                              0.855 train/mre:  
                                                              nan               
[2024-06-05 17:00:44,759][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.0/9>
[2024-06-05 17:00:44,760][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:00:44,765][HYDRA] 	#10 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 17:00:45,018][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:00:45,019][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:00:45,021][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:00:45,021][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:00:45,023][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:00:45,024][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:00:45,024][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:00:45,025][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:00:45,026][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:00:45,026][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:00:45,026][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:00:45,028][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:00:45,071][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.05/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.39it/s v_num: 0.000      
                                                              val/auc: 0.633    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.667 val/mre:    
                                                              0.092 train/auc:  
                                                              0.798 train/f1:   
                                                              0.790             
                                                              train/precision:  
                                                              0.825             
                                                              train/recall:     
                                                              0.758 train/mre:  
                                                              0.115             
[2024-06-05 17:01:00,360][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.05/0>
[2024-06-05 17:01:00,360][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:01:00,363][HYDRA] 	#11 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 17:01:00,573][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:01:00,574][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:01:00,576][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:01:00,576][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:01:00,578][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:01:00,579][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:01:00,579][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:01:00,580][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:01:00,581][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:01:00,583][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:01:00,584][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:01:00,585][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:01:00,630][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.05/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.06it/s v_num: 0.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.222 val/mre:    
                                                              0.100 train/auc:  
                                                              0.903 train/f1:   
                                                              0.905             
                                                              train/precision:  
                                                              0.891             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.114             
[2024-06-05 17:01:15,675][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.05/1>
[2024-06-05 17:01:15,676][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:01:15,681][HYDRA] 	#12 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 17:01:15,876][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:01:15,878][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:01:15,879][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:01:15,879][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:01:15,881][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:01:15,882][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:01:15,882][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:01:15,883][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:01:15,884][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:01:15,885][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:01:15,885][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:01:15,886][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:01:15,987][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.05/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.36it/s v_num: 0.000      
                                                              val/auc: 0.561    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.222 val/mre:    
                                                              0.097 train/auc:  
                                                              0.710 train/f1:   
                                                              0.735             
                                                              train/precision:  
                                                              0.676             
                                                              train/recall:     
                                                              0.806 train/mre:  
                                                              0.114             
[2024-06-05 17:01:31,178][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.05/2>
[2024-06-05 17:01:31,178][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:01:31,181][HYDRA] 	#13 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 17:01:31,385][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:01:31,386][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:01:31,387][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:01:31,388][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:01:31,390][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:01:31,390][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:01:31,391][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:01:31,391][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:01:31,392][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:01:31,393][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:01:31,393][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:01:31,394][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:01:31,437][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.05/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.96it/s v_num: 0.000      
                                                              val/auc: 0.728    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.833 val/recall: 
                                                              0.556 val/mre:    
                                                              0.091 train/auc:  
                                                              0.694 train/f1:   
                                                              0.716             
                                                              train/precision:  
                                                              0.667             
                                                              train/recall:     
                                                              0.774 train/mre:  
                                                              0.103             
[2024-06-05 17:01:46,887][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.05/3>
[2024-06-05 17:01:46,888][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:01:46,892][HYDRA] 	#14 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 17:01:47,107][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:01:47,108][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:01:47,110][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:01:47,110][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:01:47,112][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:01:47,113][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:01:47,113][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:01:47,114][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:01:47,115][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:01:47,115][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:01:47,115][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:01:47,117][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:01:47,159][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.05/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.99it/s v_num: 0.000      
                                                              val/auc: 0.683    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.667 val/mre:    
                                                              0.090 train/auc:  
                                                              0.629 train/f1:   
                                                              0.652             
                                                              train/precision:  
                                                              0.614             
                                                              train/recall:     
                                                              0.694 train/mre:  
                                                              0.107             
[2024-06-05 17:02:02,583][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.05/4>
[2024-06-05 17:02:02,585][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:02:02,590][HYDRA] 	#15 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 17:02:02,782][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:02:02,783][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:02:02,784][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:02:02,785][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:02:02,787][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:02:02,787][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:02:02,788][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:02:02,788][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:02:02,789][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:02:02,792][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:02:02,792][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:02:02,793][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:02:02,835][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.05/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.09it/s v_num: 0.000      
                                                              val/auc: 0.639    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.583 val/recall: 
                                                              0.778 val/mre:    
                                                              0.091 train/auc:  
                                                              0.718 train/f1:   
                                                              0.741             
                                                              train/precision:  
                                                              0.685             
                                                              train/recall:     
                                                              0.806 train/mre:  
                                                              0.120             
[2024-06-05 17:02:18,115][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.05/5>
[2024-06-05 17:02:18,115][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:02:18,118][HYDRA] 	#16 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 17:02:18,321][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:02:18,322][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:02:18,323][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:02:18,324][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:02:18,326][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:02:18,326][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:02:18,327][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:02:18,327][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:02:18,328][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:02:18,329][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:02:18,329][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:02:18,331][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:02:18,439][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.05/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.36it/s v_num: 0.000      
                                                              val/auc: 0.678    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.556 val/mre:    
                                                              0.104 train/auc:  
                                                              0.573 train/f1:   
                                                              0.607             
                                                              train/precision:  
                                                              0.562             
                                                              train/recall:     
                                                              0.661 train/mre:  
                                                              0.116             
[2024-06-05 17:02:33,635][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.05/6>
[2024-06-05 17:02:33,636][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:02:33,638][HYDRA] 	#17 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 17:02:33,838][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:02:33,839][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:02:33,840][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:02:33,841][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:02:33,843][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:02:33,843][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:02:33,844][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:02:33,844][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:02:33,845][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:02:33,846][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:02:33,846][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:02:33,847][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:02:33,885][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.05/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.98it/s v_num: 0.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.222 val/mre:    
                                                              0.100 train/auc:  
                                                              0.806 train/f1:   
                                                              0.829             
                                                              train/precision:  
                                                              0.744             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.119             
[2024-06-05 17:02:49,200][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.05/7>
[2024-06-05 17:02:49,203][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:02:49,209][HYDRA] 	#18 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 17:02:49,655][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:02:49,656][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:02:49,657][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:02:49,658][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:02:49,660][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:02:49,661][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:02:49,661][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:02:49,661][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:02:49,663][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:02:49,663][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:02:49,663][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:02:49,665][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:02:49,713][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.05/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.27it/s v_num: 0.000      
                                                              val/auc: 0.728    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.833 val/recall: 
                                                              0.556 val/mre:    
                                                              0.092 train/auc:  
                                                              0.661 train/f1:   
                                                              0.687             
                                                              train/precision:  
                                                              0.639             
                                                              train/recall:     
                                                              0.742 train/mre:  
                                                              0.103             
[2024-06-05 17:03:05,231][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.05/8>
[2024-06-05 17:03:05,233][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:03:05,237][HYDRA] 	#19 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 17:03:05,442][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:03:05,443][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:03:05,445][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:03:05,445][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:03:05,447][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:03:05,447][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:03:05,448][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:03:05,448][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:03:05,449][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:03:05,452][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:03:05,452][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:03:05,453][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:03:05,500][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.05/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.86it/s v_num: 0.000      
                                                              val/auc: 0.689    
                                                              val/f1: 0.700     
                                                              val/precision:    
                                                              0.636 val/recall: 
                                                              0.778 val/mre:    
                                                              0.095 train/auc:  
                                                              0.815 train/f1:   
                                                              0.789             
                                                              train/precision:  
                                                              0.915             
                                                              train/recall:     
                                                              0.694 train/mre:  
                                                              0.118             
[2024-06-05 17:03:21,066][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.05/9>
[2024-06-05 17:03:21,066][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:03:21,070][HYDRA] 	#20 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 17:03:21,297][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:03:21,299][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:03:21,301][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:03:21,301][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:03:21,303][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:03:21,303][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:03:21,304][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:03:21,304][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:03:21,305][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:03:21,306][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:03:21,307][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:03:21,308][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:03:21,449][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.1/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.84it/s v_num: 0.000      
                                                              val/auc: 0.633    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.667 val/mre:    
                                                              0.102 train/auc:  
                                                              0.847 train/f1:   
                                                              0.832             
                                                              train/precision:  
                                                              0.922             
                                                              train/recall:     
                                                              0.758 train/mre:  
                                                              0.126             
[2024-06-05 17:03:36,827][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.1/0>
[2024-06-05 17:03:36,828][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:03:36,830][HYDRA] 	#21 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 17:03:37,041][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:03:37,043][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:03:37,044][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:03:37,044][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:03:37,046][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:03:37,047][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:03:37,047][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:03:37,048][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:03:37,049][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:03:37,049][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:03:37,049][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:03:37,051][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:03:37,091][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.1/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.61it/s v_num: 0.000      
                                                              val/auc: 0.722    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.444 val/mre:    
                                                              0.098 train/auc:  
                                                              0.750 train/f1:   
                                                              0.770             
                                                              train/precision:  
                                                              0.712             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.108             
[2024-06-05 17:03:53,017][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.1/1>
[2024-06-05 17:03:53,017][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:03:53,020][HYDRA] 	#22 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 17:03:53,244][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:03:53,245][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:03:53,247][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:03:53,247][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:03:53,249][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:03:53,250][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:03:53,250][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:03:53,250][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:03:53,251][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:03:53,252][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:03:53,252][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:03:53,254][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:03:53,296][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.1/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.74it/s v_num: 0.000      
                                                              val/auc: 0.422    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.444 val/mre:    
                                                              0.117 train/auc:  
                                                              0.718 train/f1:   
                                                              0.755             
                                                              train/precision:  
                                                              0.667             
                                                              train/recall:     
                                                              0.871 train/mre:  
                                                              0.129             
[2024-06-05 17:04:08,512][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.1/2>
[2024-06-05 17:04:08,514][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:04:08,517][HYDRA] 	#23 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 17:04:08,717][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:04:08,719][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:04:08,720][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:04:08,720][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:04:08,723][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:04:08,723][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:04:08,724][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:04:08,724][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:04:08,725][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:04:08,728][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:04:08,728][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:04:08,729][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:04:08,770][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.1/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.79it/s v_num: 0.000      
                                                              val/auc: 0.678    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.556 val/mre:    
                                                              0.092 train/auc:  
                                                              0.734 train/f1:   
                                                              0.779             
                                                              train/precision:  
                                                              0.667             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.101             
[2024-06-05 17:04:24,494][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.1/3>
[2024-06-05 17:04:24,495][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:04:24,500][HYDRA] 	#24 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 17:04:24,800][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:04:24,801][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:04:24,803][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:04:24,803][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:04:24,806][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:04:24,806][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:04:24,807][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:04:24,807][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:04:24,808][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:04:24,809][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:04:24,810][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:04:24,811][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:04:25,014][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.1/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.51it/s v_num: 0.000      
                                                              val/auc: 0.667    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.333 val/mre:    
                                                              0.098 train/auc:  
                                                              0.742 train/f1:   
                                                              0.765             
                                                              train/precision:  
                                                              0.703             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.109             
[2024-06-05 17:04:40,516][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.1/4>
[2024-06-05 17:04:40,516][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:04:40,522][HYDRA] 	#25 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 17:04:40,726][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:04:40,728][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:04:40,729][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:04:40,729][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:04:40,732][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:04:40,732][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:04:40,733][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:04:40,733][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:04:40,734][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:04:40,735][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:04:40,735][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:04:40,736][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:04:40,774][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.1/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.73it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.087 train/auc:  
                                                              0.710 train/f1:   
                                                              0.739             
                                                              train/precision:  
                                                              0.671             
                                                              train/recall:     
                                                              0.823 train/mre:  
                                                              0.107             
[2024-06-05 17:04:57,194][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.1/5>
[2024-06-05 17:04:57,196][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:04:57,202][HYDRA] 	#26 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 17:04:57,428][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:04:57,430][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:04:57,431][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:04:57,432][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:04:57,435][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:04:57,435][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:04:57,436][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:04:57,436][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:04:57,437][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:04:57,438][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:04:57,438][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:04:57,439][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:04:57,499][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.1/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.67it/s v_num: 0.000      
                                                              val/auc: 0.683    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.667 val/mre:    
                                                              0.086 train/auc:  
                                                              0.661 train/f1:   
                                                              0.672             
                                                              train/precision:  
                                                              0.652             
                                                              train/recall:     
                                                              0.694 train/mre:  
                                                              0.108             
[2024-06-05 17:05:14,148][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.1/6>
[2024-06-05 17:05:14,148][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:05:14,152][HYDRA] 	#27 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 17:05:14,354][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:05:14,356][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:05:14,357][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:05:14,357][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:05:14,361][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:05:14,362][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:05:14,362][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:05:14,363][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:05:14,364][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:05:14,366][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:05:14,366][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:05:14,601][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:05:14,640][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.1/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.68it/s v_num: 0.000      
                                                              val/auc: 0.628    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre:    
                                                              0.095 train/auc:  
                                                              0.798 train/f1:   
                                                              0.818             
                                                              train/precision:  
                                                              0.747             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.107             
[2024-06-05 17:05:31,026][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.1/7>
[2024-06-05 17:05:31,028][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:05:31,033][HYDRA] 	#28 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 17:05:31,245][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:05:31,246][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:05:31,248][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:05:31,248][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:05:31,250][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:05:31,251][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:05:31,251][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:05:31,252][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:05:31,253][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:05:31,254][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:05:31,254][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:05:31,255][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:05:31,416][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.1/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.72it/s v_num: 0.000      
                                                              val/auc: 0.667    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.333 val/mre:    
                                                              0.091 train/auc:  
                                                              0.629 train/f1:   
                                                              0.589             
                                                              train/precision:  
                                                              0.660             
                                                              train/recall:     
                                                              0.532 train/mre:  
                                                              0.108             
[2024-06-05 17:05:47,136][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.1/8>
[2024-06-05 17:05:47,136][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:05:47,139][HYDRA] 	#29 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 17:05:47,345][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:05:47,346][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:05:47,348][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:05:47,348][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:05:47,351][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:05:47,351][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:05:47,352][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:05:47,352][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:05:47,353][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:05:47,354][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:05:47,354][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:05:47,355][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:05:47,390][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.1/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.19it/s v_num: 0.000      
                                                              val/auc: 0.667    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.333 val/mre:    
                                                              0.101 train/auc:  
                                                              0.847 train/f1:   
                                                              0.867             
                                                              train/precision:  
                                                              0.765             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.115             
[2024-06-05 17:06:02,523][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.1/9>
[2024-06-05 17:06:02,524][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:06:02,527][HYDRA] 	#30 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 17:06:02,730][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:06:02,732][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:06:02,733][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:06:02,733][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:06:02,736][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:06:02,736][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:06:02,736][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:06:02,737][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:06:02,738][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:06:02,738][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:06:02,739][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:06:02,740][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:06:02,788][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.15/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.64it/s v_num: 0.000      
                                                              val/auc: 0.583    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 val/mre:    
                                                              0.091 train/auc:  
                                                              0.927 train/f1:   
                                                              0.926             
                                                              train/precision:  
                                                              0.949             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.117             
[2024-06-05 17:06:17,733][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.15/0>
[2024-06-05 17:06:17,733][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:06:17,738][HYDRA] 	#31 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 17:06:17,932][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:06:17,934][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:06:17,935][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:06:17,935][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:06:17,938][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:06:17,938][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:06:17,938][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:06:17,939][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:06:17,940][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:06:17,942][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:06:17,943][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:06:17,944][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:06:17,991][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.15/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 66.28it/s v_num: 0.000      
                                                              val/auc: 0.417    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.333 val/mre:    
                                                              0.091 train/auc:  
                                                              0.863 train/f1:   
                                                              0.857             
                                                              train/precision:  
                                                              0.895             
                                                              train/recall:     
                                                              0.823 train/mre:  
                                                              0.116             
[2024-06-05 17:06:33,718][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.15/1>
[2024-06-05 17:06:33,719][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:06:33,722][HYDRA] 	#32 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 17:06:33,927][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:06:33,929][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:06:33,930][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:06:33,931][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:06:33,933][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:06:33,933][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:06:33,934][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:06:33,934][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:06:33,935][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:06:33,936][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:06:33,937][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:06:33,938][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:06:34,029][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.15/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.06it/s v_num: 0.000      
                                                              val/auc: 0.678    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.556 val/mre:    
                                                              0.085 train/auc:  
                                                              0.806 train/f1:   
                                                              0.806             
                                                              train/precision:  
                                                              0.806             
                                                              train/recall:     
                                                              0.806 train/mre:  
                                                              0.110             
[2024-06-05 17:06:49,138][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.15/2>
[2024-06-05 17:06:49,139][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:06:49,142][HYDRA] 	#33 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 17:06:49,341][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:06:49,343][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:06:49,344][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:06:49,344][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:06:49,347][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:06:49,347][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:06:49,348][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:06:49,348][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:06:49,349][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:06:49,350][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:06:49,350][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:06:49,351][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:06:49,390][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.15/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.63it/s v_num: 0.000      
                                                              val/auc: 0.628    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre:    
                                                              0.094 train/auc:  
                                                              0.718 train/f1:   
                                                              0.752             
                                                              train/precision:  
                                                              0.671             
                                                              train/recall:     
                                                              0.855 train/mre:  
                                                              0.103             
[2024-06-05 17:07:04,442][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.15/3>
[2024-06-05 17:07:04,443][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:07:04,446][HYDRA] 	#34 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 17:07:04,653][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:07:04,654][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:07:04,655][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:07:04,656][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:07:04,658][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:07:04,659][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:07:04,659][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:07:04,660][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:07:04,661][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:07:04,661][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:07:04,662][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:07:04,663][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:07:04,703][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.15/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.75it/s v_num: 0.000      
                                                              val/auc: 0.628    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre:    
                                                              0.090 train/auc:  
                                                              0.710 train/f1:   
                                                              0.731             
                                                              train/precision:  
                                                              0.681             
                                                              train/recall:     
                                                              0.790 train/mre:  
                                                              0.108             
[2024-06-05 17:07:19,884][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.15/4>
[2024-06-05 17:07:19,885][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:07:19,888][HYDRA] 	#35 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 17:07:20,089][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:07:20,090][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:07:20,091][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:07:20,092][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:07:20,094][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:07:20,094][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:07:20,095][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:07:20,095][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:07:20,096][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:07:20,098][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:07:20,098][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:07:20,100][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:07:20,139][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.15/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.20it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.092 train/auc:  
                                                              0.653 train/f1:   
                                                              0.547             
                                                              train/precision:  
                                                              0.788             
                                                              train/recall:     
                                                              0.419 train/mre:  
                                                              0.115             
[2024-06-05 17:07:35,383][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.15/5>
[2024-06-05 17:07:35,383][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:07:35,387][HYDRA] 	#36 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 17:07:35,582][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:07:35,583][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:07:35,585][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:07:35,585][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:07:35,587][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:07:35,588][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:07:35,588][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:07:35,589][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:07:35,590][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:07:35,591][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:07:35,591][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:07:35,592][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:07:35,691][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.15/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 58.84it/s v_num: 0.000      
                                                              val/auc: 0.589    
                                                              val/f1: 0.636     
                                                              val/precision:    
                                                              0.538 val/recall: 
                                                              0.778 val/mre:    
                                                              0.089 train/auc:  
                                                              0.613 train/f1:   
                                                              0.579             
                                                              train/precision:  
                                                              0.635             
                                                              train/recall:     
                                                              0.532 train/mre:  
                                                              0.103             
[2024-06-05 17:07:51,243][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.15/6>
[2024-06-05 17:07:51,246][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:07:51,250][HYDRA] 	#37 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 17:07:51,457][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:07:51,458][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:07:51,460][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:07:51,460][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:07:51,464][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:07:51,464][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:07:51,465][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:07:51,465][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:07:51,466][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:07:51,467][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:07:51,467][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:07:51,468][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:07:51,506][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.15/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.39it/s v_num: 0.000      
                                                              val/auc: 0.456    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.111 val/mre:    
                                                              0.088 train/auc:  
                                                              0.879 train/f1:   
                                                              0.885             
                                                              train/precision:  
                                                              0.841             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.113             
[2024-06-05 17:08:06,676][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.15/7>
[2024-06-05 17:08:06,677][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:08:06,681][HYDRA] 	#38 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 17:08:06,892][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:08:06,893][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:08:06,894][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:08:06,895][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:08:06,897][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:08:06,897][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:08:06,898][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:08:06,898][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:08:06,899][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:08:06,900][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:08:06,900][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:08:06,901][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:08:06,938][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.15/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.69it/s v_num: 0.000      
                                                              val/auc: 0.533    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.667 val/mre:    
                                                              0.086 train/auc:  
                                                              0.742 train/f1:   
                                                              0.771             
                                                              train/precision:  
                                                              0.692             
                                                              train/recall:     
                                                              0.871 train/mre:  
                                                              0.107             
[2024-06-05 17:08:22,293][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.15/8>
[2024-06-05 17:08:22,293][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:08:22,299][HYDRA] 	#39 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 17:08:22,534][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:08:22,535][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:08:22,536][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:08:22,537][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:08:22,539][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:08:22,539][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:08:22,540][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:08:22,540][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:08:22,541][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:08:22,544][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:08:22,544][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:08:22,546][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:08:22,616][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.15/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.22it/s v_num: 0.000      
                                                              val/auc: 0.472    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.444 val/mre:    
                                                              0.096 train/auc:  
                                                              0.919 train/f1:   
                                                              0.918             
                                                              train/precision:  
                                                              0.933             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.115             
[2024-06-05 17:08:39,279][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.15/9>
[2024-06-05 17:08:39,280][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:08:39,282][HYDRA] 	#40 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 17:08:39,520][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:08:39,521][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:08:39,523][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:08:39,523][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:08:39,527][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:08:39,528][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:08:39,528][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:08:39,529][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:08:39,530][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:08:39,531][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:08:39,531][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:08:39,532][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:08:39,656][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.2/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.66it/s v_num: 0.000      
                                                              val/auc: 0.644    
                                                              val/f1: 0.696     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.889 val/mre:    
                                                              0.105 train/auc:  
                                                              0.839 train/f1:   
                                                              0.815             
                                                              train/precision:  
                                                              0.957             
                                                              train/recall:     
                                                              0.710 train/mre:  
                                                              0.119             
[2024-06-05 17:08:55,456][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.2/0>
[2024-06-05 17:08:55,457][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:08:55,463][HYDRA] 	#41 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 17:08:55,659][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:08:55,661][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:08:55,662][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:08:55,663][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:08:55,665][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:08:55,665][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:08:55,665][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:08:55,666][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:08:55,667][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:08:55,667][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:08:55,668][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:08:55,669][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:08:55,706][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.2/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.60it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.100 train/auc:  
                                                              0.823 train/f1:   
                                                              0.828             
                                                              train/precision:  
                                                              0.803             
                                                              train/recall:     
                                                              0.855 train/mre:  
                                                              0.111             
[2024-06-05 17:09:11,247][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.2/1>
[2024-06-05 17:09:11,249][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:09:11,255][HYDRA] 	#42 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 17:09:11,478][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:09:11,479][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:09:11,480][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:09:11,481][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:09:11,483][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:09:11,483][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:09:11,484][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:09:11,484][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:09:11,485][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:09:11,486][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:09:11,486][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:09:11,487][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:09:11,542][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.2/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.57it/s v_num: 0.000      
                                                              val/auc: 0.583    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 val/mre:    
                                                              0.088 train/auc:  
                                                              0.847 train/f1:   
                                                              0.832             
                                                              train/precision:  
                                                              0.922             
                                                              train/recall:     
                                                              0.758 train/mre:  
                                                              0.119             
[2024-06-05 17:09:27,024][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.2/2>
[2024-06-05 17:09:27,025][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:09:27,030][HYDRA] 	#43 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 17:09:27,245][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:09:27,246][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:09:27,248][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:09:27,248][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:09:27,250][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:09:27,251][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:09:27,251][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:09:27,252][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:09:27,253][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:09:27,255][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:09:27,255][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:09:27,257][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:09:27,309][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.2/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.49it/s v_num: 0.000      
                                                              val/auc: 0.622    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.444 val/mre:    
                                                              0.095 train/auc:  
                                                              0.742 train/f1:   
                                                              0.765             
                                                              train/precision:  
                                                              0.703             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.104             
[2024-06-05 17:09:42,463][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.2/3>
[2024-06-05 17:09:42,464][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:09:42,469][HYDRA] 	#44 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 17:09:42,672][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:09:42,673][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:09:42,675][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:09:42,675][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:09:42,677][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:09:42,677][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:09:42,678][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:09:42,678][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:09:42,680][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:09:42,681][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:09:42,681][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:09:42,682][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:09:42,795][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.2/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.20it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.097 train/auc:  
                                                              0.702 train/f1:   
                                                              0.726             
                                                              train/precision:  
                                                              0.671             
                                                              train/recall:     
                                                              0.790 train/mre:  
                                                              0.109             
[2024-06-05 17:09:58,067][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.2/4>
[2024-06-05 17:09:58,069][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:09:58,074][HYDRA] 	#45 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 17:09:58,274][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:09:58,275][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:09:58,276][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:09:58,277][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:09:58,279][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:09:58,279][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:09:58,280][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:09:58,280][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:09:58,281][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:09:58,282][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:09:58,282][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:09:58,283][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:09:58,319][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.2/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 56.56it/s v_num: 0.000      
                                                              val/auc: 0.611    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.222 val/mre:    
                                                              0.087 train/auc:  
                                                              0.766 train/f1:   
                                                              0.797             
                                                              train/precision:  
                                                              0.704             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.107             
[2024-06-05 17:10:14,042][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.2/5>
[2024-06-05 17:10:14,044][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:10:14,047][HYDRA] 	#46 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 17:10:14,247][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:10:14,249][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:10:14,250][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:10:14,250][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:10:14,253][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:10:14,253][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:10:14,254][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:10:14,254][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:10:14,255][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:10:14,255][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:10:14,256][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:10:14,257][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:10:14,300][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.2/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.93it/s v_num: 0.000      
                                                              val/auc: 0.533    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.667 val/mre:    
                                                              0.084 train/auc:  
                                                              0.702 train/f1:   
                                                              0.709             
                                                              train/precision:  
                                                              0.692             
                                                              train/recall:     
                                                              0.726 train/mre:  
                                                              0.106             
[2024-06-05 17:10:30,142][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.2/6>
[2024-06-05 17:10:30,142][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:10:30,147][HYDRA] 	#47 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 17:10:30,379][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:10:30,381][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:10:30,382][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:10:30,382][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:10:30,386][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:10:30,386][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:10:30,387][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:10:30,387][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:10:30,388][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:10:30,391][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:10:30,391][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:10:30,392][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:10:30,454][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.2/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.82it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.089 train/auc:  
                                                              0.871 train/f1:   
                                                              0.881             
                                                              train/precision:  
                                                              0.819             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.118             
[2024-06-05 17:10:45,869][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.2/7>
[2024-06-05 17:10:45,870][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:10:45,874][HYDRA] 	#48 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 17:10:46,080][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:10:46,081][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:10:46,082][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:10:46,083][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:10:46,085][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:10:46,086][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:10:46,086][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:10:46,087][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:10:46,088][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:10:46,089][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:10:46,089][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:10:46,091][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:10:46,195][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.2/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.64it/s v_num: 0.000      
                                                              val/auc: 0.683    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.667 val/mre:    
                                                              0.091 train/auc:  
                                                              0.718 train/f1:   
                                                              0.696             
                                                              train/precision:  
                                                              0.755             
                                                              train/recall:     
                                                              0.645 train/mre:  
                                                              0.104             
[2024-06-05 17:11:01,546][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.2/8>
[2024-06-05 17:11:01,546][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:11:01,549][HYDRA] 	#49 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 17:11:01,753][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:11:01,755][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:11:01,756][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:11:01,756][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:11:01,758][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:11:01,759][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:11:01,759][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:11:01,760][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:11:01,761][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:11:01,761][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:11:01,761][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:11:01,763][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:11:01,808][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.2/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.09it/s v_num: 0.000      
                                                              val/auc: 0.722    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.444 val/mre:    
                                                              0.102 train/auc:  
                                                              0.895 train/f1:   
                                                              0.905             
                                                              train/precision:  
                                                              0.827             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.115             
[2024-06-05 17:11:16,768][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.2/9>
[2024-06-05 17:11:16,768][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:11:16,770][HYDRA] 	#50 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 17:11:16,975][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:11:16,977][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:11:16,978][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:11:16,979][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:11:16,981][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:11:16,981][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:11:16,982][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:11:16,982][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:11:16,983][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:11:16,984][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:11:16,984][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:11:16,986][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:11:17,021][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.25/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.13it/s v_num: 0.000      
                                                              val/auc: 0.539    
                                                              val/f1: 0.609     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.778 val/mre:    
                                                              0.094 train/auc:  
                                                              0.879 train/f1:   
                                                              0.865             
                                                              train/precision:  
                                                              0.980             
                                                              train/recall:     
                                                              0.774 train/mre:  
                                                              0.116             
[2024-06-05 17:11:32,351][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.25/0>
[2024-06-05 17:11:32,351][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:11:32,356][HYDRA] 	#51 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 17:11:32,556][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:11:32,557][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:11:32,558][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:11:32,559][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:11:32,561][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:11:32,561][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:11:32,562][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:11:32,562][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:11:32,563][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:11:32,566][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:11:32,566][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:11:32,567][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:11:32,613][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.25/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.15it/s v_num: 0.000      
                                                              val/auc: 0.633    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.667 val/mre:    
                                                              0.095 train/auc:  
                                                              0.831 train/f1:   
                                                              0.840             
                                                              train/precision:  
                                                              0.797             
                                                              train/recall:     
                                                              0.887 train/mre:  
                                                              0.110             
[2024-06-05 17:11:47,617][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.25/1>
[2024-06-05 17:11:47,618][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:11:47,621][HYDRA] 	#52 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 17:11:47,837][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:11:47,839][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:11:47,840][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:11:47,840][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:11:47,842][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:11:47,843][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:11:47,843][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:11:47,844][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:11:47,845][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:11:47,846][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:11:47,846][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:11:47,848][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:11:47,954][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.25/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.25it/s v_num: 0.000      
                                                              val/auc: 0.739    
                                                              val/f1: 0.737     
                                                              val/precision:    
                                                              0.700 val/recall: 
                                                              0.778 val/mre:    
                                                              0.088 train/auc:  
                                                              0.855 train/f1:   
                                                              0.852             
                                                              train/precision:  
                                                              0.867             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.116             
[2024-06-05 17:12:03,999][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.25/2>
[2024-06-05 17:12:03,999][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:12:04,002][HYDRA] 	#53 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 17:12:04,199][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:12:04,200][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:12:04,202][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:12:04,202][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:12:04,205][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:12:04,205][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:12:04,206][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:12:04,206][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:12:04,207][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:12:04,208][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:12:04,208][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:12:04,209][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:12:04,244][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.25/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.25it/s v_num: 0.000      
                                                              val/auc: 0.589    
                                                              val/f1: 0.636     
                                                              val/precision:    
                                                              0.538 val/recall: 
                                                              0.778 val/mre:    
                                                              0.091 train/auc:  
                                                              0.758 train/f1:   
                                                              0.786             
                                                              train/precision:  
                                                              0.705             
                                                              train/recall:     
                                                              0.887 train/mre:  
                                                              0.113             
[2024-06-05 17:12:19,353][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.25/3>
[2024-06-05 17:12:19,354][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:12:19,356][HYDRA] 	#54 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 17:12:19,565][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:12:19,566][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:12:19,567][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:12:19,568][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:12:19,570][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:12:19,570][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:12:19,571][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:12:19,571][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:12:19,572][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:12:19,573][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:12:19,573][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:12:19,575][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:12:19,609][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.25/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.54it/s v_num: 0.000      
                                                              val/auc: 0.511    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.222 val/mre:    
                                                              0.091 train/auc:  
                                                              0.645 train/f1:   
                                                              0.645             
                                                              train/precision:  
                                                              0.645             
                                                              train/recall:     
                                                              0.645 train/mre:  
                                                              0.103             
[2024-06-05 17:12:35,106][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.25/4>
[2024-06-05 17:12:35,106][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:12:35,109][HYDRA] 	#55 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 17:12:35,315][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:12:35,317][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:12:35,318][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:12:35,318][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:12:35,321][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:12:35,321][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:12:35,322][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:12:35,322][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:12:35,323][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:12:35,325][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:12:35,325][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:12:35,327][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:12:35,367][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.25/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 49.93it/s v_num: 0.000      
                                                              val/auc: 0.683    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.667 val/mre:    
                                                              0.096 train/auc:  
                                                              0.847 train/f1:   
                                                              0.843             
                                                              train/precision:  
                                                              0.864             
                                                              train/recall:     
                                                              0.823 train/mre:  
                                                              0.112             
[2024-06-05 17:12:51,399][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.25/5>
[2024-06-05 17:12:51,400][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:12:51,402][HYDRA] 	#56 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[2024-06-05 17:12:51,589][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:12:51,590][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:12:51,592][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:12:51,592][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[rank: 0] Seed set to 6
[2024-06-05 17:12:51,594][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:12:51,595][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:12:51,595][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:12:51,595][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:12:51,596][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:12:51,597][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:12:51,597][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:12:51,599][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:12:51,696][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.25/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 50.62it/s v_num: 0.000      
                                                              val/auc: 0.617    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.333 val/mre:    
                                                              0.106 train/auc:  
                                                              0.605 train/f1:   
                                                              0.620             
                                                              train/precision:  
                                                              0.597             
                                                              train/recall:     
                                                              0.645 train/mre:  
                                                              0.119             
[2024-06-05 17:13:07,780][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.25/6>
[2024-06-05 17:13:07,780][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:13:07,783][HYDRA] 	#57 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 17:13:07,981][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:13:07,983][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:13:07,984][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:13:07,984][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:13:07,987][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:13:07,988][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:13:07,989][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:13:07,989][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:13:07,990][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:13:07,991][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:13:07,991][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:13:07,992][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:13:08,031][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.25/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 55.26it/s v_num: 0.000      
                                                              val/auc: 0.572    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.444 val/mre:    
                                                              0.096 train/auc:  
                                                              0.798 train/f1:   
                                                              0.820             
                                                              train/precision:  
                                                              0.740             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.106             
[2024-06-05 17:13:24,152][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.25/7>
[2024-06-05 17:13:24,153][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:13:24,155][HYDRA] 	#58 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 17:13:24,364][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:13:24,365][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:13:24,367][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:13:24,367][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:13:24,369][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:13:24,369][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:13:24,370][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:13:24,370][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:13:24,371][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:13:24,372][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:13:24,372][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:13:24,373][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:13:24,408][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.25/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 58.08it/s v_num: 0.000      
                                                              val/auc: 0.722    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.444 val/mre:    
                                                              0.095 train/auc:  
                                                              0.637 train/f1:   
                                                              0.646             
                                                              train/precision:  
                                                              0.631             
                                                              train/recall:     
                                                              0.661 train/mre:  
                                                              0.106             
[2024-06-05 17:13:39,826][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.25/8>
[2024-06-05 17:13:39,826][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:13:39,829][HYDRA] 	#59 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 17:13:40,018][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:13:40,019][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:13:40,021][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:13:40,021][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:13:40,023][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:13:40,024][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:13:40,024][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:13:40,025][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:13:40,026][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:13:40,028][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:13:40,028][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:13:40,029][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:13:40,070][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.25/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 57.54it/s v_num: 0.000      
                                                              val/auc: 0.594    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.533 val/recall: 
                                                              0.889 val/mre:    
                                                              0.089 train/auc:  
                                                              0.911 train/f1:   
                                                              0.904             
                                                              train/precision:  
                                                              0.981             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.110             
[2024-06-05 17:13:55,632][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.25/9>
[2024-06-05 17:13:55,633][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:13:55,641][HYDRA] 	#60 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 17:13:55,846][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:13:55,847][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:13:55,849][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:13:55,849][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:13:55,851][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:13:55,852][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:13:55,852][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:13:55,852][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:13:55,854][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:13:55,855][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:13:55,855][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:13:55,856][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:13:55,968][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.3/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.92it/s v_num: 0.000      
                                                              val/auc: 0.583    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 val/mre:    
                                                              0.097 train/auc:  
                                                              0.855 train/f1:   
                                                              0.845             
                                                              train/precision:  
                                                              0.907             
                                                              train/recall:     
                                                              0.790 train/mre:  
                                                              0.113             
[2024-06-05 17:14:11,190][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.3/0>
[2024-06-05 17:14:11,191][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:14:11,193][HYDRA] 	#61 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 17:14:11,399][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:14:11,400][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:14:11,401][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:14:11,402][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:14:11,404][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:14:11,404][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:14:11,405][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:14:11,405][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:14:11,406][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:14:11,407][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:14:11,407][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:14:11,408][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:14:11,445][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.3/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.61it/s v_num: 0.000      
                                                              val/auc: 0.517    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre:    
                                                              0.098 train/auc:  
                                                              0.871 train/f1:   
                                                              0.869             
                                                              train/precision:  
                                                              0.883             
                                                              train/recall:     
                                                              0.855 train/mre:  
                                                              0.116             
[2024-06-05 17:14:26,470][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.3/1>
[2024-06-05 17:14:26,470][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:14:26,474][HYDRA] 	#62 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 17:14:26,677][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:14:26,678][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:14:26,680][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:14:26,680][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:14:26,682][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:14:26,683][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:14:26,683][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:14:26,684][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:14:26,685][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:14:26,685][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:14:26,685][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:14:26,687][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:14:26,725][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.3/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.01it/s v_num: 0.000      
                                                              val/auc: 0.628    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.556 val/mre:    
                                                              0.092 train/auc:  
                                                              0.879 train/f1:   
                                                              0.878             
                                                              train/precision:  
                                                              0.885             
                                                              train/recall:     
                                                              0.871 train/mre:  
                                                              0.118             
[2024-06-05 17:14:41,768][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.3/2>
[2024-06-05 17:14:41,768][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:14:41,771][HYDRA] 	#63 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 17:14:41,965][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:14:41,967][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:14:41,968][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:14:41,968][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:14:41,971][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:14:41,971][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:14:41,972][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:14:41,972][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:14:41,973][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:14:41,976][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:14:41,976][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:14:41,977][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:14:42,026][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.3/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.86it/s v_num: 0.000      
                                                              val/auc: 0.633    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.667 val/mre:    
                                                              0.092 train/auc:  
                                                              0.710 train/f1:   
                                                              0.727             
                                                              train/precision:  
                                                              0.686             
                                                              train/recall:     
                                                              0.774 train/mre:  
                                                              0.103             
[2024-06-05 17:14:56,870][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.3/3>
[2024-06-05 17:14:56,871][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:14:56,875][HYDRA] 	#64 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 17:14:57,064][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:14:57,065][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:14:57,067][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:14:57,067][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:14:57,069][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:14:57,070][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:14:57,070][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:14:57,070][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:14:57,071][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:14:57,073][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:14:57,073][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:14:57,074][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:14:57,166][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.3/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.64it/s v_num: 0.000      
                                                              val/auc: 0.722    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.444 val/mre:    
                                                              0.117 train/auc:  
                                                              0.758 train/f1:   
                                                              0.746             
                                                              train/precision:  
                                                              0.786             
                                                              train/recall:     
                                                              0.710 train/mre:  
                                                              0.119             
[2024-06-05 17:15:12,317][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.3/4>
[2024-06-05 17:15:12,318][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:15:12,323][HYDRA] 	#65 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 17:15:12,536][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:15:12,538][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:15:12,539][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:15:12,539][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:15:12,543][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:15:12,543][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:15:12,544][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:15:12,544][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:15:12,545][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:15:12,546][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:15:12,546][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:15:12,547][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:15:12,596][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.3/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.58it/s v_num: 0.000      
                                                              val/auc: 0.639    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.583 val/recall: 
                                                              0.778 val/mre:    
                                                              0.093 train/auc:  
                                                              0.952 train/f1:   
                                                              0.952             
                                                              train/precision:  
                                                              0.938             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.114             
[2024-06-05 17:15:27,725][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.3/5>
[2024-06-05 17:15:27,726][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:15:27,730][HYDRA] 	#66 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 17:15:27,958][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:15:27,959][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:15:27,961][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:15:27,961][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:15:27,964][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:15:27,965][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:15:27,965][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:15:27,965][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:15:27,967][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:15:27,967][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:15:27,967][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:15:27,969][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:15:28,008][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.3/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.09it/s v_num: 0.000      
                                                              val/auc: 0.783    
                                                              val/f1: 0.750     
                                                              val/precision:    
                                                              0.857 val/recall: 
                                                              0.667 val/mre:    
                                                              0.090 train/auc:  
                                                              0.774 train/f1:   
                                                              0.750             
                                                              train/precision:  
                                                              0.840             
                                                              train/recall:     
                                                              0.677 train/mre:  
                                                              0.108             
[2024-06-05 17:15:44,527][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.3/6>
[2024-06-05 17:15:44,530][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:15:44,535][HYDRA] 	#67 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 17:15:44,760][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:15:44,761][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:15:44,763][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:15:44,763][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:15:44,766][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:15:44,766][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:15:44,767][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:15:44,767][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:15:44,768][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:15:44,771][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:15:44,771][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:15:44,773][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:15:44,830][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.3/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 52.05it/s v_num: 0.000      
                                                              val/auc: 0.567    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.333 val/mre:    
                                                              0.090 train/auc:  
                                                              0.855 train/f1:   
                                                              0.862             
                                                              train/precision:  
                                                              0.824             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.107             
[2024-06-05 17:16:00,720][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.3/7>
[2024-06-05 17:16:00,721][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:16:00,723][HYDRA] 	#68 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 17:16:00,913][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:16:00,915][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:16:00,916][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:16:00,916][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:16:00,918][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:16:00,919][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:16:00,919][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:16:00,920][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:16:00,921][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:16:00,922][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:16:00,922][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:16:00,923][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:16:01,021][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.3/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 54.55it/s v_num: 0.000      
                                                              val/auc: 0.644    
                                                              val/f1: 0.696     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.889 val/mre:    
                                                              0.088 train/auc:  
                                                              0.726 train/f1:   
                                                              0.691             
                                                              train/precision:  
                                                              0.792             
                                                              train/recall:     
                                                              0.613 train/mre:  
                                                              0.107             
[2024-06-05 17:16:17,192][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.3/8>
[2024-06-05 17:16:17,193][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:16:17,197][HYDRA] 	#69 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=advice_yourself data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 17:16:17,412][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:16:17,413][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:16:17,414][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:16:17,415][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:16:17,417][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:16:17,417][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:16:17,418][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:16:17,418][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:16:17,419][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:16:17,420][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:16:17,420][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:16:17,421][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:16:17,460][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.3/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 56.40it/s v_num: 0.000      
                                                              val/auc: 0.694    
                                                              val/f1: 0.727     
                                                              val/precision:    
                                                              0.615 val/recall: 
                                                              0.889 val/mre:    
                                                              0.104 train/auc:  
                                                              0.944 train/f1:   
                                                              0.941             
                                                              train/precision:  
                                                              0.982             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.113             
[2024-06-05 17:16:32,850][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/advice_yourself/0.3/9>
[2024-06-05 17:16:32,851][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:16:32,855][HYDRA] 	#70 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 17:16:33,062][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:16:33,063][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:16:33,064][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:16:33,065][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:16:33,067][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:16:33,067][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:16:33,068][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:16:33,068][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:16:33,069][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:16:33,070][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:16:33,070][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:16:33,071][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:16:33,114][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.0/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.10it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.609     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.778 val/mre: nan
                                                              train/auc: 0.589  
                                                              train/f1: 0.671   
                                                              train/precision:  
                                                              0.560             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              nan               
[2024-06-05 17:16:48,231][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.0/0>
[2024-06-05 17:16:48,232][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:16:48,234][HYDRA] 	#71 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 17:16:48,430][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:16:48,431][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:16:48,433][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:16:48,433][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:16:48,435][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:16:48,436][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:16:48,436][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:16:48,437][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:16:48,438][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:16:48,440][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:16:48,440][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:16:48,442][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:16:48,483][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.0/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 67.37it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.609     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.778 val/mre: nan
                                                              train/auc: 0.643  
                                                              train/f1: 0.733   
                                                              train/precision:  
                                                              0.585             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              nan               
[2024-06-05 17:17:02,767][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.0/1>
[2024-06-05 17:17:02,767][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:17:02,771][HYDRA] 	#72 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 17:17:02,979][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:17:02,981][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:17:02,982][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:17:02,982][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:17:02,985][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:17:02,985][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:17:02,985][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:17:02,986][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:17:02,987][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:17:02,988][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:17:02,988][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:17:02,990][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:17:03,107][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.0/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 67.18it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.609     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.778 val/mre: nan
                                                              train/auc: 0.607  
                                                              train/f1: 0.699   
                                                              train/precision:  
                                                              0.567             
                                                              train/recall:     
                                                              0.911 train/mre:  
                                                              nan               
[2024-06-05 17:17:17,505][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.0/2>
[2024-06-05 17:17:17,506][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:17:17,511][HYDRA] 	#73 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 17:17:17,746][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:17:17,748][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:17:17,749][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:17:17,749][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:17:17,751][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:17:17,752][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:17:17,752][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:17:17,753][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:17:17,754][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:17:17,754][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:17:17,755][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:17:17,756][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:17:17,797][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.0/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 66.23it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.444 val/mre: nan
                                                              train/auc: 0.580  
                                                              train/f1: 0.652   
                                                              train/precision:  
                                                              0.557             
                                                              train/recall:     
                                                              0.786 train/mre:  
                                                              nan               
[2024-06-05 17:17:32,343][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.0/3>
[2024-06-05 17:17:32,344][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:17:32,347][HYDRA] 	#74 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 17:17:32,561][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:17:32,563][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:17:32,564][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:17:32,564][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:17:32,566][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:17:32,567][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:17:32,567][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:17:32,568][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:17:32,569][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:17:32,569][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:17:32,570][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:17:32,571][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:17:32,615][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.0/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 67.95it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.556 val/mre: nan
                                                              train/auc: 0.625  
                                                              train/f1: 0.708   
                                                              train/precision:  
                                                              0.580             
                                                              train/recall:     
                                                              0.911 train/mre:  
                                                              nan               
[2024-06-05 17:17:46,748][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.0/4>
[2024-06-05 17:17:46,751][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:17:46,754][HYDRA] 	#75 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 17:17:46,961][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:17:46,962][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:17:46,963][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:17:46,964][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:17:46,966][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:17:46,966][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:17:46,967][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:17:46,967][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:17:46,968][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:17:46,969][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:17:46,969][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:17:46,970][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:17:47,003][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.0/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 67.66it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.609     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.778 val/mre: nan
                                                              train/auc: 0.598  
                                                              train/f1: 0.694   
                                                              train/precision:  
                                                              0.560             
                                                              train/recall:     
                                                              0.911 train/mre:  
                                                              nan               
[2024-06-05 17:18:01,166][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.0/5>
[2024-06-05 17:18:01,167][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:18:01,171][HYDRA] 	#76 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 17:18:01,383][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:18:01,384][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:18:01,386][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:18:01,386][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:18:01,389][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:18:01,390][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:18:01,391][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:18:01,391][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:18:01,392][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:18:01,394][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:18:01,395][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:18:01,396][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:18:01,440][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.0/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 71.27it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.333 val/mre: nan
                                                              train/auc: 0.536  
                                                              train/f1: 0.552   
                                                              train/precision:  
                                                              0.533             
                                                              train/recall:     
                                                              0.571 train/mre:  
                                                              nan               
[2024-06-05 17:18:15,849][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.0/6>
[2024-06-05 17:18:15,850][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:18:15,854][HYDRA] 	#77 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 17:18:16,051][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:18:16,053][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:18:16,054][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:18:16,054][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:18:16,057][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:18:16,058][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:18:16,058][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:18:16,059][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:18:16,060][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:18:16,061][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:18:16,061][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:18:16,062][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:18:16,217][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.0/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 69.75it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre: nan
                                                              train/auc: 0.607  
                                                              train/f1: 0.672   
                                                              train/precision:  
                                                              0.577             
                                                              train/recall:     
                                                              0.804 train/mre:  
                                                              nan               
[2024-06-05 17:18:30,429][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.0/7>
[2024-06-05 17:18:30,429][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:18:30,435][HYDRA] 	#78 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 17:18:30,667][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:18:30,669][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:18:30,670][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:18:30,670][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:18:30,673][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:18:30,673][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:18:30,674][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:18:30,674][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:18:30,675][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:18:30,676][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:18:30,676][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:18:30,677][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:18:30,723][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.0/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 67.57it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.667 val/mre: nan
                                                              train/auc: 0.625  
                                                              train/f1: 0.708   
                                                              train/precision:  
                                                              0.580             
                                                              train/recall:     
                                                              0.911 train/mre:  
                                                              nan               
[2024-06-05 17:18:45,251][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.0/8>
[2024-06-05 17:18:45,252][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:18:45,255][HYDRA] 	#79 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 17:18:45,759][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:18:45,761][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:18:45,762][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:18:45,762][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:18:45,765][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:18:45,765][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:18:45,766][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:18:45,766][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:18:45,767][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:18:45,768][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:18:45,768][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:18:45,769][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:18:45,809][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.0/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 67.85it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre: nan
                                                              train/auc: 0.607  
                                                              train/f1: 0.694   
                                                              train/precision:  
                                                              0.568             
                                                              train/recall:     
                                                              0.893 train/mre:  
                                                              nan               
[2024-06-05 17:19:00,110][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.0/9>
[2024-06-05 17:19:00,111][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:19:00,115][HYDRA] 	#80 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 17:19:00,324][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:19:00,326][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:19:00,327][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:19:00,327][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:19:00,329][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:19:00,330][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:19:00,330][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:19:00,331][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:19:00,332][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:19:00,333][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:19:00,333][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:19:00,334][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:19:00,377][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.05/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 68.06it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.067 train/auc:  
                                                              0.607 train/f1:   
                                                              0.681             
                                                              train/precision:  
                                                              0.573             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.073             
[2024-06-05 17:19:15,523][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.05/0>
[2024-06-05 17:19:15,525][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:19:15,528][HYDRA] 	#81 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 17:19:15,742][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:19:15,744][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:19:15,745][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:19:15,745][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:19:15,748][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:19:15,748][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:19:15,748][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:19:15,749][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:19:15,750][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:19:15,752][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:19:15,752][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:19:15,753][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:19:15,798][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.05/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 67.10it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.066 train/auc:  
                                                              0.571 train/f1:   
                                                              0.636             
                                                              train/precision:  
                                                              0.553             
                                                              train/recall:     
                                                              0.750 train/mre:  
                                                              0.073             
[2024-06-05 17:19:30,481][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.05/1>
[2024-06-05 17:19:30,483][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:19:30,486][HYDRA] 	#82 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 17:19:30,695][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:19:30,696][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:19:30,698][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:19:30,698][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:19:30,700][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:19:30,701][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:19:30,701][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:19:30,701][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:19:30,702][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:19:30,704][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:19:30,704][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:19:30,705][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:19:30,800][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.05/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 67.69it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.067 train/auc:  
                                                              0.571 train/f1:   
                                                              0.647             
                                                              train/precision:  
                                                              0.550             
                                                              train/recall:     
                                                              0.786 train/mre:  
                                                              0.073             
[2024-06-05 17:19:45,237][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.05/2>
[2024-06-05 17:19:45,238][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:19:45,242][HYDRA] 	#83 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 17:19:45,474][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:19:45,475][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:19:45,477][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:19:45,477][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:19:45,479][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:19:45,480][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:19:45,480][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:19:45,481][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:19:45,482][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:19:45,482][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:19:45,483][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:19:45,484][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:19:45,537][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.05/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 73.58it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.067 train/auc:  
                                                              0.562 train/f1:   
                                                              0.642             
                                                              train/precision:  
                                                              0.543             
                                                              train/recall:     
                                                              0.786 train/mre:  
                                                              0.078             
[2024-06-05 17:20:00,001][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.05/3>
[2024-06-05 17:20:00,001][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:20:00,005][HYDRA] 	#84 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 17:20:00,200][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:20:00,202][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:20:00,203][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:20:00,203][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:20:00,206][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:20:00,206][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:20:00,207][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:20:00,207][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:20:00,208][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:20:00,209][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:20:00,209][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:20:00,210][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:20:00,256][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.05/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 67.72it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.583     
                                                              val/precision:    
                                                              0.467 val/recall: 
                                                              0.778 val/mre:    
                                                              0.074 train/auc:  
                                                              0.571 train/f1:   
                                                              0.662             
                                                              train/precision:  
                                                              0.547             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.078             
[2024-06-05 17:20:14,424][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.05/4>
[2024-06-05 17:20:14,424][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:20:14,427][HYDRA] 	#85 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 17:20:14,632][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:20:14,633][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:20:14,635][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:20:14,635][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:20:14,637][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:20:14,638][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:20:14,638][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:20:14,639][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:20:14,640][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:20:14,640][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:20:14,640][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:20:14,642][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:20:14,681][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.05/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 68.37it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.066 train/auc:  
                                                              0.571 train/f1:   
                                                              0.652             
                                                              train/precision:  
                                                              0.549             
                                                              train/recall:     
                                                              0.804 train/mre:  
                                                              0.077             
[2024-06-05 17:20:28,940][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.05/5>
[2024-06-05 17:20:28,944][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:20:28,947][HYDRA] 	#86 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 17:20:29,157][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:20:29,159][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:20:29,160][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:20:29,160][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:20:29,163][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:20:29,163][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:20:29,164][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:20:29,164][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:20:29,165][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:20:29,168][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:20:29,168][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:20:29,169][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:20:29,221][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.05/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 71.90it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.667 val/mre:    
                                                              0.070 train/auc:  
                                                              0.545 train/f1:   
                                                              0.611             
                                                              train/precision:  
                                                              0.533             
                                                              train/recall:     
                                                              0.714 train/mre:  
                                                              0.082             
[2024-06-05 17:20:43,719][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.05/6>
[2024-06-05 17:20:43,720][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:20:43,726][HYDRA] 	#87 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 17:20:43,967][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:20:43,968][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:20:43,970][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:20:43,970][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:20:43,973][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:20:43,973][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:20:43,974][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:20:43,974][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:20:43,975][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:20:43,977][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:20:43,977][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:20:43,978][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:20:44,166][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.05/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 70.87it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.068 train/auc:  
                                                              0.580 train/f1:   
                                                              0.636             
                                                              train/precision:  
                                                              0.562             
                                                              train/recall:     
                                                              0.732 train/mre:  
                                                              0.073             
[2024-06-05 17:20:59,878][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.05/7>
[2024-06-05 17:20:59,879][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:20:59,884][HYDRA] 	#88 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 17:21:00,105][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:21:00,107][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:21:00,108][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:21:00,108][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:21:00,111][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:21:00,111][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:21:00,111][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:21:00,112][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:21:00,113][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:21:00,113][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:21:00,114][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:21:00,115][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:21:00,170][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.05/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 69.67it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.064 train/auc:  
                                                              0.562 train/f1:   
                                                              0.626             
                                                              train/precision:  
                                                              0.547             
                                                              train/recall:     
                                                              0.732 train/mre:  
                                                              0.075             
[2024-06-05 17:21:14,695][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.05/8>
[2024-06-05 17:21:14,696][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:21:14,699][HYDRA] 	#89 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 17:21:14,914][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:21:14,916][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:21:14,917][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:21:14,917][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:21:14,921][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:21:14,922][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:21:14,922][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:21:14,923][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:21:14,924][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:21:14,925][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:21:14,925][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:21:14,926][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:21:14,966][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.05/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.53it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.068 train/auc:  
                                                              0.643 train/f1:   
                                                              0.714             
                                                              train/precision:  
                                                              0.595             
                                                              train/recall:     
                                                              0.893 train/mre:  
                                                              0.082             
[2024-06-05 17:21:29,301][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.05/9>
[2024-06-05 17:21:29,301][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:21:29,303][HYDRA] 	#90 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 17:21:29,487][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:21:29,488][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:21:29,490][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:21:29,490][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:21:29,492][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:21:29,492][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:21:29,493][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:21:29,493][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:21:29,494][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:21:29,497][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:21:29,497][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:21:29,498][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:21:29,534][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.1/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.38it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.064 train/auc:  
                                                              0.607 train/f1:   
                                                              0.681             
                                                              train/precision:  
                                                              0.573             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.072             
[2024-06-05 17:21:43,483][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.1/0>
[2024-06-05 17:21:43,484][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:21:43,488][HYDRA] 	#91 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 17:21:43,682][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:21:43,684][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:21:43,685][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:21:43,685][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:21:43,688][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:21:43,688][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:21:43,689][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:21:43,689][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:21:43,690][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:21:43,691][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:21:43,691][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:21:43,692][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:21:43,733][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.1/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 69.24it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.063 train/auc:  
                                                              0.607 train/f1:   
                                                              0.672             
                                                              train/precision:  
                                                              0.577             
                                                              train/recall:     
                                                              0.804 train/mre:  
                                                              0.073             
[2024-06-05 17:21:57,243][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.1/1>
[2024-06-05 17:21:57,243][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:21:57,245][HYDRA] 	#92 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 17:21:57,428][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:21:57,430][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:21:57,431][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:21:57,431][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:21:57,433][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:21:57,434][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:21:57,434][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:21:57,435][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:21:57,436][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:21:57,437][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:21:57,437][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:21:57,439][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:21:57,510][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.1/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 69.44it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.069 train/auc:  
                                                              0.554 train/f1:   
                                                              0.632             
                                                              train/precision:  
                                                              0.538             
                                                              train/recall:     
                                                              0.768 train/mre:  
                                                              0.072             
[2024-06-05 17:22:11,176][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.1/2>
[2024-06-05 17:22:11,177][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:22:11,181][HYDRA] 	#93 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 17:22:11,377][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:22:11,378][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:22:11,380][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:22:11,380][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:22:11,382][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:22:11,383][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:22:11,383][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:22:11,384][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:22:11,385][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:22:11,385][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:22:11,385][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:22:11,387][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:22:11,430][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.1/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.02it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.066 train/auc:  
                                                              0.580 train/f1:   
                                                              0.657             
                                                              train/precision:  
                                                              0.556             
                                                              train/recall:     
                                                              0.804 train/mre:  
                                                              0.079             
[2024-06-05 17:22:25,572][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.1/3>
[2024-06-05 17:22:25,574][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:22:25,577][HYDRA] 	#94 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 17:22:25,789][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:22:25,790][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:22:25,792][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:22:25,792][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:22:25,794][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:22:25,795][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:22:25,795][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:22:25,796][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:22:25,797][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:22:25,797][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:22:25,797][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:22:25,799][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:22:25,844][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.1/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.18it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.583     
                                                              val/precision:    
                                                              0.467 val/recall: 
                                                              0.778 val/mre:    
                                                              0.069 train/auc:  
                                                              0.571 train/f1:   
                                                              0.667             
                                                              train/precision:  
                                                              0.545             
                                                              train/recall:     
                                                              0.857 train/mre:  
                                                              0.073             
[2024-06-05 17:22:40,219][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.1/4>
[2024-06-05 17:22:40,220][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:22:40,222][HYDRA] 	#95 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 17:22:40,410][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:22:40,411][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:22:40,413][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:22:40,413][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:22:40,416][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:22:40,416][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:22:40,417][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:22:40,417][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:22:40,418][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:22:40,420][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:22:40,421][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:22:40,422][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:22:40,458][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.1/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.17it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.067 train/auc:  
                                                              0.580 train/f1:   
                                                              0.662             
                                                              train/precision:  
                                                              0.554             
                                                              train/recall:     
                                                              0.821 train/mre:  
                                                              0.076             
[2024-06-05 17:22:54,370][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.1/5>
[2024-06-05 17:22:54,371][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:22:54,373][HYDRA] 	#96 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 17:22:54,558][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:22:54,560][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:22:54,561][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:22:54,561][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:22:54,563][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:22:54,564][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:22:54,564][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:22:54,565][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:22:54,566][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:22:54,566][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:22:54,566][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:22:54,568][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:22:54,600][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.1/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 71.19it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.667 val/mre:    
                                                              0.068 train/auc:  
                                                              0.545 train/f1:   
                                                              0.622             
                                                              train/precision:  
                                                              0.532             
                                                              train/recall:     
                                                              0.750 train/mre:  
                                                              0.082             
[2024-06-05 17:23:08,575][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.1/6>
[2024-06-05 17:23:08,575][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:23:08,577][HYDRA] 	#97 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 17:23:08,771][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:23:08,773][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:23:08,774][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:23:08,774][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:23:08,777][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:23:08,777][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:23:08,778][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:23:08,778][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:23:08,779][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:23:08,780][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:23:08,781][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:23:08,782][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:23:08,885][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.1/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 66.43it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.067 train/auc:  
                                                              0.607 train/f1:   
                                                              0.651             
                                                              train/precision:  
                                                              0.586             
                                                              train/recall:     
                                                              0.732 train/mre:  
                                                              0.073             
[2024-06-05 17:23:24,051][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.1/7>
[2024-06-05 17:23:24,052][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:23:24,063][HYDRA] 	#98 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 17:23:24,275][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:23:24,277][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:23:24,278][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:23:24,278][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:23:24,282][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:23:24,282][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:23:24,283][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:23:24,283][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:23:24,284][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:23:24,285][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:23:24,285][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:23:24,287][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:23:24,325][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.1/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.14it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.066 train/auc:  
                                                              0.589 train/f1:   
                                                              0.646             
                                                              train/precision:  
                                                              0.568             
                                                              train/recall:     
                                                              0.750 train/mre:  
                                                              0.070             
[2024-06-05 17:23:39,596][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.1/8>
[2024-06-05 17:23:39,597][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:23:39,601][HYDRA] 	#99 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 17:23:39,835][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:23:39,837][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:23:39,838][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:23:39,838][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:23:39,840][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:23:39,841][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:23:39,841][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:23:39,842][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:23:39,843][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:23:39,843][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:23:39,844][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:23:39,845][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:23:39,906][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.1/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 67.62it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.065 train/auc:  
                                                              0.652 train/f1:   
                                                              0.723             
                                                              train/precision:  
                                                              0.600             
                                                              train/recall:     
                                                              0.911 train/mre:  
                                                              0.078             
[2024-06-05 17:23:54,206][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.1/9>
[2024-06-05 17:23:54,210][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:23:54,213][HYDRA] 	#100 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 17:23:54,424][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:23:54,426][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:23:54,428][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:23:54,428][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:23:54,430][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:23:54,430][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:23:54,431][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:23:54,431][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:23:54,432][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:23:54,435][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:23:54,435][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:23:54,437][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:23:54,485][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.15/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 68.20it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.064 train/auc:  
                                                              0.634 train/f1:   
                                                              0.696             
                                                              train/precision:  
                                                              0.595             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.070             
[2024-06-05 17:24:09,618][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.15/0>
[2024-06-05 17:24:09,619][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:24:09,624][HYDRA] 	#101 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 17:24:09,861][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:24:09,863][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:24:09,864][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:24:09,864][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:24:09,867][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:24:09,868][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:24:09,868][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:24:09,868][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:24:09,869][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:24:09,871][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:24:09,871][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:24:09,872][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:24:10,039][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.15/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 72.09it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.063 train/auc:  
                                                              0.589 train/f1:   
                                                              0.652             
                                                              train/precision:  
                                                              0.566             
                                                              train/recall:     
                                                              0.768 train/mre:  
                                                              0.072             
[2024-06-05 17:24:25,370][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.15/1>
[2024-06-05 17:24:25,371][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:24:25,373][HYDRA] 	#102 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 17:24:25,896][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:24:25,897][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:24:25,899][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:24:25,899][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:24:25,901][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:24:25,902][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:24:25,902][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:24:25,902][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:24:25,903][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:24:25,904][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:24:25,904][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:24:25,905][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:24:25,949][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.15/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.26it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.066 train/auc:  
                                                              0.598 train/f1:   
                                                              0.681             
                                                              train/precision:  
                                                              0.565             
                                                              train/recall:     
                                                              0.857 train/mre:  
                                                              0.072             
[2024-06-05 17:24:40,929][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.15/2>
[2024-06-05 17:24:40,930][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:24:40,934][HYDRA] 	#103 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 17:24:41,154][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:24:41,156][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:24:41,157][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:24:41,157][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:24:41,160][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:24:41,160][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:24:41,162][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:24:41,162][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:24:41,163][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:24:41,164][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:24:41,164][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:24:41,165][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:24:41,206][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.15/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 67.03it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.066 train/auc:  
                                                              0.580 train/f1:   
                                                              0.671             
                                                              train/precision:  
                                                              0.552             
                                                              train/recall:     
                                                              0.857 train/mre:  
                                                              0.080             
[2024-06-05 17:24:55,675][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.15/3>
[2024-06-05 17:24:55,676][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:24:55,680][HYDRA] 	#104 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 17:24:55,897][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:24:55,899][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:24:55,900][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:24:55,901][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:24:55,903][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:24:55,903][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:24:55,904][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:24:55,904][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:24:55,905][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:24:55,908][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:24:55,908][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:24:55,909][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:24:55,962][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.15/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 67.23it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.583     
                                                              val/precision:    
                                                              0.467 val/recall: 
                                                              0.778 val/mre:    
                                                              0.070 train/auc:  
                                                              0.554 train/f1:   
                                                              0.662             
                                                              train/precision:  
                                                              0.533             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              0.074             
[2024-06-05 17:25:11,067][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.15/4>
[2024-06-05 17:25:11,070][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:25:11,083][HYDRA] 	#105 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 17:25:11,382][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:25:11,384][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:25:11,385][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:25:11,385][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:25:11,387][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:25:11,388][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:25:11,388][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:25:11,389][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:25:11,390][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:25:11,391][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:25:11,391][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:25:11,393][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:25:11,518][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.15/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 57.97it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.065 train/auc:  
                                                              0.589 train/f1:   
                                                              0.676             
                                                              train/precision:  
                                                              0.558             
                                                              train/recall:     
                                                              0.857 train/mre:  
                                                              0.071             
[2024-06-05 17:25:26,826][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.15/5>
[2024-06-05 17:25:26,827][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:25:26,831][HYDRA] 	#106 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 17:25:27,042][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:25:27,044][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:25:27,045][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:25:27,045][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:25:27,047][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:25:27,048][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:25:27,048][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:25:27,049][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:25:27,050][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:25:27,050][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:25:27,051][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:25:27,052][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:25:27,103][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.15/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 66.99it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.068 train/auc:  
                                                              0.545 train/f1:   
                                                              0.611             
                                                              train/precision:  
                                                              0.533             
                                                              train/recall:     
                                                              0.714 train/mre:  
                                                              0.082             
[2024-06-05 17:25:42,026][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.15/6>
[2024-06-05 17:25:42,027][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:25:42,030][HYDRA] 	#107 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 17:25:42,292][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:25:42,294][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:25:42,295][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:25:42,296][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:25:42,298][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:25:42,299][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:25:42,299][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:25:42,299][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:25:42,300][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:25:42,301][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:25:42,302][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:25:42,304][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:25:42,359][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.15/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 68.40it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.667 val/mre:    
                                                              0.069 train/auc:  
                                                              0.661 train/f1:   
                                                              0.672             
                                                              train/precision:  
                                                              0.650             
                                                              train/recall:     
                                                              0.696 train/mre:  
                                                              0.072             
[2024-06-05 17:25:56,483][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.15/7>
[2024-06-05 17:25:56,484][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:25:56,488][HYDRA] 	#108 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 17:25:56,696][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:25:56,697][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:25:56,698][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:25:56,699][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:25:56,702][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:25:56,702][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:25:56,703][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:25:56,703][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:25:56,704][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:25:56,706][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:25:56,706][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:25:56,708][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:25:56,750][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.15/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 69.31it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.065 train/auc:  
                                                              0.598 train/f1:   
                                                              0.672             
                                                              train/precision:  
                                                              0.568             
                                                              train/recall:     
                                                              0.821 train/mre:  
                                                              0.070             
[2024-06-05 17:26:10,606][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.15/8>
[2024-06-05 17:26:10,607][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:26:10,609][HYDRA] 	#109 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 17:26:10,811][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:26:10,813][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:26:10,814][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:26:10,814][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:26:10,817][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:26:10,818][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:26:10,819][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:26:10,819][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:26:10,820][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:26:10,821][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:26:10,821][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:26:10,823][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:26:10,865][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.15/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 69.35it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.066 train/auc:  
                                                              0.643 train/f1:   
                                                              0.710             
                                                              train/precision:  
                                                              0.598             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              0.078             
[2024-06-05 17:26:26,514][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.15/9>
[2024-06-05 17:26:26,514][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:26:26,519][HYDRA] 	#110 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 17:26:26,771][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:26:26,773][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:26:26,775][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:26:26,775][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:26:26,780][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:26:26,780][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:26:26,781][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:26:26,781][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:26:26,782][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:26:26,783][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:26:26,784][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:26:26,785][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:26:27,062][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.2/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 68.06it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.062 train/auc:  
                                                              0.616 train/f1:   
                                                              0.672             
                                                              train/precision:  
                                                              0.587             
                                                              train/recall:     
                                                              0.786 train/mre:  
                                                              0.069             
[2024-06-05 17:26:42,462][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.2/0>
[2024-06-05 17:26:42,464][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:26:42,468][HYDRA] 	#111 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 17:26:42,733][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:26:42,734][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:26:42,735][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:26:42,736][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:26:42,738][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:26:42,738][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:26:42,739][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:26:42,739][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:26:42,740][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:26:42,741][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:26:42,741][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:26:42,743][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:26:42,791][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.2/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 71.08it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.063 train/auc:  
                                                              0.580 train/f1:   
                                                              0.647             
                                                              train/precision:  
                                                              0.558             
                                                              train/recall:     
                                                              0.768 train/mre:  
                                                              0.072             
[2024-06-05 17:26:56,923][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.2/1>
[2024-06-05 17:26:56,926][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:26:56,930][HYDRA] 	#112 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 17:26:57,137][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:26:57,139][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:26:57,140][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:26:57,140][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:26:57,143][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:26:57,143][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:26:57,144][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:26:57,144][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:26:57,145][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:26:57,147][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:26:57,147][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:26:57,149][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:26:57,205][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.2/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 70.98it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.072 train/auc:  
                                                              0.554 train/f1:   
                                                              0.643             
                                                              train/precision:  
                                                              0.536             
                                                              train/recall:     
                                                              0.804 train/mre:  
                                                              0.075             
[2024-06-05 17:27:11,858][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.2/2>
[2024-06-05 17:27:11,859][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:27:11,861][HYDRA] 	#113 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 17:27:12,061][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:27:12,062][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:27:12,064][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:27:12,064][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:27:12,067][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:27:12,067][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:27:12,068][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:27:12,068][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:27:12,069][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:27:12,070][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:27:12,070][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:27:12,072][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:27:12,108][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.2/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.23it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.065 train/auc:  
                                                              0.545 train/f1:   
                                                              0.638             
                                                              train/precision:  
                                                              0.529             
                                                              train/recall:     
                                                              0.804 train/mre:  
                                                              0.075             
[2024-06-05 17:27:27,076][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.2/3>
[2024-06-05 17:27:27,077][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:27:27,080][HYDRA] 	#114 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 17:27:27,289][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:27:27,290][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:27:27,292][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:27:27,292][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:27:27,294][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:27:27,294][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:27:27,295][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:27:27,295][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:27:27,296][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:27:27,297][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:27:27,298][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:27:27,299][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:27:27,448][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.2/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.01it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.583     
                                                              val/precision:    
                                                              0.467 val/recall: 
                                                              0.778 val/mre:    
                                                              0.072 train/auc:  
                                                              0.589 train/f1:   
                                                              0.685             
                                                              train/precision:  
                                                              0.556             
                                                              train/recall:     
                                                              0.893 train/mre:  
                                                              0.073             
[2024-06-05 17:27:42,359][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.2/4>
[2024-06-05 17:27:42,360][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:27:42,366][HYDRA] 	#115 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 17:27:42,949][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:27:42,950][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:27:42,952][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:27:42,952][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:27:42,954][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:27:42,955][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:27:42,955][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:27:42,956][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:27:42,957][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:27:42,958][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:27:42,958][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:27:42,959][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:27:43,009][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.2/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.92it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.063 train/auc:  
                                                              0.598 train/f1:   
                                                              0.681             
                                                              train/precision:  
                                                              0.565             
                                                              train/recall:     
                                                              0.857 train/mre:  
                                                              0.074             
[2024-06-05 17:27:58,107][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.2/5>
[2024-06-05 17:27:58,107][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:27:58,110][HYDRA] 	#116 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 17:27:58,345][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:27:58,347][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:27:58,348][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:27:58,348][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:27:58,350][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:27:58,351][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:27:58,351][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:27:58,352][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:27:58,353][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:27:58,355][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:27:58,355][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:27:58,356][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:27:58,414][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.2/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.64it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.070 train/auc:  
                                                              0.527 train/f1:   
                                                              0.589             
                                                              train/precision:  
                                                              0.521             
                                                              train/recall:     
                                                              0.679 train/mre:  
                                                              0.082             
[2024-06-05 17:28:13,579][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.2/6>
[2024-06-05 17:28:13,580][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:28:13,582][HYDRA] 	#117 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 17:28:13,790][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:28:13,791][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:28:13,793][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:28:13,793][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:28:13,795][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:28:13,795][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:28:13,796][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:28:13,796][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:28:13,797][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:28:13,798][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:28:13,798][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:28:13,799][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:28:13,845][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.2/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 68.42it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.068 train/auc:  
                                                              0.607 train/f1:   
                                                              0.639             
                                                              train/precision:  
                                                              0.591             
                                                              train/recall:     
                                                              0.696 train/mre:  
                                                              0.071             
[2024-06-05 17:28:28,480][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.2/7>
[2024-06-05 17:28:28,481][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:28:28,487][HYDRA] 	#118 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 17:28:28,714][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:28:28,716][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:28:28,717][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:28:28,717][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:28:28,721][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:28:28,721][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:28:28,722][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:28:28,722][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:28:28,723][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:28:28,724][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:28:28,724][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:28:28,726][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:28:28,920][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.2/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 70.05it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.064 train/auc:  
                                                              0.527 train/f1:   
                                                              0.569             
                                                              train/precision:  
                                                              0.522             
                                                              train/recall:     
                                                              0.625 train/mre:  
                                                              0.069             
[2024-06-05 17:28:43,206][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.2/8>
[2024-06-05 17:28:43,209][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:28:43,214][HYDRA] 	#119 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 17:28:43,433][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:28:43,435][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:28:43,436][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:28:43,437][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:28:43,439][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:28:43,439][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:28:43,440][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:28:43,440][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:28:43,441][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:28:43,442][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:28:43,442][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:28:43,444][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:28:43,511][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.2/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 71.21it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.065 train/auc:  
                                                              0.634 train/f1:   
                                                              0.696             
                                                              train/precision:  
                                                              0.595             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.077             
[2024-06-05 17:28:57,847][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.2/9>
[2024-06-05 17:28:57,848][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:28:57,852][HYDRA] 	#120 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 17:28:58,050][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:28:58,051][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:28:58,053][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:28:58,053][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:28:58,055][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:28:58,056][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:28:58,056][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:28:58,057][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:28:58,058][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:28:58,058][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:28:58,059][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:28:58,060][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:28:58,096][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.25/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 71.27it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.062 train/auc:  
                                                              0.607 train/f1:   
                                                              0.662             
                                                              train/precision:  
                                                              0.581             
                                                              train/recall:     
                                                              0.768 train/mre:  
                                                              0.069             
[2024-06-05 17:29:12,452][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.25/0>
[2024-06-05 17:29:12,453][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:29:12,457][HYDRA] 	#121 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 17:29:12,673][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:29:12,674][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:29:12,675][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:29:12,676][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:29:12,679][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:29:12,680][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:29:12,681][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:29:12,681][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:29:12,682][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:29:12,685][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:29:12,685][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:29:12,686][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:29:12,745][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.25/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 68.42it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.064 train/auc:  
                                                              0.580 train/f1:   
                                                              0.662             
                                                              train/precision:  
                                                              0.554             
                                                              train/recall:     
                                                              0.821 train/mre:  
                                                              0.074             
[2024-06-05 17:29:26,892][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.25/1>
[2024-06-05 17:29:26,893][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:29:26,896][HYDRA] 	#122 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 17:29:27,127][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:29:27,128][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:29:27,130][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:29:27,130][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:29:27,132][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:29:27,133][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:29:27,133][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:29:27,133][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:29:27,135][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:29:27,135][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:29:27,135][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:29:27,137][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:29:27,194][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.25/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 72.34it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.583     
                                                              val/precision:    
                                                              0.467 val/recall: 
                                                              0.778 val/mre:    
                                                              0.067 train/auc:  
                                                              0.625 train/f1:   
                                                              0.700             
                                                              train/precision:  
                                                              0.583             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              0.074             
[2024-06-05 17:29:42,310][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.25/2>
[2024-06-05 17:29:42,311][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:29:42,315][HYDRA] 	#123 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 17:29:42,546][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:29:42,548][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:29:42,549][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:29:42,550][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:29:42,552][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:29:42,552][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:29:42,553][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:29:42,553][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:29:42,554][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:29:42,555][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:29:42,556][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:29:42,557][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:29:42,723][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.25/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 69.35it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.667 val/mre:    
                                                              0.064 train/auc:  
                                                              0.580 train/f1:   
                                                              0.657             
                                                              train/precision:  
                                                              0.556             
                                                              train/recall:     
                                                              0.804 train/mre:  
                                                              0.073             
[2024-06-05 17:29:56,749][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.25/3>
[2024-06-05 17:29:56,749][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:29:56,753][HYDRA] 	#124 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 17:29:56,971][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:29:56,972][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:29:56,973][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:29:56,974][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:29:56,976][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:29:56,976][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:29:56,977][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:29:56,977][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:29:56,978][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:29:56,979][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:29:56,979][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:29:56,981][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:29:57,021][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.25/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 71.56it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.583     
                                                              val/precision:    
                                                              0.467 val/recall: 
                                                              0.778 val/mre:    
                                                              0.069 train/auc:  
                                                              0.571 train/f1:   
                                                              0.676             
                                                              train/precision:  
                                                              0.543             
                                                              train/recall:     
                                                              0.893 train/mre:  
                                                              0.073             
[2024-06-05 17:30:11,173][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.25/4>
[2024-06-05 17:30:11,174][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:30:11,179][HYDRA] 	#125 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 17:30:11,409][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:30:11,410][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:30:11,412][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:30:11,412][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:30:11,414][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:30:11,414][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:30:11,415][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:30:11,415][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:30:11,417][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:30:11,417][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:30:11,417][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:30:11,418][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:30:11,458][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.25/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 71.93it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.069 train/auc:  
                                                              0.580 train/f1:   
                                                              0.667             
                                                              train/precision:  
                                                              0.553             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.079             
[2024-06-05 17:30:25,619][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.25/5>
[2024-06-05 17:30:25,619][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:30:25,622][HYDRA] 	#126 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 17:30:25,830][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:30:25,831][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:30:25,833][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:30:25,833][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:30:25,835][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:30:25,836][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:30:25,836][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:30:25,837][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:30:25,838][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:30:25,840][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:30:25,840][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:30:25,841][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:30:25,885][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.25/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.82it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.069 train/auc:  
                                                              0.545 train/f1:   
                                                              0.622             
                                                              train/precision:  
                                                              0.532             
                                                              train/recall:     
                                                              0.750 train/mre:  
                                                              0.079             
[2024-06-05 17:30:40,648][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.25/6>
[2024-06-05 17:30:40,649][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:30:40,653][HYDRA] 	#127 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 17:30:40,861][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:30:40,863][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:30:40,864][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:30:40,864][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:30:40,866][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:30:40,867][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:30:40,867][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:30:40,868][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:30:40,869][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:30:40,869][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:30:40,869][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:30:40,871][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:30:40,913][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.25/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 70.08it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.068 train/auc:  
                                                              0.598 train/f1:   
                                                              0.640             
                                                              train/precision:  
                                                              0.580             
                                                              train/recall:     
                                                              0.714 train/mre:  
                                                              0.072             
[2024-06-05 17:30:55,066][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.25/7>
[2024-06-05 17:30:55,066][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:30:55,083][HYDRA] 	#128 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 17:30:55,285][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:30:55,287][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:30:55,288][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:30:55,289][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:30:55,292][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:30:55,292][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:30:55,293][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:30:55,293][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:30:55,294][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:30:55,296][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:30:55,296][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:30:55,298][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:30:55,407][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.25/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 69.23it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.667 val/mre:    
                                                              0.064 train/auc:  
                                                              0.545 train/f1:   
                                                              0.557             
                                                              train/precision:  
                                                              0.542             
                                                              train/recall:     
                                                              0.571 train/mre:  
                                                              0.071             
[2024-06-05 17:31:09,559][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.25/8>
[2024-06-05 17:31:09,560][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:31:09,563][HYDRA] 	#129 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 17:31:09,774][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:31:09,776][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:31:09,777][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:31:09,777][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:31:09,779][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:31:09,780][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:31:09,780][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:31:09,781][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:31:09,782][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:31:09,783][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:31:09,783][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:31:09,784][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:31:09,832][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.25/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 70.41it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.067 train/auc:  
                                                              0.643 train/f1:   
                                                              0.706             
                                                              train/precision:  
                                                              0.600             
                                                              train/recall:     
                                                              0.857 train/mre:  
                                                              0.077             
[2024-06-05 17:31:24,849][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.25/9>
[2024-06-05 17:31:24,850][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:31:24,853][HYDRA] 	#130 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 17:31:25,121][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:31:25,123][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:31:25,124][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:31:25,124][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:31:25,128][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:31:25,128][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:31:25,128][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:31:25,129][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:31:25,130][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:31:25,130][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:31:25,131][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:31:25,133][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:31:25,182][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.3/0/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 74.91it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.064 train/auc:  
                                                              0.598 train/f1:   
                                                              0.662             
                                                              train/precision:  
                                                              0.571             
                                                              train/recall:     
                                                              0.786 train/mre:  
                                                              0.069             
[2024-06-05 17:31:39,177][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.3/0>
[2024-06-05 17:31:39,178][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:31:39,181][HYDRA] 	#131 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 17:31:39,386][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:31:39,388][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:31:39,389][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:31:39,389][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:31:39,391][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:31:39,392][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:31:39,392][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:31:39,393][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:31:39,394][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:31:39,396][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:31:39,396][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:31:39,397][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:31:39,449][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.3/1/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 68.28it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.062 train/auc:  
                                                              0.598 train/f1:   
                                                              0.656             
                                                              train/precision:  
                                                              0.573             
                                                              train/recall:     
                                                              0.768 train/mre:  
                                                              0.072             
[2024-06-05 17:31:53,432][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.3/1>
[2024-06-05 17:31:53,432][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:31:53,434][HYDRA] 	#132 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 17:31:53,641][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:31:53,642][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:31:53,644][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:31:53,644][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:31:53,646][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:31:53,647][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:31:53,647][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:31:53,647][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:31:53,649][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:31:53,649][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:31:53,649][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:31:53,651][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:31:53,701][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.3/2/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 69.64it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.583     
                                                              val/precision:    
                                                              0.467 val/recall: 
                                                              0.778 val/mre:    
                                                              0.066 train/auc:  
                                                              0.580 train/f1:   
                                                              0.652             
                                                              train/precision:  
                                                              0.557             
                                                              train/recall:     
                                                              0.786 train/mre:  
                                                              0.075             
[2024-06-05 17:32:07,812][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.3/2>
[2024-06-05 17:32:07,812][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:32:07,815][HYDRA] 	#133 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 17:32:08,004][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:32:08,006][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:32:08,007][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:32:08,007][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:32:08,010][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:32:08,010][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:32:08,011][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:32:08,011][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:32:08,012][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:32:08,013][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:32:08,014][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:32:08,015][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:32:08,121][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.3/3/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.74it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.064 train/auc:  
                                                              0.598 train/f1:   
                                                              0.667             
                                                              train/precision:  
                                                              0.570             
                                                              train/recall:     
                                                              0.804 train/mre:  
                                                              0.072             
[2024-06-05 17:32:23,149][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.3/3>
[2024-06-05 17:32:23,150][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:32:23,153][HYDRA] 	#134 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 17:32:23,389][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:32:23,390][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:32:23,392][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:32:23,392][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:32:23,394][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:32:23,395][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:32:23,395][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:32:23,395][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:32:23,397][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:32:23,397][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:32:23,398][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:32:23,400][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:32:23,450][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.3/4/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 67.34it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.583     
                                                              val/precision:    
                                                              0.467 val/recall: 
                                                              0.778 val/mre:    
                                                              0.075 train/auc:  
                                                              0.571 train/f1:   
                                                              0.667             
                                                              train/precision:  
                                                              0.545             
                                                              train/recall:     
                                                              0.857 train/mre:  
                                                              0.074             
[2024-06-05 17:32:37,898][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.3/4>
[2024-06-05 17:32:37,899][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:32:37,902][HYDRA] 	#135 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 17:32:38,096][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:32:38,097][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:32:38,099][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:32:38,099][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:32:38,101][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:32:38,102][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:32:38,102][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:32:38,103][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:32:38,104][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:32:38,104][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:32:38,104][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:32:38,106][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:32:38,146][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.3/5/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 74.04it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.064 train/auc:  
                                                              0.616 train/f1:   
                                                              0.699             
                                                              train/precision:  
                                                              0.575             
                                                              train/recall:     
                                                              0.893 train/mre:  
                                                              0.075             
[2024-06-05 17:32:52,439][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.3/5>
[2024-06-05 17:32:52,440][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:32:52,443][HYDRA] 	#136 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 17:32:52,645][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:32:52,647][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:32:52,648][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:32:52,648][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:32:52,652][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:32:52,653][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:32:52,654][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:32:52,654][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:32:52,655][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:32:52,657][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:32:52,657][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:32:52,659][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:32:52,703][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.3/6/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 71.93it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.667 val/mre:    
                                                              0.070 train/auc:  
                                                              0.571 train/f1:   
                                                              0.647             
                                                              train/precision:  
                                                              0.550             
                                                              train/recall:     
                                                              0.786 train/mre:  
                                                              0.080             
[2024-06-05 17:33:06,950][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.3/6>
[2024-06-05 17:33:06,950][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:33:06,955][HYDRA] 	#137 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 17:33:07,198][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:33:07,200][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:33:07,201][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:33:07,201][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:33:07,205][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:33:07,206][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:33:07,206][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:33:07,207][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:33:07,208][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:33:07,208][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:33:07,208][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:33:07,210][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:33:07,253][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.3/7/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 71.17it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.068 train/auc:  
                                                              0.598 train/f1:   
                                                              0.634             
                                                              train/precision:  
                                                              0.582             
                                                              train/recall:     
                                                              0.696 train/mre:  
                                                              0.072             
[2024-06-05 17:33:22,089][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.3/7>
[2024-06-05 17:33:22,091][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:33:22,096][HYDRA] 	#138 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 17:33:22,312][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:33:22,314][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:33:22,315][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:33:22,315][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:33:22,318][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:33:22,319][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:33:22,319][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:33:22,320][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:33:22,321][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:33:22,322][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:33:22,322][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:33:22,324][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:33:22,500][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.3/8/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 68.64it/s v_num: 0.000      
                                                              val/auc: 0.444    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.667 val/mre:    
                                                              0.065 train/auc:  
                                                              0.598 train/f1:   
                                                              0.656             
                                                              train/precision:  
                                                              0.573             
                                                              train/recall:     
                                                              0.768 train/mre:  
                                                              0.071             
[2024-06-05 17:33:36,635][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.3/8>
[2024-06-05 17:33:36,636][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:33:36,640][HYDRA] 	#139 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=anything_regret data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 17:33:36,862][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:33:36,864][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:33:36,865][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:33:36,865][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:33:36,867][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:33:36,868][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:33:36,868][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:33:36,869][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:33:36,870][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:33:36,871][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:33:36,871][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:33:36,872][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:33:36,910][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.3/9/csv_artifacts/
/home/khickey/test_impute/src/datamodules/daicwoz.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.
  df1: pd.DataFrame = pd.concat([df1, df_zeros],ignore_index=True)
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.20it/s v_num: 0.000      
                                                              val/auc: 0.389    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.667 val/mre:    
                                                              0.066 train/auc:  
                                                              0.661 train/f1:   
                                                              0.725             
                                                              train/precision:  
                                                              0.610             
                                                              train/recall:     
                                                              0.893 train/mre:  
                                                              0.077             
[2024-06-05 17:33:51,713][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/anything_regret/0.3/9>
[2024-06-05 17:33:51,714][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:33:51,719][HYDRA] 	#140 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 17:33:51,947][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:33:51,949][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:33:51,950][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:33:51,950][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:33:51,952][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:33:51,953][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:33:51,953][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:33:51,954][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:33:51,955][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:33:51,955][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:33:51,955][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:33:51,957][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:33:52,393][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.0/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 56.75it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.600 val/mre: nan
                                                              train/auc: 0.831  
                                                              train/f1: 0.837   
                                                              train/precision:  
                                                              0.806             
                                                              train/recall:     
                                                              0.871 train/mre:  
                                                              nan               
[2024-06-05 17:34:10,050][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.0/0>
[2024-06-05 17:34:10,051][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:34:10,056][HYDRA] 	#141 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 17:34:10,318][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:34:10,319][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:34:10,320][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:34:10,321][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:34:10,324][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:34:10,325][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:34:10,326][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:34:10,326][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:34:10,327][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:34:10,330][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:34:10,330][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:34:10,331][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:34:10,391][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.0/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 56.14it/s v_num: 0.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.500 val/mre: nan
                                                              train/auc: 0.726  
                                                              train/f1: 0.707   
                                                              train/precision:  
                                                              0.759             
                                                              train/recall:     
                                                              0.661 train/mre:  
                                                              nan               
[2024-06-05 17:34:28,003][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.0/1>
[2024-06-05 17:34:28,004][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:34:28,015][HYDRA] 	#142 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 17:34:28,288][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:34:28,289][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:34:28,291][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:34:28,291][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:34:28,294][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:34:28,295][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:34:28,295][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:34:28,296][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:34:28,297][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:34:28,298][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:34:28,298][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:34:28,299][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:34:28,543][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.0/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.14it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.800 val/mre: nan
                                                              train/auc: 0.694  
                                                              train/f1: 0.725   
                                                              train/precision:  
                                                              0.658             
                                                              train/recall:     
                                                              0.806 train/mre:  
                                                              nan               
[2024-06-05 17:34:45,307][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.0/2>
[2024-06-05 17:34:45,308][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:34:45,312][HYDRA] 	#143 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 17:34:45,575][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:34:45,577][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:34:45,578][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:34:45,579][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:34:45,581][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:34:45,581][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:34:45,582][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:34:45,582][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:34:45,583][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:34:45,584][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:34:45,585][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:34:45,586][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:34:45,639][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.0/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.28it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre: nan
                                                              train/auc: 0.758  
                                                              train/f1: 0.797   
                                                              train/precision:  
                                                              0.686             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              nan               
[2024-06-05 17:35:01,807][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.0/3>
[2024-06-05 17:35:01,808][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:35:01,813][HYDRA] 	#144 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 17:35:02,054][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:35:02,056][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:35:02,057][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:35:02,058][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:35:02,060][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:35:02,060][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:35:02,061][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:35:02,061][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:35:02,062][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:35:02,063][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:35:02,063][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:35:02,064][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:35:02,106][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.0/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.61it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.727     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.800 val/mre: nan
                                                              train/auc: 0.661  
                                                              train/f1: 0.691   
                                                              train/precision:  
                                                              0.635             
                                                              train/recall:     
                                                              0.758 train/mre:  
                                                              nan               
[2024-06-05 17:35:17,779][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.0/4>
[2024-06-05 17:35:17,780][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:35:17,782][HYDRA] 	#145 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 17:35:18,025][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:35:18,026][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:35:18,028][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:35:18,028][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:35:18,032][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:35:18,033][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:35:18,033][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:35:18,034][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:35:18,035][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:35:18,037][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:35:18,037][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:35:18,038][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:35:18,091][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.0/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 66.48it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.636 val/recall: 
                                                              0.700 val/mre: nan
                                                              train/auc: 0.718  
                                                              train/f1: 0.755   
                                                              train/precision:  
                                                              0.667             
                                                              train/recall:     
                                                              0.871 train/mre:  
                                                              nan               
[2024-06-05 17:35:33,543][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.0/5>
[2024-06-05 17:35:33,543][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:35:33,546][HYDRA] 	#146 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 17:35:33,754][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:35:33,755][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:35:33,756][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:35:33,757][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:35:33,759][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:35:33,759][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:35:33,760][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:35:33,760][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:35:33,761][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:35:33,762][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:35:33,762][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:35:33,764][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:35:33,885][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.0/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.17it/s v_num: 0.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.706     
                                                              val/precision:    
                                                              0.857 val/recall: 
                                                              0.600 val/mre: nan
                                                              train/auc: 0.798  
                                                              train/f1: 0.793   
                                                              train/precision:  
                                                              0.814             
                                                              train/recall:     
                                                              0.774 train/mre:  
                                                              nan               
[2024-06-05 17:35:50,105][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.0/6>
[2024-06-05 17:35:50,106][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:35:50,111][HYDRA] 	#147 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 17:35:50,361][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:35:50,362][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:35:50,364][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:35:50,364][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:35:50,366][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:35:50,367][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:35:50,367][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:35:50,368][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:35:50,369][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:35:50,370][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:35:50,370][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:35:50,371][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:35:50,419][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.0/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.48it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre: nan
                                                              train/auc: 0.806  
                                                              train/f1: 0.826   
                                                              train/precision:  
                                                              0.750             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              nan               
[2024-06-05 17:36:07,084][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.0/7>
[2024-06-05 17:36:07,085][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:36:07,091][HYDRA] 	#148 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 17:36:07,334][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:36:07,336][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:36:07,337][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:36:07,337][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:36:07,342][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:36:07,343][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:36:07,343][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:36:07,344][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:36:07,345][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:36:07,345][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:36:07,345][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:36:07,347][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:36:07,411][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.0/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 66.69it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre: nan
                                                              train/auc: 0.726  
                                                              train/f1: 0.776   
                                                              train/precision:  
                                                              0.656             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              nan               
[2024-06-05 17:36:23,725][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.0/8>
[2024-06-05 17:36:23,725][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:36:23,731][HYDRA] 	#149 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 17:36:23,964][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:36:23,966][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:36:23,967][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:36:23,967][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:36:23,970][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:36:23,970][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:36:23,971][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:36:23,971][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:36:23,972][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:36:23,975][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:36:23,975][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:36:23,976][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:36:24,031][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.0/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 58.53it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.600 val/mre: nan
                                                              train/auc: 0.790  
                                                              train/f1: 0.812   
                                                              train/precision:  
                                                              0.737             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              nan               
[2024-06-05 17:36:40,811][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.0/9>
[2024-06-05 17:36:40,812][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:36:40,816][HYDRA] 	#150 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 17:36:41,017][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:36:41,018][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:36:41,019][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:36:41,020][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:36:41,022][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:36:41,022][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:36:41,023][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:36:41,023][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:36:41,024][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:36:41,025][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:36:41,026][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:36:41,027][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:36:41,121][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.05/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 55.80it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.696     
                                                              val/precision:    
                                                              0.615 val/recall: 
                                                              0.800 val/mre:    
                                                              0.112 train/auc:  
                                                              0.855 train/f1:   
                                                              0.859             
                                                              train/precision:  
                                                              0.833             
                                                              train/recall:     
                                                              0.887 train/mre:  
                                                              0.128             
[2024-06-05 17:36:57,500][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.05/0>
[2024-06-05 17:36:57,501][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:36:57,503][HYDRA] 	#151 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 17:36:57,723][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:36:57,724][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:36:57,726][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:36:57,726][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:36:57,728][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:36:57,728][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:36:57,729][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:36:57,729][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:36:57,730][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:36:57,731][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:36:57,731][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:36:57,733][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:36:57,786][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.05/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.28it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.600 val/mre:    
                                                              0.111 train/auc:  
                                                              0.823 train/f1:   
                                                              0.838             
                                                              train/precision:  
                                                              0.770             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.125             
[2024-06-05 17:37:13,704][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.05/1>
[2024-06-05 17:37:13,705][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:37:13,707][HYDRA] 	#152 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 17:37:13,910][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:37:13,911][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:37:13,913][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:37:13,913][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:37:13,915][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:37:13,916][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:37:13,916][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:37:13,916][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:37:13,917][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:37:13,918][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:37:13,918][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:37:13,919][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:37:13,959][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.05/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 55.66it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.104 train/auc:  
                                                              0.790 train/f1:   
                                                              0.809             
                                                              train/precision:  
                                                              0.743             
                                                              train/recall:     
                                                              0.887 train/mre:  
                                                              0.120             
[2024-06-05 17:37:30,319][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.05/2>
[2024-06-05 17:37:30,320][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:37:30,323][HYDRA] 	#153 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 17:37:30,550][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:37:30,552][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:37:30,553][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:37:30,553][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:37:30,556][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:37:30,556][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:37:30,557][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:37:30,557][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:37:30,558][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:37:30,560][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:37:30,560][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:37:30,561][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:37:30,621][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.05/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 54.90it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.696     
                                                              val/precision:    
                                                              0.615 val/recall: 
                                                              0.800 val/mre:    
                                                              0.107 train/auc:  
                                                              0.694 train/f1:   
                                                              0.743             
                                                              train/precision:  
                                                              0.640             
                                                              train/recall:     
                                                              0.887 train/mre:  
                                                              0.125             
[2024-06-05 17:37:47,142][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.05/3>
[2024-06-05 17:37:47,144][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:37:47,146][HYDRA] 	#154 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 17:37:47,345][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:37:47,346][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:37:47,347][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:37:47,348][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:37:47,352][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:37:47,353][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:37:47,353][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:37:47,354][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:37:47,355][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:37:47,356][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:37:47,356][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:37:47,357][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:37:47,465][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.05/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 57.95it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.102 train/auc:  
                                                              0.694 train/f1:   
                                                              0.712             
                                                              train/precision:  
                                                              0.671             
                                                              train/recall:     
                                                              0.758 train/mre:  
                                                              0.119             
[2024-06-05 17:38:03,860][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.05/4>
[2024-06-05 17:38:03,861][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:38:03,865][HYDRA] 	#155 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 17:38:04,117][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:38:04,119][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:38:04,120][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:38:04,121][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:38:04,123][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:38:04,123][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:38:04,124][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:38:04,124][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:38:04,125][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:38:04,126][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:38:04,126][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:38:04,127][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:38:04,196][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.05/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.46it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.636 val/recall: 
                                                              0.700 val/mre:    
                                                              0.112 train/auc:  
                                                              0.806 train/f1:   
                                                              0.810             
                                                              train/precision:  
                                                              0.797             
                                                              train/recall:     
                                                              0.823 train/mre:  
                                                              0.133             
[2024-06-05 17:38:21,046][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.05/5>
[2024-06-05 17:38:21,047][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:38:21,051][HYDRA] 	#156 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 17:38:21,274][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:38:21,275][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:38:21,277][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:38:21,277][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:38:21,279][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:38:21,279][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:38:21,280][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:38:21,280][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:38:21,281][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:38:21,282][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:38:21,282][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:38:21,283][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:38:21,320][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.05/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.48it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.104 train/auc:  
                                                              0.871 train/f1:   
                                                              0.875             
                                                              train/precision:  
                                                              0.848             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.125             
[2024-06-05 17:38:37,179][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.05/6>
[2024-06-05 17:38:37,179][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:38:37,183][HYDRA] 	#157 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 17:38:37,394][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:38:37,396][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:38:37,397][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:38:37,397][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:38:37,399][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:38:37,400][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:38:37,400][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:38:37,401][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:38:37,402][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:38:37,404][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:38:37,404][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:38:37,405][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:38:37,450][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.05/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.95it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.111 train/auc:  
                                                              0.782 train/f1:   
                                                              0.800             
                                                              train/precision:  
                                                              0.740             
                                                              train/recall:     
                                                              0.871 train/mre:  
                                                              0.121             
[2024-06-05 17:38:53,045][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.05/7>
[2024-06-05 17:38:53,046][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:38:53,050][HYDRA] 	#158 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 17:38:53,257][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:38:53,259][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:38:53,260][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:38:53,261][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:38:53,263][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:38:53,264][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:38:53,264][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:38:53,265][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:38:53,266][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:38:53,267][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:38:53,267][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:38:53,268][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:38:53,370][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.05/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.00it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.692     
                                                              val/precision:    
                                                              0.562 val/recall: 
                                                              0.900 val/mre:    
                                                              0.106 train/auc:  
                                                              0.621 train/f1:   
                                                              0.569             
                                                              train/precision:  
                                                              0.660             
                                                              train/recall:     
                                                              0.500 train/mre:  
                                                              0.115             
[2024-06-05 17:39:08,669][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.05/8>
[2024-06-05 17:39:08,671][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:39:08,673][HYDRA] 	#159 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 17:39:08,890][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:39:08,891][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:39:08,892][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:39:08,893][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:39:08,895][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:39:08,895][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:39:08,896][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:39:08,896][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:39:08,897][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:39:08,898][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:39:08,898][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:39:08,900][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:39:08,937][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.05/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.22it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.636     
                                                              val/precision:    
                                                              0.583 val/recall: 
                                                              0.700 val/mre:    
                                                              0.106 train/auc:  
                                                              0.734 train/f1:   
                                                              0.727             
                                                              train/precision:  
                                                              0.746             
                                                              train/recall:     
                                                              0.710 train/mre:  
                                                              0.128             
[2024-06-05 17:39:24,139][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.05/9>
[2024-06-05 17:39:24,140][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:39:24,146][HYDRA] 	#160 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 17:39:24,346][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:39:24,348][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:39:24,349][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:39:24,349][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:39:24,352][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:39:24,352][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:39:24,352][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:39:24,353][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:39:24,354][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:39:24,354][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:39:24,355][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:39:24,356][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:39:24,393][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.1/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.55it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.110 train/auc:  
                                                              0.863 train/f1:   
                                                              0.876             
                                                              train/precision:  
                                                              0.800             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.129             
[2024-06-05 17:39:39,823][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.1/0>
[2024-06-05 17:39:39,824][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:39:39,830][HYDRA] 	#161 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 17:39:40,047][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:39:40,049][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:39:40,050][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:39:40,050][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:39:40,053][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:39:40,054][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:39:40,054][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:39:40,054][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:39:40,055][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:39:40,058][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:39:40,058][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:39:40,059][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:39:40,100][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.1/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 66.05it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.107 train/auc:  
                                                              0.782 train/f1:   
                                                              0.784             
                                                              train/precision:  
                                                              0.778             
                                                              train/recall:     
                                                              0.790 train/mre:  
                                                              0.120             
[2024-06-05 17:39:55,351][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.1/1>
[2024-06-05 17:39:55,351][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:39:55,355][HYDRA] 	#162 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 17:39:55,559][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:39:55,561][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:39:55,562][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:39:55,562][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:39:55,564][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:39:55,565][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:39:55,565][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:39:55,566][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:39:55,567][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:39:55,568][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:39:55,568][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:39:55,570][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:39:55,680][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.1/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 53.33it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.700     
                                                              val/precision:    
                                                              0.700 val/recall: 
                                                              0.700 val/mre:    
                                                              0.101 train/auc:  
                                                              0.847 train/f1:   
                                                              0.838             
                                                              train/precision:  
                                                              0.891             
                                                              train/recall:     
                                                              0.790 train/mre:  
                                                              0.118             
[2024-06-05 17:40:11,160][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.1/2>
[2024-06-05 17:40:11,161][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:40:11,163][HYDRA] 	#163 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 17:40:11,374][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:40:11,376][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:40:11,377][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:40:11,377][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:40:11,380][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:40:11,380][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:40:11,381][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:40:11,381][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:40:11,382][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:40:11,383][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:40:11,383][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:40:11,384][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:40:11,426][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.1/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.66it/s v_num: 0.000      
                                                              val/auc: 0.800    
                                                              val/f1: 0.778     
                                                              val/precision:    
                                                              0.875 val/recall: 
                                                              0.700 val/mre:    
                                                              0.102 train/auc:  
                                                              0.798 train/f1:   
                                                              0.786             
                                                              train/precision:  
                                                              0.836             
                                                              train/recall:     
                                                              0.742 train/mre:  
                                                              0.120             
[2024-06-05 17:40:27,158][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.1/3>
[2024-06-05 17:40:27,158][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:40:27,163][HYDRA] 	#164 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 17:40:27,381][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:40:27,383][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:40:27,384][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:40:27,384][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:40:27,386][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:40:27,387][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:40:27,387][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:40:27,388][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:40:27,389][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:40:27,389][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:40:27,389][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:40:27,391][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:40:27,434][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.1/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.41it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.106 train/auc:  
                                                              0.710 train/f1:   
                                                              0.746             
                                                              train/precision:  
                                                              0.663             
                                                              train/recall:     
                                                              0.855 train/mre:  
                                                              0.119             
[2024-06-05 17:40:43,445][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.1/4>
[2024-06-05 17:40:43,448][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:40:43,453][HYDRA] 	#165 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 17:40:43,673][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:40:43,675][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:40:43,676][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:40:43,676][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:40:43,681][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:40:43,682][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:40:43,683][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:40:43,683][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:40:43,684][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:40:43,687][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:40:43,687][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:40:43,688][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:40:43,754][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.1/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.98it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.700     
                                                              val/precision:    
                                                              0.700 val/recall: 
                                                              0.700 val/mre:    
                                                              0.105 train/auc:  
                                                              0.806 train/f1:   
                                                              0.793             
                                                              train/precision:  
                                                              0.852             
                                                              train/recall:     
                                                              0.742 train/mre:  
                                                              0.126             
[2024-06-05 17:40:59,276][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.1/5>
[2024-06-05 17:40:59,276][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:40:59,281][HYDRA] 	#166 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[2024-06-05 17:40:59,511][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:40:59,513][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[rank: 0] Seed set to 6
[2024-06-05 17:40:59,514][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:40:59,514][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:40:59,516][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:40:59,517][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:40:59,517][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:40:59,517][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:40:59,518][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:40:59,520][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:40:59,520][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:40:59,521][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-06-05 17:40:59,645][train.py][INFO] - Starting training...
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.1/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 50.55it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.102 train/auc:  
                                                              0.895 train/f1:   
                                                              0.905             
                                                              train/precision:  
                                                              0.827             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.124             
[2024-06-05 17:41:15,321][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.1/6>
[2024-06-05 17:41:15,322][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:41:15,324][HYDRA] 	#167 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 17:41:15,539][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:41:15,541][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:41:15,542][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:41:15,542][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:41:15,545][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:41:15,545][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:41:15,546][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:41:15,546][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:41:15,547][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:41:15,548][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:41:15,548][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:41:15,549][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:41:15,588][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.1/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 55.37it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.800 val/mre:    
                                                              0.098 train/auc:  
                                                              0.685 train/f1:   
                                                              0.678             
                                                              train/precision:  
                                                              0.695             
                                                              train/recall:     
                                                              0.661 train/mre:  
                                                              0.121             
[2024-06-05 17:41:32,774][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.1/7>
[2024-06-05 17:41:32,775][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:41:32,783][HYDRA] 	#168 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 17:41:33,036][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:41:33,037][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:41:33,039][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:41:33,039][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:41:33,042][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:41:33,042][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:41:33,043][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:41:33,043][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:41:33,044][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:41:33,045][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:41:33,045][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:41:33,046][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:41:33,102][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.1/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.17it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.098 train/auc:  
                                                              0.718 train/f1:   
                                                              0.715             
                                                              train/precision:  
                                                              0.721             
                                                              train/recall:     
                                                              0.710 train/mre:  
                                                              0.115             
[2024-06-05 17:41:50,667][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.1/8>
[2024-06-05 17:41:50,673][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:41:50,687][HYDRA] 	#169 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 17:41:50,918][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:41:50,919][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:41:50,920][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:41:50,921][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:41:50,923][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:41:50,923][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:41:50,924][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:41:50,924][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:41:50,925][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:41:50,927][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:41:50,927][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:41:50,928][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:41:50,998][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.1/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.01it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.636     
                                                              val/precision:    
                                                              0.583 val/recall: 
                                                              0.700 val/mre:    
                                                              0.107 train/auc:  
                                                              0.863 train/f1:   
                                                              0.862             
                                                              train/precision:  
                                                              0.869             
                                                              train/recall:     
                                                              0.855 train/mre:  
                                                              0.127             
[2024-06-05 17:42:07,408][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.1/9>
[2024-06-05 17:42:07,411][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:42:07,421][HYDRA] 	#170 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 17:42:07,654][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:42:07,656][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:42:07,657][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:42:07,657][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:42:07,659][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:42:07,660][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:42:07,660][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:42:07,661][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:42:07,662][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:42:07,664][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:42:07,664][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:42:07,665][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:42:07,827][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.15/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 56.79it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.600 val/mre:    
                                                              0.104 train/auc:  
                                                              0.871 train/f1:   
                                                              0.882             
                                                              train/precision:  
                                                              0.811             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.124             
[2024-06-05 17:42:24,859][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.15/0>
[2024-06-05 17:42:24,860][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:42:24,862][HYDRA] 	#171 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 17:42:25,099][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:42:25,100][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:42:25,102][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:42:25,102][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:42:25,107][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:42:25,108][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:42:25,109][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:42:25,109][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:42:25,110][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:42:25,111][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:42:25,111][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:42:25,112][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:42:25,157][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.15/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 55.70it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.600 val/mre:    
                                                              0.109 train/auc:  
                                                              0.718 train/f1:   
                                                              0.733             
                                                              train/precision:  
                                                              0.696             
                                                              train/recall:     
                                                              0.774 train/mre:  
                                                              0.128             
[2024-06-05 17:42:41,864][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.15/1>
[2024-06-05 17:42:41,865][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:42:41,870][HYDRA] 	#172 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 17:42:42,100][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:42:42,101][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:42:42,102][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:42:42,103][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:42:42,105][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:42:42,105][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:42:42,106][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:42:42,106][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:42:42,107][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:42:42,108][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:42:42,108][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:42:42,110][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:42:42,149][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.15/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 57.52it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.101 train/auc:  
                                                              0.855 train/f1:   
                                                              0.855             
                                                              train/precision:  
                                                              0.855             
                                                              train/recall:     
                                                              0.855 train/mre:  
                                                              0.125             
[2024-06-05 17:42:58,404][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.15/2>
[2024-06-05 17:42:58,405][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:42:58,408][HYDRA] 	#173 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 17:42:58,658][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:42:58,660][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:42:58,661][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:42:58,662][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:42:58,665][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:42:58,665][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:42:58,666][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:42:58,666][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:42:58,667][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:42:58,668][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:42:58,668][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:42:58,670][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:42:58,723][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.15/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.56it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.696     
                                                              val/precision:    
                                                              0.615 val/recall: 
                                                              0.800 val/mre:    
                                                              0.100 train/auc:  
                                                              0.750 train/f1:   
                                                              0.770             
                                                              train/precision:  
                                                              0.712             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.120             
[2024-06-05 17:43:14,976][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.15/3>
[2024-06-05 17:43:14,976][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:43:14,978][HYDRA] 	#174 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 17:43:15,192][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:43:15,194][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:43:15,195][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:43:15,195][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:43:15,197][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:43:15,198][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:43:15,198][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:43:15,199][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:43:15,200][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:43:15,202][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:43:15,202][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:43:15,204][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:43:15,340][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.15/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 56.75it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.106 train/auc:  
                                                              0.677 train/f1:   
                                                              0.718             
                                                              train/precision:  
                                                              0.637             
                                                              train/recall:     
                                                              0.823 train/mre:  
                                                              0.117             
[2024-06-05 17:43:31,799][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.15/4>
[2024-06-05 17:43:31,801][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:43:31,805][HYDRA] 	#175 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 17:43:32,025][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:43:32,026][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:43:32,027][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:43:32,028][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:43:32,030][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:43:32,030][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:43:32,031][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:43:32,031][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:43:32,032][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:43:32,033][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:43:32,033][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:43:32,034][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:43:32,076][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.15/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 57.86it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.110 train/auc:  
                                                              0.879 train/f1:   
                                                              0.887             
                                                              train/precision:  
                                                              0.831             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.126             
[2024-06-05 17:43:47,994][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.15/5>
[2024-06-05 17:43:47,995][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:43:47,997][HYDRA] 	#176 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 17:43:48,228][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:43:48,229][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:43:48,230][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:43:48,231][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:43:48,233][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:43:48,233][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:43:48,234][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:43:48,234][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:43:48,235][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:43:48,236][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:43:48,236][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:43:48,237][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:43:48,278][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.15/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.55it/s v_num: 0.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.737     
                                                              val/precision:    
                                                              0.778 val/recall: 
                                                              0.700 val/mre:    
                                                              0.110 train/auc:  
                                                              0.887 train/f1:   
                                                              0.891             
                                                              train/precision:  
                                                              0.864             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.127             
[2024-06-05 17:44:04,023][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.15/6>
[2024-06-05 17:44:04,026][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:44:04,028][HYDRA] 	#177 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 17:44:04,252][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:44:04,253][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:44:04,255][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:44:04,255][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:44:04,257][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:44:04,258][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:44:04,258][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:44:04,259][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:44:04,260][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:44:04,261][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:44:04,261][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:44:04,262][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:44:04,322][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.15/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.44it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.800 val/mre:    
                                                              0.101 train/auc:  
                                                              0.742 train/f1:   
                                                              0.742             
                                                              train/precision:  
                                                              0.742             
                                                              train/recall:     
                                                              0.742 train/mre:  
                                                              0.119             
[2024-06-05 17:44:19,995][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.15/7>
[2024-06-05 17:44:19,996][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:44:20,002][HYDRA] 	#178 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 17:44:20,213][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:44:20,215][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:44:20,216][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:44:20,216][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:44:20,218][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:44:20,219][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:44:20,219][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:44:20,220][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:44:20,221][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:44:20,223][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:44:20,223][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:44:20,225][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-06-05 17:44:20,365][train.py][INFO] - Starting training...
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.15/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.41it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.102 train/auc:  
                                                              0.629 train/f1:   
                                                              0.623             
                                                              train/precision:  
                                                              0.633             
                                                              train/recall:     
                                                              0.613 train/mre:  
                                                              0.121             
[2024-06-05 17:44:36,546][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.15/8>
[2024-06-05 17:44:36,547][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:44:36,551][HYDRA] 	#179 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 17:44:36,780][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:44:36,781][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:44:36,783][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:44:36,783][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:44:36,785][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:44:36,785][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:44:36,786][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:44:36,786][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:44:36,787][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:44:36,788][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:44:36,788][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:44:36,789][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:44:36,835][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.15/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.91it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.636 val/recall: 
                                                              0.700 val/mre:    
                                                              0.117 train/auc:  
                                                              0.806 train/f1:   
                                                              0.806             
                                                              train/precision:  
                                                              0.806             
                                                              train/recall:     
                                                              0.806 train/mre:  
                                                              0.126             
[2024-06-05 17:44:53,549][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.15/9>
[2024-06-05 17:44:53,551][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:44:53,563][HYDRA] 	#180 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 17:44:53,768][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:44:53,770][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:44:53,771][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:44:53,772][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:44:53,778][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:44:53,779][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:44:53,779][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:44:53,779][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:44:53,781][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:44:53,781][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:44:53,781][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:44:53,782][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:44:53,834][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.2/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 50.91it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.104 train/auc:  
                                                              0.879 train/f1:   
                                                              0.889             
                                                              train/precision:  
                                                              0.822             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.123             
[2024-06-05 17:45:09,967][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.2/0>
[2024-06-05 17:45:09,967][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:45:09,970][HYDRA] 	#181 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 17:45:10,178][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:45:10,179][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:45:10,181][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:45:10,181][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:45:10,184][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:45:10,184][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:45:10,184][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:45:10,185][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:45:10,186][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:45:10,187][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:45:10,187][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:45:10,189][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:45:10,229][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.2/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.48it/s v_num: 0.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.737     
                                                              val/precision:    
                                                              0.778 val/recall: 
                                                              0.700 val/mre:    
                                                              0.104 train/auc:  
                                                              0.782 train/f1:   
                                                              0.814             
                                                              train/precision:  
                                                              0.711             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.123             
[2024-06-05 17:45:26,606][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.2/1>
[2024-06-05 17:45:26,607][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:45:26,612][HYDRA] 	#182 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 17:45:27,207][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:45:27,208][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:45:27,210][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:45:27,210][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:45:27,213][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:45:27,214][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:45:27,214][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:45:27,215][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:45:27,216][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:45:27,218][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:45:27,218][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:45:27,219][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:45:27,320][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.2/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 53.87it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.108 train/auc:  
                                                              0.863 train/f1:   
                                                              0.868             
                                                              train/precision:  
                                                              0.836             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.126             
[2024-06-05 17:45:44,093][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.2/2>
[2024-06-05 17:45:44,093][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:45:44,099][HYDRA] 	#183 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 17:45:44,332][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:45:44,333][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:45:44,335][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:45:44,335][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:45:44,337][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:45:44,338][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:45:44,338][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:45:44,338][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:45:44,339][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:45:44,340][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:45:44,340][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:45:44,342][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:45:44,407][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.2/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 50.14it/s v_num: 0.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.737     
                                                              val/precision:    
                                                              0.778 val/recall: 
                                                              0.700 val/mre:    
                                                              0.105 train/auc:  
                                                              0.831 train/f1:   
                                                              0.847             
                                                              train/precision:  
                                                              0.773             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.123             
[2024-06-05 17:46:01,182][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.2/3>
[2024-06-05 17:46:01,183][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:46:01,187][HYDRA] 	#184 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 17:46:01,403][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:46:01,405][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:46:01,406][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:46:01,406][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:46:01,409][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:46:01,409][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:46:01,410][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:46:01,410][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:46:01,411][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:46:01,412][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:46:01,412][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:46:01,413][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:46:01,462][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.2/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 50.99it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.102 train/auc:  
                                                              0.661 train/f1:   
                                                              0.687             
                                                              train/precision:  
                                                              0.639             
                                                              train/recall:     
                                                              0.742 train/mre:  
                                                              0.122             
[2024-06-05 17:46:18,197][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.2/4>
[2024-06-05 17:46:18,198][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:46:18,203][HYDRA] 	#185 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 17:46:18,436][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:46:18,438][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:46:18,439][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:46:18,440][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:46:18,442][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:46:18,442][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:46:18,443][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:46:18,443][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:46:18,444][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:46:18,445][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:46:18,446][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:46:18,447][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:46:18,493][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.2/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.53it/s v_num: 0.000      
                                                              val/auc: 0.800    
                                                              val/f1: 0.778     
                                                              val/precision:    
                                                              0.875 val/recall: 
                                                              0.700 val/mre:    
                                                              0.107 train/auc:  
                                                              0.919 train/f1:   
                                                              0.922             
                                                              train/precision:  
                                                              0.894             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.125             
[2024-06-05 17:46:34,456][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.2/5>
[2024-06-05 17:46:34,459][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:46:34,463][HYDRA] 	#186 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 17:46:34,661][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:46:34,663][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:46:34,664][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:46:34,664][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:46:34,667][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:46:34,667][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:46:34,668][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:46:34,668][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:46:34,669][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:46:34,672][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:46:34,672][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:46:34,673][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:46:34,802][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.2/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.18it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.696     
                                                              val/precision:    
                                                              0.615 val/recall: 
                                                              0.800 val/mre:    
                                                              0.109 train/auc:  
                                                              0.855 train/f1:   
                                                              0.866             
                                                              train/precision:  
                                                              0.806             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.130             
[2024-06-05 17:46:51,971][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.2/6>
[2024-06-05 17:46:51,972][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:46:51,975][HYDRA] 	#187 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 17:46:52,222][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:46:52,223][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:46:52,225][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:46:52,225][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:46:52,231][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:46:52,231][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:46:52,232][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:46:52,232][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:46:52,233][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:46:52,234][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:46:52,234][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:46:52,235][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:46:52,277][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.2/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.97it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.108 train/auc:  
                                                              0.766 train/f1:   
                                                              0.785             
                                                              train/precision:  
                                                              0.726             
                                                              train/recall:     
                                                              0.855 train/mre:  
                                                              0.117             
[2024-06-05 17:47:08,522][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.2/7>
[2024-06-05 17:47:08,523][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:47:08,527][HYDRA] 	#188 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 17:47:08,772][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:47:08,773][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:47:08,774][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:47:08,775][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:47:08,777][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:47:08,778][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:47:08,778][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:47:08,779][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:47:08,780][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:47:08,780][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:47:08,780][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:47:08,782][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:47:08,828][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.2/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.81it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.800 val/mre:    
                                                              0.102 train/auc:  
                                                              0.702 train/f1:   
                                                              0.661             
                                                              train/precision:  
                                                              0.766             
                                                              train/recall:     
                                                              0.581 train/mre:  
                                                              0.114             
[2024-06-05 17:47:25,002][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.2/8>
[2024-06-05 17:47:25,002][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:47:25,006][HYDRA] 	#189 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 17:47:25,229][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:47:25,230][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:47:25,232][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:47:25,232][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:47:25,234][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:47:25,234][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:47:25,235][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:47:25,235][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:47:25,236][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:47:25,237][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:47:25,238][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:47:25,239][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:47:25,304][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.2/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 58.96it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.700     
                                                              val/precision:    
                                                              0.700 val/recall: 
                                                              0.700 val/mre:    
                                                              0.111 train/auc:  
                                                              0.847 train/f1:   
                                                              0.859             
                                                              train/precision:  
                                                              0.795             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.124             
[2024-06-05 17:47:41,855][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.2/9>
[2024-06-05 17:47:41,855][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:47:41,860][HYDRA] 	#190 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 17:47:42,096][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:47:42,098][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:47:42,099][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:47:42,100][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:47:42,102][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:47:42,102][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:47:42,103][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:47:42,103][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:47:42,104][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:47:42,106][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:47:42,107][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:47:42,108][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:47:42,252][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.25/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 56.53it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.105 train/auc:  
                                                              0.839 train/f1:   
                                                              0.851             
                                                              train/precision:  
                                                              0.792             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.126             
[2024-06-05 17:47:58,664][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.25/0>
[2024-06-05 17:47:58,667][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:47:58,669][HYDRA] 	#191 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 17:47:58,871][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:47:58,872][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:47:58,874][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:47:58,874][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:47:58,877][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:47:58,877][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:47:58,878][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:47:58,878][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:47:58,879][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:47:58,880][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:47:58,880][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:47:58,881][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:47:58,921][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.25/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.42it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.696     
                                                              val/precision:    
                                                              0.615 val/recall: 
                                                              0.800 val/mre:    
                                                              0.113 train/auc:  
                                                              0.815 train/f1:   
                                                              0.816             
                                                              train/precision:  
                                                              0.810             
                                                              train/recall:     
                                                              0.823 train/mre:  
                                                              0.129             
[2024-06-05 17:48:14,880][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.25/1>
[2024-06-05 17:48:14,880][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:48:14,883][HYDRA] 	#192 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 17:48:15,094][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:48:15,096][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:48:15,097][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:48:15,097][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:48:15,100][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:48:15,100][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:48:15,101][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:48:15,101][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:48:15,102][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:48:15,103][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:48:15,103][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:48:15,104][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:48:15,143][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.25/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 67.61it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.103 train/auc:  
                                                              0.919 train/f1:   
                                                              0.919             
                                                              train/precision:  
                                                              0.919             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.123             
[2024-06-05 17:48:31,110][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.25/2>
[2024-06-05 17:48:31,110][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:48:31,117][HYDRA] 	#193 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 17:48:31,338][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:48:31,339][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:48:31,340][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:48:31,340][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:48:31,343][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:48:31,343][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:48:31,344][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:48:31,344][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:48:31,345][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:48:31,347][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:48:31,347][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:48:31,348][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-06-05 17:48:31,399][train.py][INFO] - Starting training...
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.25/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 54.85it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.700     
                                                              val/precision:    
                                                              0.700 val/recall: 
                                                              0.700 val/mre:    
                                                              0.102 train/auc:  
                                                              0.855 train/f1:   
                                                              0.862             
                                                              train/precision:  
                                                              0.824             
                                                              train/recall:     
                                                              0.903 train/mre:  
                                                              0.121             
[2024-06-05 17:48:47,181][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.25/3>
[2024-06-05 17:48:47,182][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:48:47,185][HYDRA] 	#194 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 17:48:47,381][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:48:47,383][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:48:47,384][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:48:47,384][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:48:47,390][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:48:47,390][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:48:47,391][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:48:47,391][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:48:47,392][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:48:47,395][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:48:47,395][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:48:47,396][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:48:47,514][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.25/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.81it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.636 val/recall: 
                                                              0.700 val/mre:    
                                                              0.102 train/auc:  
                                                              0.613 train/f1:   
                                                              0.671             
                                                              train/precision:  
                                                              0.583             
                                                              train/recall:     
                                                              0.790 train/mre:  
                                                              0.111             
[2024-06-05 17:49:03,238][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.25/4>
[2024-06-05 17:49:03,239][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:49:03,243][HYDRA] 	#195 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 17:49:03,458][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:49:03,459][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:49:03,461][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:49:03,461][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:49:03,463][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:49:03,463][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:49:03,464][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:49:03,464][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:49:03,465][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:49:03,466][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:49:03,466][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:49:03,468][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:49:03,505][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.25/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 55.88it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.833 val/recall: 
                                                              0.500 val/mre:    
                                                              0.107 train/auc:  
                                                              0.903 train/f1:   
                                                              0.905             
                                                              train/precision:  
                                                              0.891             
                                                              train/recall:     
                                                              0.919 train/mre:  
                                                              0.126             
[2024-06-05 17:49:19,402][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.25/5>
[2024-06-05 17:49:19,403][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:49:19,406][HYDRA] 	#196 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 17:49:19,610][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:49:19,612][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:49:19,613][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:49:19,613][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:49:19,616][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:49:19,616][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:49:19,617][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:49:19,617][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:49:19,618][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:49:19,619][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:49:19,619][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:49:19,620][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:49:19,656][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.25/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 55.36it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.300 val/mre:    
                                                              0.106 train/auc:  
                                                              0.839 train/f1:   
                                                              0.855             
                                                              train/precision:  
                                                              0.776             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.125             
[2024-06-05 17:49:35,937][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.25/6>
[2024-06-05 17:49:35,938][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:49:35,941][HYDRA] 	#197 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 17:49:36,136][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:49:36,138][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:49:36,139][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:49:36,140][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:49:36,142][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:49:36,142][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:49:36,143][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:49:36,143][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:49:36,144][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:49:36,145][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:49:36,146][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:49:36,147][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:49:36,186][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.25/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.05it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.636 val/recall: 
                                                              0.700 val/mre:    
                                                              0.101 train/auc:  
                                                              0.734 train/f1:   
                                                              0.740             
                                                              train/precision:  
                                                              0.723             
                                                              train/recall:     
                                                              0.758 train/mre:  
                                                              0.116             
[2024-06-05 17:49:51,831][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.25/7>
[2024-06-05 17:49:51,833][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:49:51,835][HYDRA] 	#198 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 17:49:52,030][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:49:52,032][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:49:52,033][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:49:52,034][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:49:52,037][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:49:52,037][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:49:52,038][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:49:52,038][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:49:52,039][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:49:52,042][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:49:52,042][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:49:52,044][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:49:52,169][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.25/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 50.21it/s v_num: 0.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.737     
                                                              val/precision:    
                                                              0.778 val/recall: 
                                                              0.700 val/mre:    
                                                              0.100 train/auc:  
                                                              0.758 train/f1:   
                                                              0.758             
                                                              train/precision:  
                                                              0.758             
                                                              train/recall:     
                                                              0.758 train/mre:  
                                                              0.117             
[2024-06-05 17:50:07,508][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.25/8>
[2024-06-05 17:50:07,508][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:50:07,511][HYDRA] 	#199 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 17:50:07,717][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:50:07,719][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:50:07,720][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:50:07,720][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:50:07,722][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:50:07,723][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:50:07,723][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:50:07,724][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:50:07,725][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:50:07,725][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:50:07,726][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:50:07,727][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:50:07,762][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.25/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 57.13it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.640     
                                                              val/precision:    
                                                              0.533 val/recall: 
                                                              0.800 val/mre:    
                                                              0.112 train/auc:  
                                                              0.871 train/f1:   
                                                              0.873             
                                                              train/precision:  
                                                              0.859             
                                                              train/recall:     
                                                              0.887 train/mre:  
                                                              0.126             
[2024-06-05 17:50:23,407][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.25/9>
[2024-06-05 17:50:23,408][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:50:23,412][HYDRA] 	#200 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 17:50:23,612][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:50:23,613][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:50:23,615][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:50:23,615][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:50:23,618][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:50:23,618][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:50:23,618][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:50:23,619][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:50:23,620][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:50:23,620][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:50:23,621][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:50:23,622][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:50:23,660][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.3/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 57.12it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.109 train/auc:  
                                                              0.879 train/f1:   
                                                              0.892             
                                                              train/precision:  
                                                              0.805             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.128             
[2024-06-05 17:50:39,406][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.3/0>
[2024-06-05 17:50:39,407][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:50:39,410][HYDRA] 	#201 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 17:50:39,619][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:50:39,620][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:50:39,622][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:50:39,622][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:50:39,624][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:50:39,625][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:50:39,625][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:50:39,625][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:50:39,626][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:50:39,628][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:50:39,628][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:50:39,629][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:50:39,668][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.3/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.28it/s v_num: 0.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.762     
                                                              val/precision:    
                                                              0.727 val/recall: 
                                                              0.800 val/mre:    
                                                              0.115 train/auc:  
                                                              0.863 train/f1:   
                                                              0.852             
                                                              train/precision:  
                                                              0.925             
                                                              train/recall:     
                                                              0.790 train/mre:  
                                                              0.129             
[2024-06-05 17:50:55,204][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.3/1>
[2024-06-05 17:50:55,204][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:50:55,206][HYDRA] 	#202 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 17:50:55,420][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:50:55,421][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:50:55,423][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:50:55,423][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:50:55,426][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:50:55,426][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:50:55,427][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:50:55,427][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:50:55,428][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:50:55,430][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:50:55,430][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:50:55,432][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:50:55,540][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.3/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.69it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.700     
                                                              val/precision:    
                                                              0.700 val/recall: 
                                                              0.700 val/mre:    
                                                              0.106 train/auc:  
                                                              0.895 train/f1:   
                                                              0.891             
                                                              train/precision:  
                                                              0.930             
                                                              train/recall:     
                                                              0.855 train/mre:  
                                                              0.125             
[2024-06-05 17:51:10,964][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.3/2>
[2024-06-05 17:51:10,965][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:51:10,971][HYDRA] 	#203 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 17:51:11,173][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:51:11,174][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:51:11,176][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:51:11,176][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:51:11,178][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:51:11,179][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:51:11,179][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:51:11,180][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:51:11,181][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:51:11,181][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:51:11,181][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:51:11,183][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:51:11,218][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.3/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.33it/s v_num: 0.000      
                                                              val/auc: 0.800    
                                                              val/f1: 0.778     
                                                              val/precision:    
                                                              0.875 val/recall: 
                                                              0.700 val/mre:    
                                                              0.110 train/auc:  
                                                              0.847 train/f1:   
                                                              0.846             
                                                              train/precision:  
                                                              0.852             
                                                              train/recall:     
                                                              0.839 train/mre:  
                                                              0.127             
[2024-06-05 17:51:26,715][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.3/3>
[2024-06-05 17:51:26,717][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:51:26,720][HYDRA] 	#204 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 17:51:26,912][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:51:26,914][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:51:26,915][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:51:26,915][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:51:26,917][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:51:26,918][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:51:26,918][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:51:26,919][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:51:26,920][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:51:26,920][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:51:26,921][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:51:26,923][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:51:26,964][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.3/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.71it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.500 val/mre:    
                                                              0.109 train/auc:  
                                                              0.645 train/f1:   
                                                              0.690             
                                                              train/precision:  
                                                              0.613             
                                                              train/recall:     
                                                              0.790 train/mre:  
                                                              0.113             
[2024-06-05 17:51:43,068][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.3/4>
[2024-06-05 17:51:43,069][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:51:43,072][HYDRA] 	#205 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 17:51:43,299][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:51:43,300][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:51:43,301][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:51:43,302][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:51:43,304][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:51:43,305][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:51:43,305][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:51:43,306][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:51:43,307][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:51:43,308][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:51:43,308][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:51:43,310][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:51:43,363][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.3/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.57it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.400 val/mre:    
                                                              0.117 train/auc:  
                                                              0.895 train/f1:   
                                                              0.899             
                                                              train/precision:  
                                                              0.866             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.127             
[2024-06-05 17:51:58,829][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.3/5>
[2024-06-05 17:51:58,829][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:51:58,832][HYDRA] 	#206 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 17:51:59,030][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:51:59,031][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:51:59,032][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:51:59,033][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:51:59,035][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:51:59,035][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:51:59,036][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:51:59,036][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:51:59,037][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:51:59,040][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:51:59,040][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:51:59,041][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:51:59,154][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.3/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.57it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre:    
                                                              0.108 train/auc:  
                                                              0.863 train/f1:   
                                                              0.878             
                                                              train/precision:  
                                                              0.792             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.132             
[2024-06-05 17:52:14,304][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.3/6>
[2024-06-05 17:52:14,304][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:52:14,307][HYDRA] 	#207 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 17:52:14,508][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:52:14,509][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:52:14,511][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:52:14,511][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:52:14,514][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:52:14,514][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:52:14,515][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:52:14,515][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:52:14,516][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:52:14,517][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:52:14,517][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:52:14,518][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:52:14,553][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.3/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.04it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.600 val/mre:    
                                                              0.101 train/auc:  
                                                              0.718 train/f1:   
                                                              0.711             
                                                              train/precision:  
                                                              0.729             
                                                              train/recall:     
                                                              0.694 train/mre:  
                                                              0.118             
[2024-06-05 17:52:29,738][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.3/7>
[2024-06-05 17:52:29,739][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:52:29,742][HYDRA] 	#208 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 17:52:29,948][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:52:29,949][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:52:29,951][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:52:29,951][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:52:29,953][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:52:29,954][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:52:29,954][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:52:29,954][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:52:29,956][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:52:29,956][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:52:29,956][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:52:29,958][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:52:29,995][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.3/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.03it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.700     
                                                              val/precision:    
                                                              0.700 val/recall: 
                                                              0.700 val/mre:    
                                                              0.102 train/auc:  
                                                              0.742 train/f1:   
                                                              0.724             
                                                              train/precision:  
                                                              0.778             
                                                              train/recall:     
                                                              0.677 train/mre:  
                                                              0.119             
[2024-06-05 17:52:45,999][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.3/8>
[2024-06-05 17:52:45,999][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:52:46,003][HYDRA] 	#209 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=argued_someone data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 17:52:46,194][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:52:46,196][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:52:46,197][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:52:46,197][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:52:46,199][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:52:46,200][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:52:46,200][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:52:46,201][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:52:46,202][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:52:46,203][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:52:46,203][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:52:46,205][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:52:46,242][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.3/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.52it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.640     
                                                              val/precision:    
                                                              0.533 val/recall: 
                                                              0.800 val/mre:    
                                                              0.112 train/auc:  
                                                              0.887 train/f1:   
                                                              0.892             
                                                              train/precision:  
                                                              0.853             
                                                              train/recall:     
                                                              0.935 train/mre:  
                                                              0.127             
[2024-06-05 17:53:01,839][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/argued_someone/0.3/9>
[2024-06-05 17:53:01,840][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:53:01,843][HYDRA] 	#210 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 17:53:02,057][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:53:02,059][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:53:02,061][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:53:02,061][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:53:02,063][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:53:02,064][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:53:02,064][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:53:02,064][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:53:02,065][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:53:02,068][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:53:02,068][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:53:02,070][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:53:02,177][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.0/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 50.07it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 0.695  
                                                              train/f1: 0.695   
                                                              train/precision:  
                                                              0.695             
                                                              train/recall:     
                                                              0.695 train/mre:  
                                                              nan               
[2024-06-05 17:53:18,647][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.0/0>
[2024-06-05 17:53:18,648][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:53:18,657][HYDRA] 	#211 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 17:53:18,911][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:53:18,913][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:53:18,914][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:53:18,914][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:53:18,918][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:53:18,918][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:53:18,919][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:53:18,919][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:53:18,920][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:53:18,921][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:53:18,921][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:53:18,922][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:53:18,963][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.0/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.30it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.720  
                                                              train/f1: 0.637   
                                                              train/precision:  
                                                              0.906             
                                                              train/recall:     
                                                              0.492 train/mre:  
                                                              nan               
[2024-06-05 17:53:34,595][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.0/1>
[2024-06-05 17:53:34,596][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:53:34,601][HYDRA] 	#212 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 17:53:35,244][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:53:35,245][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:53:35,247][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:53:35,247][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:53:35,249][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:53:35,250][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:53:35,250][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:53:35,251][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:53:35,252][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:53:35,252][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:53:35,253][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:53:35,254][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:53:35,291][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.0/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 67.67it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.686  
                                                              train/f1: 0.689   
                                                              train/precision:  
                                                              0.683             
                                                              train/recall:     
                                                              0.695 train/mre:  
                                                              nan               
[2024-06-05 17:53:50,536][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.0/2>
[2024-06-05 17:53:50,536][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:53:50,542][HYDRA] 	#213 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 17:53:50,754][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:53:50,755][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:53:50,757][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:53:50,757][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:53:50,759][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:53:50,760][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:53:50,760][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:53:50,761][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:53:50,762][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:53:50,763][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:53:50,763][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:53:50,764][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:53:50,813][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.0/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 56.34it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.729  
                                                              train/f1: 0.667   
                                                              train/precision:  
                                                              0.865             
                                                              train/recall:     
                                                              0.542 train/mre:  
                                                              nan               
[2024-06-05 17:54:05,529][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.0/3>
[2024-06-05 17:54:05,530][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:54:05,532][HYDRA] 	#214 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 17:54:05,732][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:54:05,734][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:54:05,735][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:54:05,735][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:54:05,738][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:54:05,739][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:54:05,739][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:54:05,739][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:54:05,741][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:54:05,743][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:54:05,743][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:54:05,744][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:54:05,858][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.0/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.33it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 0.737  
                                                              train/f1: 0.760   
                                                              train/precision:  
                                                              0.700             
                                                              train/recall:     
                                                              0.831 train/mre:  
                                                              nan               
[2024-06-05 17:54:20,600][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.0/4>
[2024-06-05 17:54:20,601][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:54:20,605][HYDRA] 	#215 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 17:54:20,819][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:54:20,820][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:54:20,822][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:54:20,822][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:54:20,824][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:54:20,825][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:54:20,825][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:54:20,825][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:54:20,827][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:54:20,827][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:54:20,827][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:54:20,829][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:54:20,870][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.0/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.99it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.729  
                                                              train/f1: 0.750   
                                                              train/precision:  
                                                              0.696             
                                                              train/recall:     
                                                              0.814 train/mre:  
                                                              nan               
[2024-06-05 17:54:35,666][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.0/5>
[2024-06-05 17:54:35,667][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:54:35,672][HYDRA] 	#216 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 17:54:35,883][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:54:35,885][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:54:35,886][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:54:35,886][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:54:35,888][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:54:35,889][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:54:35,889][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:54:35,890][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:54:35,891][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:54:35,891][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:54:35,891][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:54:35,894][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:54:35,937][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.0/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.79it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.695  
                                                              train/f1: 0.633   
                                                              train/precision:  
                                                              0.795             
                                                              train/recall:     
                                                              0.525 train/mre:  
                                                              nan               
[2024-06-05 17:54:52,208][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.0/6>
[2024-06-05 17:54:52,211][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:54:52,215][HYDRA] 	#217 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 17:54:52,525][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:54:52,527][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:54:52,528][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:54:52,528][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:54:52,531][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:54:52,532][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:54:52,532][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:54:52,532][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:54:52,533][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:54:52,535][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:54:52,535][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:54:52,536][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:54:52,629][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.0/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.93it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.621     
                                                              val/precision:    
                                                              0.474 val/recall: 
                                                              0.900 val/mre: nan
                                                              train/auc: 0.661  
                                                              train/f1: 0.701   
                                                              train/precision:  
                                                              0.627             
                                                              train/recall:     
                                                              0.797 train/mre:  
                                                              nan               
[2024-06-05 17:55:08,004][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.0/7>
[2024-06-05 17:55:08,005][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:55:08,015][HYDRA] 	#218 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 17:55:08,237][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:55:08,238][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:55:08,240][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:55:08,240][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:55:08,243][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:55:08,243][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:55:08,244][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:55:08,244][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:55:08,245][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:55:08,248][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:55:08,248][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:55:08,249][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:55:08,401][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.0/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.56it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 0.720  
                                                              train/f1: 0.629   
                                                              train/precision:  
                                                              0.933             
                                                              train/recall:     
                                                              0.475 train/mre:  
                                                              nan               
[2024-06-05 17:55:22,987][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.0/8>
[2024-06-05 17:55:22,988][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:55:22,991][HYDRA] 	#219 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 17:55:23,196][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:55:23,197][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:55:23,198][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:55:23,199][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:55:23,201][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:55:23,201][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:55:23,202][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:55:23,202][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:55:23,203][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:55:23,204][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:55:23,204][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:55:23,205][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:55:23,240][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.0/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.51it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 0.763  
                                                              train/f1: 0.770   
                                                              train/precision:  
                                                              0.746             
                                                              train/recall:     
                                                              0.797 train/mre:  
                                                              nan               
[2024-06-05 17:55:38,284][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.0/9>
[2024-06-05 17:55:38,284][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:55:38,289][HYDRA] 	#220 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 17:55:38,493][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:55:38,494][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:55:38,496][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:55:38,496][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:55:38,499][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:55:38,499][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:55:38,500][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:55:38,500][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:55:38,501][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:55:38,502][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:55:38,502][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:55:38,503][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:55:38,542][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.05/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.29it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.070 train/auc:  
                                                              0.729 train/f1:   
                                                              0.750             
                                                              train/precision:  
                                                              0.696             
                                                              train/recall:     
                                                              0.814 train/mre:  
                                                              0.080             
[2024-06-05 17:55:53,429][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.05/0>
[2024-06-05 17:55:53,429][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:55:53,433][HYDRA] 	#221 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 17:55:53,647][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:55:53,649][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:55:53,650][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:55:53,650][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:55:53,652][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:55:53,653][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:55:53,653][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:55:53,654][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:55:53,655][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:55:53,656][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:55:53,656][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:55:53,658][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:55:53,703][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.05/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 70.23it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.300 val/mre:    
                                                              0.076 train/auc:  
                                                              0.712 train/f1:   
                                                              0.750             
                                                              train/precision:  
                                                              0.662             
                                                              train/recall:     
                                                              0.864 train/mre:  
                                                              0.084             
[2024-06-05 17:56:08,092][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.05/1>
[2024-06-05 17:56:08,093][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:56:08,095][HYDRA] 	#222 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 17:56:08,294][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:56:08,295][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:56:08,297][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:56:08,297][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:56:08,300][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:56:08,300][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:56:08,301][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:56:08,301][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:56:08,302][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:56:08,304][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:56:08,305][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:56:08,307][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:56:08,418][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.05/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.74it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.072 train/auc:  
                                                              0.686 train/f1:   
                                                              0.709             
                                                              train/precision:  
                                                              0.662             
                                                              train/recall:     
                                                              0.763 train/mre:  
                                                              0.079             
[2024-06-05 17:56:23,389][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.05/2>
[2024-06-05 17:56:23,390][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:56:23,393][HYDRA] 	#223 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 17:56:23,608][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:56:23,610][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:56:23,611][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:56:23,612][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:56:23,614][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:56:23,614][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:56:23,615][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:56:23,615][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:56:23,616][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:56:23,617][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:56:23,617][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:56:23,618][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:56:23,654][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.05/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.58it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.076 train/auc:  
                                                              0.814 train/f1:   
                                                              0.804             
                                                              train/precision:  
                                                              0.849             
                                                              train/recall:     
                                                              0.763 train/mre:  
                                                              0.086             
[2024-06-05 17:56:38,409][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.05/3>
[2024-06-05 17:56:38,409][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:56:38,412][HYDRA] 	#224 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 17:56:38,612][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:56:38,614][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:56:38,615][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:56:38,616][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:56:38,620][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:56:38,621][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:56:38,621][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:56:38,622][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:56:38,623][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:56:38,624][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:56:38,624][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:56:38,625][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:56:38,667][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.05/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.91it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.069 train/auc:  
                                                              0.746 train/f1:   
                                                              0.737             
                                                              train/precision:  
                                                              0.764             
                                                              train/recall:     
                                                              0.712 train/mre:  
                                                              0.089             
[2024-06-05 17:56:53,423][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.05/4>
[2024-06-05 17:56:53,423][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:56:53,426][HYDRA] 	#225 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 17:56:53,637][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:56:53,639][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:56:53,640][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:56:53,640][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:56:53,642][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:56:53,643][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:56:53,643][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:56:53,644][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:56:53,645][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:56:53,645][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:56:53,645][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:56:53,647][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:56:53,690][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.05/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 70.12it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.077 train/auc:  
                                                              0.797 train/f1:   
                                                              0.797             
                                                              train/precision:  
                                                              0.797             
                                                              train/recall:     
                                                              0.797 train/mre:  
                                                              0.093             
[2024-06-05 17:57:08,107][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.05/5>
[2024-06-05 17:57:08,108][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:57:08,110][HYDRA] 	#226 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 17:57:08,301][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:57:08,302][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:57:08,304][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:57:08,304][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:57:08,306][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:57:08,307][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:57:08,307][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:57:08,307][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:57:08,308][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:57:08,311][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:57:08,312][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:57:08,313][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:57:08,365][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.05/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.52it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.060 train/auc:  
                                                              0.653 train/f1:   
                                                              0.692             
                                                              train/precision:  
                                                              0.622             
                                                              train/recall:     
                                                              0.780 train/mre:  
                                                              0.071             
[2024-06-05 17:57:23,294][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.05/6>
[2024-06-05 17:57:23,296][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:57:23,299][HYDRA] 	#227 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 17:57:23,503][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:57:23,505][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:57:23,506][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:57:23,506][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:57:23,509][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:57:23,510][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:57:23,510][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:57:23,511][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:57:23,512][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:57:23,512][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:57:23,513][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:57:23,514][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:57:23,606][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.05/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.90it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.062 train/auc:  
                                                              0.737 train/f1:   
                                                              0.770             
                                                              train/precision:  
                                                              0.684             
                                                              train/recall:     
                                                              0.881 train/mre:  
                                                              0.076             
[2024-06-05 17:57:38,662][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.05/7>
[2024-06-05 17:57:38,662][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:57:38,665][HYDRA] 	#228 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 17:57:38,874][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:57:38,876][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:57:38,877][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:57:38,877][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:57:38,879][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:57:38,880][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:57:38,880][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:57:38,881][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:57:38,882][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:57:38,882][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:57:38,882][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:57:38,884][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:57:38,918][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.05/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 58.72it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.476     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.500 val/mre:    
                                                              0.068 train/auc:  
                                                              0.653 train/f1:   
                                                              0.687             
                                                              train/precision:  
                                                              0.625             
                                                              train/recall:     
                                                              0.763 train/mre:  
                                                              0.082             
[2024-06-05 17:57:54,798][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.05/8>
[2024-06-05 17:57:54,800][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:57:54,808][HYDRA] 	#229 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 17:57:55,019][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:57:55,021][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:57:55,022][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:57:55,022][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:57:55,025][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:57:55,025][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:57:55,026][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:57:55,026][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:57:55,027][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:57:55,028][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:57:55,028][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:57:55,029][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:57:55,075][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.05/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.21it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.080 train/auc:  
                                                              0.839 train/f1:   
                                                              0.832             
                                                              train/precision:  
                                                              0.870             
                                                              train/recall:     
                                                              0.797 train/mre:  
                                                              0.087             
[2024-06-05 17:58:10,194][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.05/9>
[2024-06-05 17:58:10,195][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:58:10,198][HYDRA] 	#230 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 17:58:10,399][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:58:10,400][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:58:10,402][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:58:10,402][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:58:10,404][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:58:10,404][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:58:10,405][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:58:10,405][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:58:10,406][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:58:10,409][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:58:10,410][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:58:10,411][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:58:10,460][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.1/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 55.41it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.066 train/auc:  
                                                              0.737 train/f1:   
                                                              0.735             
                                                              train/precision:  
                                                              0.741             
                                                              train/recall:     
                                                              0.729 train/mre:  
                                                              0.084             
[2024-06-05 17:58:25,639][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.1/0>
[2024-06-05 17:58:25,639][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:58:25,644][HYDRA] 	#231 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 17:58:25,859][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:58:25,861][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:58:25,862][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:58:25,862][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:58:25,865][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:58:25,865][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:58:25,866][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:58:25,866][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:58:25,867][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:58:25,868][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:58:25,868][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:58:25,869][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:58:25,958][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.1/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.03it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.068 train/auc:  
                                                              0.763 train/f1:   
                                                              0.750             
                                                              train/precision:  
                                                              0.792             
                                                              train/recall:     
                                                              0.712 train/mre:  
                                                              0.087             
[2024-06-05 17:58:41,105][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.1/1>
[2024-06-05 17:58:41,106][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:58:41,110][HYDRA] 	#232 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 17:58:41,335][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:58:41,337][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:58:41,338][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:58:41,338][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:58:41,340][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:58:41,341][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:58:41,341][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:58:41,342][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:58:41,343][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:58:41,343][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:58:41,344][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:58:41,345][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:58:41,391][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.1/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.11it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.600 val/mre:    
                                                              0.068 train/auc:  
                                                              0.669 train/f1:   
                                                              0.667             
                                                              train/precision:  
                                                              0.672             
                                                              train/recall:     
                                                              0.661 train/mre:  
                                                              0.091             
[2024-06-05 17:58:56,329][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.1/2>
[2024-06-05 17:58:56,330][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:58:56,333][HYDRA] 	#233 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 17:58:56,542][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:58:56,544][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:58:56,545][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:58:56,545][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:58:56,548][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:58:56,548][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:58:56,549][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:58:56,549][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:58:56,550][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:58:56,551][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:58:56,551][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:58:56,552][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:58:56,594][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.1/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.84it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre:    
                                                              0.069 train/auc:  
                                                              0.898 train/f1:   
                                                              0.897             
                                                              train/precision:  
                                                              0.912             
                                                              train/recall:     
                                                              0.881 train/mre:  
                                                              0.086             
[2024-06-05 17:59:11,394][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.1/3>
[2024-06-05 17:59:11,394][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:59:11,396][HYDRA] 	#234 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 17:59:11,594][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:59:11,596][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:59:11,597][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:59:11,597][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:59:11,602][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:59:11,602][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:59:11,603][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:59:11,603][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:59:11,604][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:59:11,610][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:59:11,610][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:59:11,611][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:59:11,670][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.1/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.35it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.077 train/auc:  
                                                              0.788 train/f1:   
                                                              0.775             
                                                              train/precision:  
                                                              0.827             
                                                              train/recall:     
                                                              0.729 train/mre:  
                                                              0.088             
[2024-06-05 17:59:27,021][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.1/4>
[2024-06-05 17:59:27,022][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:59:27,031][HYDRA] 	#235 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 17:59:27,257][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:59:27,259][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:59:27,260][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:59:27,260][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:59:27,262][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:59:27,263][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:59:27,263][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:59:27,264][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:59:27,265][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:59:27,265][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:59:27,266][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:59:27,267][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:59:27,364][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.1/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.89it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.073 train/auc:  
                                                              0.915 train/f1:   
                                                              0.909             
                                                              train/precision:  
                                                              0.980             
                                                              train/recall:     
                                                              0.847 train/mre:  
                                                              0.089             
[2024-06-05 17:59:42,510][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.1/5>
[2024-06-05 17:59:42,511][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:59:42,514][HYDRA] 	#236 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 17:59:42,727][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:59:42,728][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:59:42,730][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:59:42,730][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:59:42,732][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:59:42,733][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:59:42,733][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:59:42,733][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:59:42,735][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:59:42,735][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:59:42,735][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:59:42,737][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:59:42,795][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.1/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 67.52it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.066 train/auc:  
                                                              0.653 train/f1:   
                                                              0.687             
                                                              train/precision:  
                                                              0.625             
                                                              train/recall:     
                                                              0.763 train/mre:  
                                                              0.074             
[2024-06-05 17:59:57,552][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.1/6>
[2024-06-05 17:59:57,553][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 17:59:57,557][HYDRA] 	#237 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 17:59:57,764][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 17:59:57,765][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 17:59:57,766][train.py][INFO] - Instantiating callbacks...
[2024-06-05 17:59:57,767][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 17:59:57,769][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 17:59:57,770][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 17:59:57,770][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 17:59:57,771][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 17:59:57,772][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 17:59:57,772][train.py][INFO] - Instantiating loggers...
[2024-06-05 17:59:57,772][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 17:59:57,774][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 17:59:57,814][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.1/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 67.53it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.064 train/auc:  
                                                              0.669 train/f1:   
                                                              0.727             
                                                              train/precision:  
                                                              0.619             
                                                              train/recall:     
                                                              0.881 train/mre:  
                                                              0.080             
[2024-06-05 18:00:12,419][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.1/7>
[2024-06-05 18:00:12,419][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:00:12,423][HYDRA] 	#238 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 18:00:12,628][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:00:12,629][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:00:12,630][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:00:12,631][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:00:12,633][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:00:12,633][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:00:12,634][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:00:12,634][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:00:12,635][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:00:12,638][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:00:12,638][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:00:12,639][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:00:12,685][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.1/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 66.65it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre:    
                                                              0.074 train/auc:  
                                                              0.729 train/f1:   
                                                              0.761             
                                                              train/precision:  
                                                              0.680             
                                                              train/recall:     
                                                              0.864 train/mre:  
                                                              0.084             
[2024-06-05 18:00:27,522][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.1/8>
[2024-06-05 18:00:27,524][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:00:27,529][HYDRA] 	#239 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 18:00:27,736][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:00:27,737][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:00:27,739][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:00:27,739][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:00:27,742][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:00:27,742][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:00:27,743][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:00:27,743][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:00:27,744][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:00:27,745][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:00:27,745][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:00:27,746][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:00:27,851][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.1/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 69.97it/s v_num: 0.000      
                                                              val/auc: 0.300    
                                                              val/f1: 0.300     
                                                              val/precision:    
                                                              0.300 val/recall: 
                                                              0.300 val/mre:    
                                                              0.083 train/auc:  
                                                              0.797 train/f1:   
                                                              0.760             
                                                              train/precision:  
                                                              0.927             
                                                              train/recall:     
                                                              0.644 train/mre:  
                                                              0.085             
[2024-06-05 18:00:42,663][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.1/9>
[2024-06-05 18:00:42,663][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:00:42,667][HYDRA] 	#240 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 18:00:42,862][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:00:42,864][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:00:42,865][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:00:42,865][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:00:42,868][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:00:42,868][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:00:42,869][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:00:42,869][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:00:42,870][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:00:42,871][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:00:42,872][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:00:42,873][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:00:42,908][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.15/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.61it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.068 train/auc:  
                                                              0.780 train/f1:   
                                                              0.764             
                                                              train/precision:  
                                                              0.824             
                                                              train/recall:     
                                                              0.712 train/mre:  
                                                              0.085             
[2024-06-05 18:00:57,853][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.15/0>
[2024-06-05 18:00:57,854][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:00:57,857][HYDRA] 	#241 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 18:00:58,069][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:00:58,071][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:00:58,072][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:00:58,072][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:00:58,075][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:00:58,075][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:00:58,076][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:00:58,076][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:00:58,077][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:00:58,078][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:00:58,078][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:00:58,080][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:00:58,125][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.15/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.18it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.076 train/auc:  
                                                              0.653 train/f1:   
                                                              0.732             
                                                              train/precision:  
                                                              0.596             
                                                              train/recall:     
                                                              0.949 train/mre:  
                                                              0.091             
[2024-06-05 18:01:13,597][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.15/1>
[2024-06-05 18:01:13,598][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:01:13,614][HYDRA] 	#242 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 18:01:13,920][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:01:13,922][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:01:13,923][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:01:13,923][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:01:13,927][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:01:13,928][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:01:13,928][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:01:13,929][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:01:13,930][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:01:13,933][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:01:13,933][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:01:13,934][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:01:14,045][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.15/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 67.84it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.069 train/auc:  
                                                              0.797 train/f1:   
                                                              0.782             
                                                              train/precision:  
                                                              0.843             
                                                              train/recall:     
                                                              0.729 train/mre:  
                                                              0.082             
[2024-06-05 18:01:29,284][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.15/2>
[2024-06-05 18:01:29,285][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:01:29,288][HYDRA] 	#243 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 18:01:29,484][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:01:29,486][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:01:29,487][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:01:29,487][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:01:29,489][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:01:29,490][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:01:29,490][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:01:29,491][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:01:29,492][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:01:29,492][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:01:29,492][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:01:29,494][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:01:29,584][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.15/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 58.78it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre:    
                                                              0.070 train/auc:  
                                                              0.881 train/f1:   
                                                              0.883             
                                                              train/precision:  
                                                              0.869             
                                                              train/recall:     
                                                              0.898 train/mre:  
                                                              0.085             
[2024-06-05 18:01:44,477][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.15/3>
[2024-06-05 18:01:44,478][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:01:44,480][HYDRA] 	#244 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 18:01:44,686][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:01:44,687][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:01:44,689][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:01:44,689][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:01:44,691][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:01:44,692][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:01:44,692][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:01:44,693][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:01:44,694][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:01:44,695][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:01:44,695][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:01:44,696][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:01:44,737][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.15/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.94it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre:    
                                                              0.094 train/auc:  
                                                              0.737 train/f1:   
                                                              0.777             
                                                              train/precision:  
                                                              0.675             
                                                              train/recall:     
                                                              0.915 train/mre:  
                                                              0.096             
[2024-06-05 18:01:59,431][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.15/4>
[2024-06-05 18:01:59,432][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:01:59,437][HYDRA] 	#245 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 18:01:59,639][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:01:59,640][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:01:59,642][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:01:59,642][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:01:59,644][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:01:59,645][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:01:59,645][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:01:59,646][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:01:59,647][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:01:59,647][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:01:59,647][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:01:59,649][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:01:59,689][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.15/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.16it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.071 train/auc:  
                                                              0.890 train/f1:   
                                                              0.885             
                                                              train/precision:  
                                                              0.926             
                                                              train/recall:     
                                                              0.847 train/mre:  
                                                              0.085             
[2024-06-05 18:02:14,634][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.15/5>
[2024-06-05 18:02:14,636][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:02:14,640][HYDRA] 	#246 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 18:02:14,842][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:02:14,843][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:02:14,845][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:02:14,845][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:02:14,848][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:02:14,848][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:02:14,849][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:02:14,849][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:02:14,850][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:02:14,853][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:02:14,853][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:02:14,854][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:02:14,909][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.15/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 56.51it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.072 train/auc:  
                                                              0.737 train/f1:   
                                                              0.756             
                                                              train/precision:  
                                                              0.706             
                                                              train/recall:     
                                                              0.814 train/mre:  
                                                              0.077             
[2024-06-05 18:02:30,263][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.15/6>
[2024-06-05 18:02:30,264][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:02:30,266][HYDRA] 	#247 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 18:02:30,465][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:02:30,467][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:02:30,468][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:02:30,468][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:02:30,470][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:02:30,471][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:02:30,471][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:02:30,472][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:02:30,473][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:02:30,473][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:02:30,473][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:02:30,475][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:02:30,562][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.15/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.95it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.545 val/recall: 
                                                              0.600 val/mre:    
                                                              0.060 train/auc:  
                                                              0.627 train/f1:   
                                                              0.694             
                                                              train/precision:  
                                                              0.588             
                                                              train/recall:     
                                                              0.847 train/mre:  
                                                              0.074             
[2024-06-05 18:02:45,464][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.15/7>
[2024-06-05 18:02:45,464][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:02:45,467][HYDRA] 	#248 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 18:02:45,689][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:02:45,690][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:02:45,691][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:02:45,692][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:02:45,694][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:02:45,694][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:02:45,695][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:02:45,695][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:02:45,696][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:02:45,697][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:02:45,697][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:02:45,698][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:02:45,737][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.15/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.55it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.069 train/auc:  
                                                              0.669 train/f1:   
                                                              0.723             
                                                              train/precision:  
                                                              0.622             
                                                              train/recall:     
                                                              0.864 train/mre:  
                                                              0.079             
[2024-06-05 18:03:01,870][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.15/8>
[2024-06-05 18:03:01,871][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:03:01,874][HYDRA] 	#249 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 18:03:02,098][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:03:02,100][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:03:02,101][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:03:02,102][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:03:02,104][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:03:02,104][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:03:02,105][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:03:02,105][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:03:02,106][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:03:02,107][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:03:02,108][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:03:02,109][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:03:02,159][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.15/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.79it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.480     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.600 val/mre:    
                                                              0.085 train/auc:  
                                                              0.797 train/f1:   
                                                              0.778             
                                                              train/precision:  
                                                              0.857             
                                                              train/recall:     
                                                              0.712 train/mre:  
                                                              0.082             
[2024-06-05 18:03:17,423][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.15/9>
[2024-06-05 18:03:17,423][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:03:17,429][HYDRA] 	#250 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 18:03:17,627][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:03:17,628][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:03:17,629][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:03:17,630][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:03:17,632][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:03:17,632][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:03:17,633][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:03:17,633][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:03:17,634][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:03:17,637][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:03:17,637][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:03:17,638][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:03:17,678][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.2/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 66.38it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.070 train/auc:  
                                                              0.856 train/f1:   
                                                              0.857             
                                                              train/precision:  
                                                              0.850             
                                                              train/recall:     
                                                              0.864 train/mre:  
                                                              0.079             
[2024-06-05 18:03:32,219][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.2/0>
[2024-06-05 18:03:32,220][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:03:32,222][HYDRA] 	#251 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 18:03:32,421][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:03:32,422][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:03:32,424][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:03:32,424][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:03:32,426][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:03:32,427][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:03:32,427][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:03:32,427][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:03:32,429][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:03:32,429][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:03:32,429][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:03:32,431][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:03:32,527][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.2/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 68.27it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.068 train/auc:  
                                                              0.814 train/f1:   
                                                              0.820             
                                                              train/precision:  
                                                              0.794             
                                                              train/recall:     
                                                              0.847 train/mre:  
                                                              0.086             
[2024-06-05 18:03:46,918][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.2/1>
[2024-06-05 18:03:46,919][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:03:46,921][HYDRA] 	#252 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 18:03:47,122][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:03:47,124][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:03:47,125][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:03:47,125][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:03:47,130][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:03:47,130][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:03:47,130][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:03:47,131][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:03:47,132][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:03:47,133][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:03:47,133][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:03:47,134][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:03:47,170][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.2/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.49it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.068 train/auc:  
                                                              0.695 train/f1:   
                                                              0.700             
                                                              train/precision:  
                                                              0.689             
                                                              train/recall:     
                                                              0.712 train/mre:  
                                                              0.079             
[2024-06-05 18:04:02,193][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.2/2>
[2024-06-05 18:04:02,194][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:04:02,197][HYDRA] 	#253 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 18:04:02,408][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:04:02,409][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:04:02,411][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:04:02,411][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:04:02,413][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:04:02,414][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:04:02,414][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:04:02,415][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:04:02,416][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:04:02,416][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:04:02,417][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:04:02,418][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:04:02,459][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.2/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.47it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.070 train/auc:  
                                                              0.890 train/f1:   
                                                              0.889             
                                                              train/precision:  
                                                              0.897             
                                                              train/recall:     
                                                              0.881 train/mre:  
                                                              0.086             
[2024-06-05 18:04:16,975][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.2/3>
[2024-06-05 18:04:16,976][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:04:16,978][HYDRA] 	#254 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 18:04:17,175][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:04:17,177][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:04:17,178][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:04:17,178][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:04:17,180][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:04:17,181][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:04:17,181][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:04:17,182][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:04:17,183][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:04:17,184][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:04:17,184][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:04:17,185][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:04:17,223][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.2/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 69.28it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.064 train/auc:  
                                                              0.636 train/f1:   
                                                              0.715             
                                                              train/precision:  
                                                              0.587             
                                                              train/recall:     
                                                              0.915 train/mre:  
                                                              0.078             
[2024-06-05 18:04:31,999][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.2/4>
[2024-06-05 18:04:31,999][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:04:32,002][HYDRA] 	#255 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 18:04:32,201][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:04:32,203][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:04:32,204][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:04:32,204][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:04:32,207][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:04:32,208][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:04:32,208][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:04:32,209][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:04:32,210][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:04:32,212][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:04:32,212][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:04:32,214][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:04:32,263][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.2/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.98it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.069 train/auc:  
                                                              0.890 train/f1:   
                                                              0.894             
                                                              train/precision:  
                                                              0.859             
                                                              train/recall:     
                                                              0.932 train/mre:  
                                                              0.087             
[2024-06-05 18:04:47,068][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.2/5>
[2024-06-05 18:04:47,069][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:04:47,072][HYDRA] 	#256 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 18:04:47,273][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:04:47,275][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:04:47,276][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:04:47,276][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:04:47,280][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:04:47,281][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:04:47,281][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:04:47,282][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:04:47,283][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:04:47,283][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:04:47,284][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:04:47,285][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:04:47,380][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.2/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.32it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.073 train/auc:  
                                                              0.703 train/f1:   
                                                              0.720             
                                                              train/precision:  
                                                              0.682             
                                                              train/recall:     
                                                              0.763 train/mre:  
                                                              0.076             
[2024-06-05 18:05:01,992][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.2/6>
[2024-06-05 18:05:01,995][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:05:01,999][HYDRA] 	#257 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 18:05:02,186][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:05:02,188][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:05:02,189][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:05:02,190][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:05:02,192][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:05:02,192][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:05:02,192][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:05:02,193][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:05:02,194][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:05:02,195][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:05:02,195][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:05:02,197][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:05:02,236][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.2/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.22it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.063 train/auc:  
                                                              0.669 train/f1:   
                                                              0.727             
                                                              train/precision:  
                                                              0.619             
                                                              train/recall:     
                                                              0.881 train/mre:  
                                                              0.074             
[2024-06-05 18:05:17,057][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.2/7>
[2024-06-05 18:05:17,058][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:05:17,061][HYDRA] 	#258 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 18:05:17,271][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:05:17,273][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:05:17,274][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:05:17,274][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:05:17,277][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:05:17,277][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:05:17,278][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:05:17,278][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:05:17,279][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:05:17,281][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:05:17,282][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:05:17,284][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:05:17,326][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.2/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 57.56it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.061 train/auc:  
                                                              0.686 train/f1:   
                                                              0.722             
                                                              train/precision:  
                                                              0.649             
                                                              train/recall:     
                                                              0.814 train/mre:  
                                                              0.075             
[2024-06-05 18:05:32,395][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.2/8>
[2024-06-05 18:05:32,396][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:05:32,399][HYDRA] 	#259 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[2024-06-05 18:05:32,603][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:05:32,605][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:05:32,606][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:05:32,606][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[rank: 0] Seed set to 9
[2024-06-05 18:05:32,609][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:05:32,609][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:05:32,610][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:05:32,610][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:05:32,611][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:05:32,614][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:05:32,614][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:05:32,615][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-06-05 18:05:32,665][train.py][INFO] - Starting training...
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.2/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.06it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.435     
                                                              val/precision:    
                                                              0.385 val/recall: 
                                                              0.500 val/mre:    
                                                              0.076 train/auc:  
                                                              0.898 train/f1:   
                                                              0.895             
                                                              train/precision:  
                                                              0.927             
                                                              train/recall:     
                                                              0.864 train/mre:  
                                                              0.084             
[2024-06-05 18:05:47,458][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.2/9>
[2024-06-05 18:05:47,459][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:05:47,462][HYDRA] 	#260 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 18:05:47,666][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:05:47,667][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:05:47,669][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:05:47,669][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:05:47,671][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:05:47,672][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:05:47,672][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:05:47,672][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:05:47,674][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:05:47,674][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:05:47,674][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:05:47,676][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:05:47,768][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.25/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 58.44it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.066 train/auc:  
                                                              0.805 train/f1:   
                                                              0.768             
                                                              train/precision:  
                                                              0.950             
                                                              train/recall:     
                                                              0.644 train/mre:  
                                                              0.085             
[2024-06-05 18:06:02,987][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.25/0>
[2024-06-05 18:06:02,988][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:06:02,992][HYDRA] 	#261 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 18:06:03,192][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:06:03,194][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:06:03,195][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:06:03,195][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:06:03,201][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:06:03,202][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:06:03,202][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:06:03,203][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:06:03,204][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:06:03,204][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:06:03,205][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:06:03,206][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:06:03,254][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.25/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 57.96it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.500 val/mre:    
                                                              0.075 train/auc:  
                                                              0.856 train/f1:   
                                                              0.850             
                                                              train/precision:  
                                                              0.889             
                                                              train/recall:     
                                                              0.814 train/mre:  
                                                              0.086             
[2024-06-05 18:06:18,039][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.25/1>
[2024-06-05 18:06:18,039][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:06:18,042][HYDRA] 	#262 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 18:06:18,253][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:06:18,254][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:06:18,255][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:06:18,256][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:06:18,258][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:06:18,258][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:06:18,259][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:06:18,259][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:06:18,260][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:06:18,261][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:06:18,261][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:06:18,262][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:06:18,314][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.25/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.70it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.068 train/auc:  
                                                              0.831 train/f1:   
                                                              0.831             
                                                              train/precision:  
                                                              0.831             
                                                              train/recall:     
                                                              0.831 train/mre:  
                                                              0.082             
[2024-06-05 18:06:33,372][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.25/2>
[2024-06-05 18:06:33,372][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:06:33,375][HYDRA] 	#263 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 18:06:33,581][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:06:33,583][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:06:33,584][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:06:33,584][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:06:33,587][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:06:33,588][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:06:33,588][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:06:33,589][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:06:33,590][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:06:33,593][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:06:33,593][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:06:33,594][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:06:33,654][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.25/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.98it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.068 train/auc:  
                                                              0.941 train/f1:   
                                                              0.941             
                                                              train/precision:  
                                                              0.933             
                                                              train/recall:     
                                                              0.949 train/mre:  
                                                              0.083             
[2024-06-05 18:06:48,432][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.25/3>
[2024-06-05 18:06:48,433][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:06:48,438][HYDRA] 	#264 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 18:06:48,637][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:06:48,639][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:06:48,640][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:06:48,641][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:06:48,643][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:06:48,644][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:06:48,644][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:06:48,645][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:06:48,646][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:06:48,646][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:06:48,646][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:06:48,648][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:06:48,741][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.25/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.30it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.455     
                                                              val/precision:    
                                                              0.417 val/recall: 
                                                              0.500 val/mre:    
                                                              0.065 train/auc:  
                                                              0.746 train/f1:   
                                                              0.727             
                                                              train/precision:  
                                                              0.784             
                                                              train/recall:     
                                                              0.678 train/mre:  
                                                              0.083             
[2024-06-05 18:07:03,477][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.25/4>
[2024-06-05 18:07:03,477][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:07:03,483][HYDRA] 	#265 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 18:07:03,672][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:07:03,674][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:07:03,675][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:07:03,675][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:07:03,677][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:07:03,678][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:07:03,678][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:07:03,679][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:07:03,680][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:07:03,680][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:07:03,680][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:07:03,682][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:07:03,714][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.25/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.28it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.073 train/auc:  
                                                              0.915 train/f1:   
                                                              0.907             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.831 train/mre:  
                                                              0.088             
[2024-06-05 18:07:18,159][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.25/5>
[2024-06-05 18:07:18,160][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:07:18,162][HYDRA] 	#266 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 18:07:18,378][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:07:18,380][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:07:18,381][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:07:18,381][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:07:18,384][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:07:18,385][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:07:18,385][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:07:18,386][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:07:18,387][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:07:18,387][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:07:18,387][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:07:18,389][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:07:18,435][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.25/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.35it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.690     
                                                              val/precision:    
                                                              0.526 val/recall: 
                                                              1.000 val/mre:    
                                                              0.065 train/auc:  
                                                              0.678 train/f1:   
                                                              0.712             
                                                              train/precision:  
                                                              0.644             
                                                              train/recall:     
                                                              0.797 train/mre:  
                                                              0.076             
[2024-06-05 18:07:33,287][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.25/6>
[2024-06-05 18:07:33,287][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:07:33,291][HYDRA] 	#267 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 18:07:33,489][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:07:33,490][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:07:33,492][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:07:33,492][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:07:33,495][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:07:33,495][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:07:33,496][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:07:33,496][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:07:33,497][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:07:33,498][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:07:33,498][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:07:33,499][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:07:33,546][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.25/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 70.66it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.643     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.900 val/mre:    
                                                              0.066 train/auc:  
                                                              0.737 train/f1:   
                                                              0.760             
                                                              train/precision:  
                                                              0.700             
                                                              train/recall:     
                                                              0.831 train/mre:  
                                                              0.076             
[2024-06-05 18:07:48,855][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.25/7>
[2024-06-05 18:07:48,856][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:07:48,860][HYDRA] 	#268 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 18:07:49,078][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:07:49,080][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:07:49,081][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:07:49,081][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:07:49,086][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:07:49,086][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:07:49,087][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:07:49,087][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:07:49,088][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:07:49,098][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:07:49,098][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:07:49,099][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:07:49,226][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.25/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 58.04it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.062 train/auc:  
                                                              0.712 train/f1:   
                                                              0.726             
                                                              train/precision:  
                                                              0.692             
                                                              train/recall:     
                                                              0.763 train/mre:  
                                                              0.075             
[2024-06-05 18:08:04,358][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.25/8>
[2024-06-05 18:08:04,359][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:08:04,363][HYDRA] 	#269 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[2024-06-05 18:08:04,562][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:08:04,563][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:08:04,564][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:08:04,565][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[rank: 0] Seed set to 9
[2024-06-05 18:08:04,567][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:08:04,568][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:08:04,568][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:08:04,568][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:08:04,569][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:08:04,570][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:08:04,570][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:08:04,571][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-06-05 18:08:04,610][train.py][INFO] - Starting training...
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.25/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.67it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.400 val/mre:    
                                                              0.072 train/auc:  
                                                              0.924 train/f1:   
                                                              0.923             
                                                              train/precision:  
                                                              0.931             
                                                              train/recall:     
                                                              0.915 train/mre:  
                                                              0.084             
[2024-06-05 18:08:19,190][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.25/9>
[2024-06-05 18:08:19,192][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:08:19,197][HYDRA] 	#270 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 18:08:19,395][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:08:19,396][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:08:19,398][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:08:19,398][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:08:19,400][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:08:19,401][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:08:19,401][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:08:19,402][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:08:19,403][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:08:19,403][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:08:19,404][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:08:19,406][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:08:19,451][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.3/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.81it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre:    
                                                              0.073 train/auc:  
                                                              0.822 train/f1:   
                                                              0.837             
                                                              train/precision:  
                                                              0.771             
                                                              train/recall:     
                                                              0.915 train/mre:  
                                                              0.082             
[2024-06-05 18:08:34,164][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.3/0>
[2024-06-05 18:08:34,165][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:08:34,169][HYDRA] 	#271 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 18:08:34,379][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:08:34,381][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:08:34,382][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:08:34,382][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:08:34,385][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:08:34,385][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:08:34,386][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:08:34,386][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:08:34,387][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:08:34,388][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:08:34,388][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:08:34,389][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:08:34,433][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.3/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.14it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.067 train/auc:  
                                                              0.890 train/f1:   
                                                              0.899             
                                                              train/precision:  
                                                              0.829             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.084             
[2024-06-05 18:08:49,207][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.3/1>
[2024-06-05 18:08:49,207][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:08:49,209][HYDRA] 	#272 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 18:08:49,408][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:08:49,409][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:08:49,411][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:08:49,411][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:08:49,413][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:08:49,414][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:08:49,414][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:08:49,414][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:08:49,415][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:08:49,418][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:08:49,418][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:08:49,420][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:08:49,514][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.3/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.00it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.300 val/mre:    
                                                              0.074 train/auc:  
                                                              0.788 train/f1:   
                                                              0.815             
                                                              train/precision:  
                                                              0.724             
                                                              train/recall:     
                                                              0.932 train/mre:  
                                                              0.085             
[2024-06-05 18:09:04,425][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.3/2>
[2024-06-05 18:09:04,426][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:09:04,428][HYDRA] 	#273 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[2024-06-05 18:09:04,625][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:09:04,626][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:09:04,627][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:09:04,628][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[rank: 0] Seed set to 3
[2024-06-05 18:09:04,630][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:09:04,630][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:09:04,631][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:09:04,631][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:09:04,632][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:09:04,633][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:09:04,633][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:09:04,634][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-06-05 18:09:04,669][train.py][INFO] - Starting training...
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.3/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.96it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.500 val/mre:    
                                                              0.068 train/auc:  
                                                              0.890 train/f1:   
                                                              0.887             
                                                              train/precision:  
                                                              0.911             
                                                              train/recall:     
                                                              0.864 train/mre:  
                                                              0.089             
[2024-06-05 18:09:19,462][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.3/3>
[2024-06-05 18:09:19,462][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:09:19,466][HYDRA] 	#274 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 18:09:19,675][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:09:19,677][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:09:19,679][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:09:19,679][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:09:19,681][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:09:19,681][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:09:19,682][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:09:19,682][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:09:19,683][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:09:19,684][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:09:19,684][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:09:19,685][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:09:19,724][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.3/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.64it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre:    
                                                              0.084 train/auc:  
                                                              0.576 train/f1:   
                                                              0.688             
                                                              train/precision:  
                                                              0.545             
                                                              train/recall:     
                                                              0.932 train/mre:  
                                                              0.080             
[2024-06-05 18:09:34,618][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.3/4>
[2024-06-05 18:09:34,619][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:09:34,622][HYDRA] 	#275 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 18:09:34,827][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:09:34,829][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:09:34,830][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:09:34,831][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:09:34,833][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:09:34,833][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:09:34,834][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:09:34,834][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:09:34,835][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:09:34,836][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:09:34,836][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:09:34,837][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:09:34,875][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.3/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 66.06it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.073 train/auc:  
                                                              0.975 train/f1:   
                                                              0.975             
                                                              train/precision:  
                                                              0.967             
                                                              train/recall:     
                                                              0.983 train/mre:  
                                                              0.089             
[2024-06-05 18:09:49,359][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.3/5>
[2024-06-05 18:09:49,359][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:09:49,363][HYDRA] 	#276 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 18:09:49,573][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:09:49,575][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:09:49,576][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:09:49,576][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:09:49,578][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:09:49,579][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:09:49,579][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:09:49,580][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:09:49,581][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:09:49,584][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:09:49,584][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:09:49,586][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:09:49,676][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.3/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.44it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.621     
                                                              val/precision:    
                                                              0.474 val/recall: 
                                                              0.900 val/mre:    
                                                              0.066 train/auc:  
                                                              0.703 train/f1:   
                                                              0.720             
                                                              train/precision:  
                                                              0.682             
                                                              train/recall:     
                                                              0.763 train/mre:  
                                                              0.077             
[2024-06-05 18:10:04,240][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.3/6>
[2024-06-05 18:10:04,241][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:10:04,245][HYDRA] 	#277 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[2024-06-05 18:10:04,439][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:10:04,441][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[rank: 0] Seed set to 7
[2024-06-05 18:10:04,442][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:10:04,442][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:10:04,445][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:10:04,445][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:10:04,445][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:10:04,446][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:10:04,447][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:10:04,447][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:10:04,447][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:10:04,449][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-06-05 18:10:04,495][train.py][INFO] - Starting training...
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.3/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 66.53it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.100 val/mre:    
                                                              0.067 train/auc:  
                                                              0.593 train/f1:   
                                                              0.704             
                                                              train/precision:  
                                                              0.553             
                                                              train/recall:     
                                                              0.966 train/mre:  
                                                              0.081             
[2024-06-05 18:10:19,062][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.3/7>
[2024-06-05 18:10:19,063][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:10:19,065][HYDRA] 	#278 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 18:10:19,738][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:10:19,740][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:10:19,741][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:10:19,741][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:10:19,743][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:10:19,744][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:10:19,744][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:10:19,745][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:10:19,746][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:10:19,746][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:10:19,747][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:10:19,748][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:10:19,794][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.3/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 68.18it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.063 train/auc:  
                                                              0.720 train/f1:   
                                                              0.763             
                                                              train/precision:  
                                                              0.663             
                                                              train/recall:     
                                                              0.898 train/mre:  
                                                              0.078             
[2024-06-05 18:10:34,328][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.3/8>
[2024-06-05 18:10:34,333][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:10:34,337][HYDRA] 	#279 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=controlling_temper data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 18:10:34,536][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:10:34,538][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:10:34,539][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:10:34,540][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:10:34,543][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:10:34,544][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:10:34,544][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:10:34,545][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:10:34,546][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:10:34,546][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:10:34,547][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:10:34,549][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:10:34,584][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.3/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.03it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.400 val/mre:    
                                                              0.070 train/auc:  
                                                              0.797 train/f1:   
                                                              0.745             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.593 train/mre:  
                                                              0.089             
[2024-06-05 18:10:49,398][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/controlling_temper/0.3/9>
[2024-06-05 18:10:49,399][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:10:49,401][HYDRA] 	#280 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 18:10:49,604][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:10:49,605][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:10:49,606][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:10:49,607][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:10:49,610][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:10:49,610][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:10:49,610][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:10:49,611][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:10:49,612][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:10:49,615][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:10:49,615][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:10:49,616][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:10:49,720][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.0/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.15it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.825  
                                                              train/f1: 0.823   
                                                              train/precision:  
                                                              0.836             
                                                              train/recall:     
                                                              0.810 train/mre:  
                                                              nan               
[2024-06-05 18:11:05,611][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.0/0>
[2024-06-05 18:11:05,612][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:11:05,615][HYDRA] 	#281 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 18:11:05,832][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:11:05,834][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:11:05,835][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:11:05,835][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:11:05,838][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:11:05,838][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:11:05,839][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:11:05,839][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:11:05,840][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:11:05,841][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:11:05,841][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:11:05,842][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:11:05,884][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.0/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 53.90it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.952  
                                                              train/f1: 0.954   
                                                              train/precision:  
                                                              0.925             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              nan               
[2024-06-05 18:11:20,859][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.0/1>
[2024-06-05 18:11:20,860][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:11:20,863][HYDRA] 	#282 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 18:11:21,069][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:11:21,071][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:11:21,072][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:11:21,072][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:11:21,074][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:11:21,075][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:11:21,075][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:11:21,076][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:11:21,077][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:11:21,077][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:11:21,078][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:11:21,079][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:11:21,123][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.0/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.55it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.873  
                                                              train/f1: 0.887   
                                                              train/precision:  
                                                              0.797             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              nan               
[2024-06-05 18:11:36,219][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.0/2>
[2024-06-05 18:11:36,219][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:11:36,222][HYDRA] 	#283 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 18:11:36,427][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:11:36,428][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:11:36,430][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:11:36,430][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:11:36,433][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:11:36,433][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:11:36,434][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:11:36,434][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:11:36,435][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:11:36,436][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:11:36,436][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:11:36,437][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:11:36,475][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.0/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 48.88it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.937  
                                                              train/f1: 0.940   
                                                              train/precision:  
                                                              0.887             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              nan               
[2024-06-05 18:11:51,615][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.0/3>
[2024-06-05 18:11:51,616][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:11:51,625][HYDRA] 	#284 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 18:11:51,838][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:11:51,839][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:11:51,841][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:11:51,841][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:11:51,843][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:11:51,843][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:11:51,844][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:11:51,844][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:11:51,845][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:11:51,848][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:11:51,848][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:11:51,850][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:11:51,957][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.0/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.88it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.800 val/mre: nan
                                                              train/auc: 0.849  
                                                              train/f1: 0.835   
                                                              train/precision:  
                                                              0.923             
                                                              train/recall:     
                                                              0.762 train/mre:  
                                                              nan               
[2024-06-05 18:12:06,667][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.0/4>
[2024-06-05 18:12:06,668][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:12:06,671][HYDRA] 	#285 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 18:12:06,887][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:12:06,888][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:12:06,889][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:12:06,890][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:12:06,892][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:12:06,892][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:12:06,893][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:12:06,893][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:12:06,894][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:12:06,895][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:12:06,895][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:12:06,897][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:12:06,940][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.0/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.83it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre: nan
                                                              train/auc: 0.889  
                                                              train/f1: 0.891   
                                                              train/precision:  
                                                              0.877             
                                                              train/recall:     
                                                              0.905 train/mre:  
                                                              nan               
[2024-06-05 18:12:21,464][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.0/5>
[2024-06-05 18:12:21,465][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:12:21,474][HYDRA] 	#286 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 18:12:21,685][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:12:21,686][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:12:21,687][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:12:21,688][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:12:21,690][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:12:21,690][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:12:21,691][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:12:21,691][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:12:21,692][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:12:21,693][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:12:21,693][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:12:21,694][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:12:21,740][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.0/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.84it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre: nan
                                                              train/auc: 0.881  
                                                              train/f1: 0.880   
                                                              train/precision:  
                                                              0.887             
                                                              train/recall:     
                                                              0.873 train/mre:  
                                                              nan               
[2024-06-05 18:12:36,634][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.0/6>
[2024-06-05 18:12:36,634][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:12:36,638][HYDRA] 	#287 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 18:12:36,839][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:12:36,840][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:12:36,842][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:12:36,842][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:12:36,844][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:12:36,844][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:12:36,845][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:12:36,845][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:12:36,846][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:12:36,847][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:12:36,847][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:12:36,848][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:12:36,890][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.0/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.33it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.913  
                                                              train/f1: 0.913   
                                                              train/precision:  
                                                              0.906             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              nan               
[2024-06-05 18:12:52,109][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.0/7>
[2024-06-05 18:12:52,110][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:12:52,115][HYDRA] 	#288 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 18:12:52,334][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:12:52,335][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:12:52,337][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:12:52,337][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:12:52,339][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:12:52,340][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:12:52,340][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:12:52,341][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:12:52,342][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:12:52,344][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:12:52,344][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:12:52,346][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:12:52,479][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.0/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 58.04it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.889  
                                                              train/f1: 0.896   
                                                              train/precision:  
                                                              0.845             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              nan               
[2024-06-05 18:13:08,222][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.0/8>
[2024-06-05 18:13:08,223][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:13:08,226][HYDRA] 	#289 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 18:13:08,432][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:13:08,434][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:13:08,435][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:13:08,435][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:13:08,438][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:13:08,438][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:13:08,439][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:13:08,439][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:13:08,440][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:13:08,441][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:13:08,441][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:13:08,442][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:13:08,480][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.0/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.23it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.921  
                                                              train/f1: 0.921   
                                                              train/precision:  
                                                              0.921             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              nan               
[2024-06-05 18:13:23,785][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.0/9>
[2024-06-05 18:13:23,787][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:13:23,791][HYDRA] 	#290 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 18:13:23,984][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:13:23,985][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:13:23,986][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:13:23,987][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:13:23,989][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:13:23,989][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:13:23,990][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:13:23,990][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:13:23,991][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:13:23,992][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:13:23,992][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:13:23,994][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:13:24,031][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.05/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 51.51it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.400 val/mre:    
                                                              0.047 train/auc:  
                                                              0.976 train/f1:   
                                                              0.976             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.066             
[2024-06-05 18:13:38,967][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.05/0>
[2024-06-05 18:13:38,967][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:13:38,970][HYDRA] 	#291 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 18:13:39,159][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:13:39,161][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:13:39,162][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:13:39,163][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:13:39,165][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:13:39,165][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:13:39,166][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:13:39,166][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:13:39,167][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:13:39,168][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:13:39,168][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:13:39,170][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:13:39,202][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.05/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 58.86it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.600 val/mre:    
                                                              0.050 train/auc:  
                                                              0.913 train/f1:   
                                                              0.916             
                                                              train/precision:  
                                                              0.882             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.066             
[2024-06-05 18:13:54,078][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.05/1>
[2024-06-05 18:13:54,079][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:13:54,080][HYDRA] 	#292 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 18:13:54,278][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:13:54,280][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:13:54,281][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:13:54,281][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:13:54,283][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:13:54,284][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:13:54,284][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:13:54,285][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:13:54,286][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:13:54,288][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:13:54,288][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:13:54,290][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:13:54,395][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.05/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.15it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.041 train/auc:  
                                                              0.929 train/f1:   
                                                              0.931             
                                                              train/precision:  
                                                              0.897             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.062             
[2024-06-05 18:14:08,960][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.05/2>
[2024-06-05 18:14:08,960][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:14:08,963][HYDRA] 	#293 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 18:14:09,169][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:14:09,171][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:14:09,173][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:14:09,173][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:14:09,180][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:14:09,181][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:14:09,181][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:14:09,182][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:14:09,183][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:14:09,183][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:14:09,183][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:14:09,185][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:14:09,225][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.05/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 53.71it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.200 val/mre:    
                                                              0.043 train/auc:  
                                                              0.929 train/f1:   
                                                              0.931             
                                                              train/precision:  
                                                              0.897             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.063             
[2024-06-05 18:14:24,293][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.05/3>
[2024-06-05 18:14:24,294][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:14:24,298][HYDRA] 	#294 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 18:14:24,502][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:14:24,503][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:14:24,505][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:14:24,505][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:14:24,508][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:14:24,508][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:14:24,508][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:14:24,509][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:14:24,510][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:14:24,511][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:14:24,511][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:14:24,512][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:14:24,551][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.05/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 53.98it/s v_num: 0.000      
                                                              val/auc: 0.750    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.800 val/mre:    
                                                              0.045 train/auc:  
                                                              0.929 train/f1:   
                                                              0.929             
                                                              train/precision:  
                                                              0.922             
                                                              train/recall:     
                                                              0.937 train/mre:  
                                                              0.066             
[2024-06-05 18:14:39,652][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.05/4>
[2024-06-05 18:14:39,652][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:14:39,655][HYDRA] 	#295 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 18:14:39,856][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:14:39,857][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:14:39,859][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:14:39,859][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:14:39,861][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:14:39,861][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:14:39,862][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:14:39,862][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:14:39,863][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:14:39,864][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:14:39,864][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:14:39,865][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:14:39,896][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.05/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 56.07it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.055 train/auc:  
                                                              0.865 train/f1:   
                                                              0.876             
                                                              train/precision:  
                                                              0.811             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.073             
[2024-06-05 18:14:55,249][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.05/5>
[2024-06-05 18:14:55,249][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:14:55,251][HYDRA] 	#296 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 18:14:55,467][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:14:55,469][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:14:55,470][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:14:55,470][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:14:55,473][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:14:55,474][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:14:55,474][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:14:55,475][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:14:55,476][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:14:55,479][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:14:55,479][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:14:55,480][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:14:55,581][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.05/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 57.94it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.600 val/mre:    
                                                              0.065 train/auc:  
                                                              0.905 train/f1:   
                                                              0.912             
                                                              train/precision:  
                                                              0.849             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.068             
[2024-06-05 18:15:10,604][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.05/6>
[2024-06-05 18:15:10,606][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:15:10,612][HYDRA] 	#297 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 18:15:10,833][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:15:10,834][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:15:10,835][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:15:10,836][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:15:10,838][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:15:10,838][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:15:10,839][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:15:10,839][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:15:10,840][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:15:10,841][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:15:10,841][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:15:10,842][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:15:10,877][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.05/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 57.33it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.045 train/auc:  
                                                              0.905 train/f1:   
                                                              0.906             
                                                              train/precision:  
                                                              0.892             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              0.059             
[2024-06-05 18:15:25,917][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.05/7>
[2024-06-05 18:15:25,917][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:15:25,919][HYDRA] 	#298 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 18:15:26,121][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:15:26,122][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:15:26,124][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:15:26,124][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:15:26,126][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:15:26,127][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:15:26,127][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:15:26,128][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:15:26,129][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:15:26,129][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:15:26,130][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:15:26,131][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:15:26,178][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.05/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.45it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.048 train/auc:  
                                                              0.921 train/f1:   
                                                              0.924             
                                                              train/precision:  
                                                              0.884             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.063             
[2024-06-05 18:15:40,679][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.05/8>
[2024-06-05 18:15:40,679][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:15:40,681][HYDRA] 	#299 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 18:15:40,872][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:15:40,874][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:15:40,875][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:15:40,876][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:15:40,878][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:15:40,878][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:15:40,878][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:15:40,879][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:15:40,880][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:15:40,881][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:15:40,881][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:15:40,882][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:15:40,920][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.05/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.25it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.040 train/auc:  
                                                              0.944 train/f1:   
                                                              0.946             
                                                              train/precision:  
                                                              0.924             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.063             
[2024-06-05 18:15:55,458][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.05/9>
[2024-06-05 18:15:55,458][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:15:55,462][HYDRA] 	#300 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 18:15:55,669][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:15:55,671][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:15:55,672][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:15:55,672][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:15:55,675][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:15:55,675][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:15:55,676][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:15:55,676][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:15:55,677][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:15:55,680][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:15:55,680][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:15:55,681][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:15:55,793][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.1/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.23it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.043 train/auc:  
                                                              0.984 train/f1:   
                                                              0.984             
                                                              train/precision:  
                                                              0.969             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.062             
[2024-06-05 18:16:11,995][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.1/0>
[2024-06-05 18:16:11,995][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:16:11,998][HYDRA] 	#301 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 18:16:12,204][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:16:12,205][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:16:12,207][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:16:12,207][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:16:12,209][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:16:12,209][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:16:12,210][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:16:12,210][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:16:12,211][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:16:12,212][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:16:12,212][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:16:12,214][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:16:12,255][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.1/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.31it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.045 train/auc:  
                                                              0.976 train/f1:   
                                                              0.977             
                                                              train/precision:  
                                                              0.955             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.062             
[2024-06-05 18:16:27,690][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.1/1>
[2024-06-05 18:16:27,691][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:16:27,696][HYDRA] 	#302 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 18:16:27,899][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:16:27,901][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:16:27,902][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:16:27,902][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:16:27,905][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:16:27,905][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:16:27,906][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:16:27,906][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:16:27,907][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:16:27,908][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:16:27,908][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:16:27,909][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:16:27,949][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.1/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.50it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.043 train/auc:  
                                                              0.968 train/f1:   
                                                              0.969             
                                                              train/precision:  
                                                              0.954             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.064             
[2024-06-05 18:16:43,284][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.1/2>
[2024-06-05 18:16:43,286][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:16:43,289][HYDRA] 	#303 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 18:16:43,490][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:16:43,492][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:16:43,493][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:16:43,494][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:16:43,496][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:16:43,496][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:16:43,497][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:16:43,497][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:16:43,498][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:16:43,499][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:16:43,499][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:16:43,500][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:16:43,541][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.1/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.88it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.600 val/mre:    
                                                              0.040 train/auc:  
                                                              0.937 train/f1:   
                                                              0.939             
                                                              train/precision:  
                                                              0.899             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.059             
[2024-06-05 18:16:58,237][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.1/3>
[2024-06-05 18:16:58,238][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:16:58,242][HYDRA] 	#304 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 18:16:58,447][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:16:58,448][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:16:58,449][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:16:58,450][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:16:58,453][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:16:58,454][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:16:58,454][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:16:58,454][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:16:58,455][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:16:58,458][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:16:58,459][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:16:58,460][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:16:58,557][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.1/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.26it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.600 val/mre:    
                                                              0.047 train/auc:  
                                                              0.937 train/f1:   
                                                              0.932             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.873 train/mre:  
                                                              0.065             
[2024-06-05 18:17:13,111][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.1/4>
[2024-06-05 18:17:13,112][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:17:13,117][HYDRA] 	#305 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 18:17:13,319][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:17:13,320][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:17:13,322][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:17:13,322][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:17:13,324][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:17:13,324][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:17:13,325][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:17:13,325][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:17:13,326][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:17:13,327][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:17:13,327][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:17:13,328][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:17:13,370][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.1/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.52it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.055 train/auc:  
                                                              0.929 train/f1:   
                                                              0.928             
                                                              train/precision:  
                                                              0.935             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              0.068             
[2024-06-05 18:17:28,088][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.1/5>
[2024-06-05 18:17:28,089][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:17:28,093][HYDRA] 	#306 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 18:17:28,294][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:17:28,296][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:17:28,297][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:17:28,298][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:17:28,300][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:17:28,301][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:17:28,301][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:17:28,302][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:17:28,303][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:17:28,303][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:17:28,304][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:17:28,305][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:17:28,352][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.1/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.80it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.600 val/mre:    
                                                              0.042 train/auc:  
                                                              0.889 train/f1:   
                                                              0.889             
                                                              train/precision:  
                                                              0.889             
                                                              train/recall:     
                                                              0.889 train/mre:  
                                                              0.067             
[2024-06-05 18:17:43,203][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.1/6>
[2024-06-05 18:17:43,204][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:17:43,207][HYDRA] 	#307 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 18:17:43,413][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:17:43,414][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:17:43,416][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:17:43,416][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:17:43,419][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:17:43,419][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:17:43,419][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:17:43,420][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:17:43,421][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:17:43,421][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:17:43,422][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:17:43,423][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:17:43,457][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.1/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.93it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.200 val/mre:    
                                                              0.042 train/auc:  
                                                              0.944 train/f1:   
                                                              0.946             
                                                              train/precision:  
                                                              0.924             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.060             
[2024-06-05 18:17:58,351][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.1/7>
[2024-06-05 18:17:58,352][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:17:58,356][HYDRA] 	#308 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 18:17:58,579][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:17:58,580][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:17:58,582][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:17:58,582][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:17:58,586][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:17:58,586][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:17:58,587][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:17:58,588][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:17:58,589][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:17:58,592][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:17:58,592][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:17:58,593][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:17:58,719][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.1/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.50it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.600 val/mre:    
                                                              0.049 train/auc:  
                                                              0.833 train/f1:   
                                                              0.855             
                                                              train/precision:  
                                                              0.756             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.061             
[2024-06-05 18:18:14,346][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.1/8>
[2024-06-05 18:18:14,347][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:18:14,349][HYDRA] 	#309 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 18:18:14,555][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:18:14,556][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:18:14,557][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:18:14,558][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:18:14,560][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:18:14,560][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:18:14,561][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:18:14,561][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:18:14,562][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:18:14,563][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:18:14,563][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:18:14,564][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:18:14,620][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.1/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.55it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.053 train/auc:  
                                                              0.913 train/f1:   
                                                              0.911             
                                                              train/precision:  
                                                              0.933             
                                                              train/recall:     
                                                              0.889 train/mre:  
                                                              0.074             
[2024-06-05 18:18:29,255][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.1/9>
[2024-06-05 18:18:29,255][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:18:29,259][HYDRA] 	#310 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 18:18:29,460][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:18:29,461][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:18:29,463][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:18:29,463][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:18:29,465][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:18:29,465][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:18:29,466][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:18:29,466][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:18:29,467][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:18:29,468][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:18:29,468][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:18:29,470][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:18:29,505][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.15/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 56.93it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre:    
                                                              0.041 train/auc:  
                                                              0.960 train/f1:   
                                                              0.961             
                                                              train/precision:  
                                                              0.953             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.061             
[2024-06-05 18:18:44,621][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.15/0>
[2024-06-05 18:18:44,622][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:18:44,626][HYDRA] 	#311 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 18:18:44,818][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:18:44,820][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:18:44,821][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:18:44,821][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:18:44,824][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:18:44,824][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:18:44,825][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:18:44,825][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:18:44,826][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:18:44,827][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:18:44,827][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:18:44,828][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:18:44,863][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.15/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.70it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.600 val/mre:    
                                                              0.050 train/auc:  
                                                              0.944 train/f1:   
                                                              0.944             
                                                              train/precision:  
                                                              0.952             
                                                              train/recall:     
                                                              0.937 train/mre:  
                                                              0.063             
[2024-06-05 18:18:59,755][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.15/1>
[2024-06-05 18:18:59,755][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:18:59,759][HYDRA] 	#312 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 18:18:59,976][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:18:59,978][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:18:59,979][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:18:59,979][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:18:59,982][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:18:59,982][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:18:59,983][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:18:59,983][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:18:59,984][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:18:59,987][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:18:59,987][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:18:59,988][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:19:00,092][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.15/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.32it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.200 val/mre:    
                                                              0.052 train/auc:  
                                                              0.897 train/f1:   
                                                              0.905             
                                                              train/precision:  
                                                              0.838             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.066             
[2024-06-05 18:19:14,860][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.15/2>
[2024-06-05 18:19:14,860][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:19:14,863][HYDRA] 	#313 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 18:19:15,069][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:19:15,070][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:19:15,072][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:19:15,072][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:19:15,074][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:19:15,074][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:19:15,075][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:19:15,075][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:19:15,076][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:19:15,077][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:19:15,077][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:19:15,078][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:19:15,115][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.15/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 56.12it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.038 train/auc:  
                                                              0.937 train/f1:   
                                                              0.940             
                                                              train/precision:  
                                                              0.887             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.062             
[2024-06-05 18:19:30,427][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.15/3>
[2024-06-05 18:19:30,428][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:19:30,430][HYDRA] 	#314 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 18:19:30,623][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:19:30,624][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:19:30,625][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:19:30,626][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:19:30,628][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:19:30,629][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:19:30,629][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:19:30,630][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:19:30,631][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:19:30,632][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:19:30,632][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:19:30,633][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:19:30,678][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.15/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.26it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.042 train/auc:  
                                                              0.968 train/f1:   
                                                              0.968             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.064             
[2024-06-05 18:19:45,582][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.15/4>
[2024-06-05 18:19:45,584][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:19:45,586][HYDRA] 	#315 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 18:19:45,785][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:19:45,786][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:19:45,788][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:19:45,788][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:19:45,790][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:19:45,790][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:19:45,791][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:19:45,791][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:19:45,792][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:19:45,793][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:19:45,793][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:19:45,794][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:19:45,828][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.15/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 58.60it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.400 val/mre:    
                                                              0.040 train/auc:  
                                                              0.968 train/f1:   
                                                              0.968             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.061             
[2024-06-05 18:20:00,649][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.15/5>
[2024-06-05 18:20:00,650][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:20:00,653][HYDRA] 	#316 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 18:20:00,856][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:20:00,857][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:20:00,859][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:20:00,859][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:20:00,861][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:20:00,861][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:20:00,862][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:20:00,862][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:20:00,863][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:20:00,866][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:20:00,866][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:20:00,867][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:20:00,977][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.15/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.85it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.054 train/auc:  
                                                              0.937 train/f1:   
                                                              0.937             
                                                              train/precision:  
                                                              0.937             
                                                              train/recall:     
                                                              0.937 train/mre:  
                                                              0.070             
[2024-06-05 18:20:15,779][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.15/6>
[2024-06-05 18:20:15,780][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:20:15,783][HYDRA] 	#317 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 18:20:15,985][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:20:15,986][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:20:15,988][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:20:15,988][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:20:15,990][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:20:15,991][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:20:15,991][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:20:15,992][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:20:15,993][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:20:15,993][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:20:15,993][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:20:15,995][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:20:16,039][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.15/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.22it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.600 val/mre:    
                                                              0.051 train/auc:  
                                                              0.873 train/f1:   
                                                              0.887             
                                                              train/precision:  
                                                              0.797             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.064             
[2024-06-05 18:20:30,699][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.15/7>
[2024-06-05 18:20:30,700][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:20:30,702][HYDRA] 	#318 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 18:20:30,901][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:20:30,902][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:20:30,904][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:20:30,904][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:20:30,907][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:20:30,907][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:20:30,908][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:20:30,908][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:20:30,909][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:20:30,910][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:20:30,910][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:20:30,912][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:20:30,950][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.15/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.98it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.800 val/mre:    
                                                              0.038 train/auc:  
                                                              0.976 train/f1:   
                                                              0.976             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.057             
[2024-06-05 18:20:45,562][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.15/8>
[2024-06-05 18:20:45,563][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:20:45,565][HYDRA] 	#319 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 18:20:45,759][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:20:45,760][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:20:45,761][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:20:45,762][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:20:45,764][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:20:45,764][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:20:45,765][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:20:45,765][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:20:45,766][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:20:45,767][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:20:45,767][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:20:45,768][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:20:45,812][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.15/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.08it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.800 val/mre:    
                                                              0.041 train/auc:  
                                                              0.944 train/f1:   
                                                              0.943             
                                                              train/precision:  
                                                              0.967             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              0.065             
[2024-06-05 18:21:00,677][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.15/9>
[2024-06-05 18:21:00,678][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:21:00,680][HYDRA] 	#320 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 18:21:00,890][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:21:00,891][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:21:00,892][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:21:00,893][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:21:00,895][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:21:00,895][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:21:00,896][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:21:00,896][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:21:00,897][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:21:00,900][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:21:00,900][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:21:00,901][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:21:00,993][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.2/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.44it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.049 train/auc:  
                                                              0.937 train/f1:   
                                                              0.935             
                                                              train/precision:  
                                                              0.951             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              0.062             
[2024-06-05 18:21:15,977][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.2/0>
[2024-06-05 18:21:15,978][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:21:15,983][HYDRA] 	#321 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 18:21:16,204][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:21:16,205][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:21:16,207][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:21:16,207][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:21:16,209][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:21:16,210][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:21:16,210][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:21:16,211][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:21:16,212][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:21:16,212][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:21:16,212][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:21:16,214][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:21:16,259][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.2/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.30it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.600 val/mre:    
                                                              0.045 train/auc:  
                                                              0.976 train/f1:   
                                                              0.976             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.062             
[2024-06-05 18:21:31,357][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.2/1>
[2024-06-05 18:21:31,359][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:21:31,362][HYDRA] 	#322 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 18:21:31,565][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:21:31,566][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:21:31,567][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:21:31,568][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:21:31,570][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:21:31,571][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:21:31,571][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:21:31,572][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:21:31,573][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:21:31,573][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:21:31,574][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:21:31,575][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:21:31,613][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.2/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.51it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.200 val/mre:    
                                                              0.047 train/auc:  
                                                              0.937 train/f1:   
                                                              0.940             
                                                              train/precision:  
                                                              0.887             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.067             
[2024-06-05 18:21:46,347][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.2/2>
[2024-06-05 18:21:46,348][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:21:46,352][HYDRA] 	#323 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 18:21:46,554][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:21:46,555][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:21:46,557][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:21:46,557][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:21:46,560][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:21:46,560][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:21:46,561][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:21:46,561][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:21:46,562][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:21:46,563][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:21:46,563][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:21:46,564][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:21:46,602][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.2/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.50it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.039 train/auc:  
                                                              0.937 train/f1:   
                                                              0.937             
                                                              train/precision:  
                                                              0.937             
                                                              train/recall:     
                                                              0.937 train/mre:  
                                                              0.063             
[2024-06-05 18:22:01,266][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.2/3>
[2024-06-05 18:22:01,267][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:22:01,269][HYDRA] 	#324 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 18:22:01,477][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:22:01,478][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:22:01,480][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:22:01,480][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:22:01,482][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:22:01,482][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:22:01,483][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:22:01,483][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:22:01,484][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:22:01,487][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:22:01,488][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:22:01,490][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:22:01,593][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.2/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 56.51it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.800 val/mre:    
                                                              0.055 train/auc:  
                                                              0.937 train/f1:   
                                                              0.932             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.873 train/mre:  
                                                              0.069             
[2024-06-05 18:22:16,798][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.2/4>
[2024-06-05 18:22:16,800][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:22:16,802][HYDRA] 	#325 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 18:22:16,999][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:22:17,000][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:22:17,002][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:22:17,002][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:22:17,004][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:22:17,005][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:22:17,005][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:22:17,005][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:22:17,007][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:22:17,007][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:22:17,007][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:22:17,009][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:22:17,040][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.2/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.37it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.400 val/mre:    
                                                              0.041 train/auc:  
                                                              0.968 train/f1:   
                                                              0.968             
                                                              train/precision:  
                                                              0.968             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.059             
[2024-06-05 18:22:32,018][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.2/5>
[2024-06-05 18:22:32,019][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:22:32,022][HYDRA] 	#326 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 18:22:32,226][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:22:32,227][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:22:32,228][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:22:32,229][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:22:32,233][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:22:32,234][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:22:32,234][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:22:32,235][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:22:32,236][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:22:32,236][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:22:32,236][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:22:32,238][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:22:32,285][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.2/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.51it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.600 val/mre:    
                                                              0.057 train/auc:  
                                                              0.889 train/f1:   
                                                              0.889             
                                                              train/precision:  
                                                              0.889             
                                                              train/recall:     
                                                              0.889 train/mre:  
                                                              0.063             
[2024-06-05 18:22:47,318][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.2/6>
[2024-06-05 18:22:47,318][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:22:47,320][HYDRA] 	#327 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 18:22:47,516][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:22:47,517][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:22:47,519][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:22:47,519][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:22:47,521][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:22:47,522][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:22:47,522][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:22:47,522][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:22:47,523][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:22:47,524][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:22:47,524][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:22:47,526][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:22:47,564][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.2/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 57.74it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.200 val/mre:    
                                                              0.068 train/auc:  
                                                              0.937 train/f1:   
                                                              0.940             
                                                              train/precision:  
                                                              0.887             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.069             
[2024-06-05 18:23:03,415][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.2/7>
[2024-06-05 18:23:03,415][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:23:03,419][HYDRA] 	#328 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 18:23:03,636][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:23:03,637][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:23:03,639][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:23:03,639][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:23:03,641][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:23:03,642][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:23:03,642][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:23:03,642][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:23:03,643][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:23:03,646][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:23:03,646][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:23:03,648][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:23:03,789][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.2/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.85it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.600 val/mre:    
                                                              0.049 train/auc:  
                                                              0.849 train/f1:   
                                                              0.829             
                                                              train/precision:  
                                                              0.958             
                                                              train/recall:     
                                                              0.730 train/mre:  
                                                              0.059             
[2024-06-05 18:23:18,929][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.2/8>
[2024-06-05 18:23:18,930][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:23:18,934][HYDRA] 	#329 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 18:23:19,136][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:23:19,138][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:23:19,139][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:23:19,139][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:23:19,141][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:23:19,142][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:23:19,142][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:23:19,143][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:23:19,144][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:23:19,144][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:23:19,145][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:23:19,146][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:23:19,183][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.2/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.22it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.600 val/mre:    
                                                              0.040 train/auc:  
                                                              0.960 train/f1:   
                                                              0.961             
                                                              train/precision:  
                                                              0.953             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.064             
[2024-06-05 18:23:34,118][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.2/9>
[2024-06-05 18:23:34,119][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:23:34,121][HYDRA] 	#330 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 18:23:34,336][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:23:34,338][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:23:34,339][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:23:34,339][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:23:34,342][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:23:34,342][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:23:34,343][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:23:34,343][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:23:34,344][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:23:34,345][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:23:34,345][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:23:34,347][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2024-06-05 18:23:34,385][train.py][INFO] - Starting training...
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.25/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 56.77it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.043 train/auc:  
                                                              0.984 train/f1:   
                                                              0.984             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.059             
[2024-06-05 18:23:49,644][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.25/0>
[2024-06-05 18:23:49,645][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:23:49,650][HYDRA] 	#331 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 18:23:49,843][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:23:49,844][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:23:49,846][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:23:49,846][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:23:49,848][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:23:49,848][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:23:49,849][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:23:49,849][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:23:49,850][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:23:49,851][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:23:49,851][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:23:49,852][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:23:49,893][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.25/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.00it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.600 val/mre:    
                                                              0.040 train/auc:  
                                                              0.976 train/f1:   
                                                              0.976             
                                                              train/precision:  
                                                              0.969             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.063             
[2024-06-05 18:24:04,580][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.25/1>
[2024-06-05 18:24:04,581][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:24:04,586][HYDRA] 	#332 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 18:24:04,777][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:24:04,779][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:24:04,780][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:24:04,781][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:24:04,783][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:24:04,783][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:24:04,784][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:24:04,784][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:24:04,785][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:24:04,788][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:24:04,788][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:24:04,789][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:24:04,900][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.25/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 55.82it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.600 val/mre:    
                                                              0.045 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.062             
[2024-06-05 18:24:20,593][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.25/2>
[2024-06-05 18:24:20,595][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:24:20,597][HYDRA] 	#333 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 18:24:20,802][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:24:20,804][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:24:20,805][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:24:20,806][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:24:20,808][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:24:20,808][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:24:20,809][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:24:20,809][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:24:20,810][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:24:20,811][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:24:20,811][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:24:20,812][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:24:20,850][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.25/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 58.86it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.042 train/auc:  
                                                              1.000 train/f1:   
                                                              1.000             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.063             
[2024-06-05 18:24:35,823][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.25/3>
[2024-06-05 18:24:35,823][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:24:35,826][HYDRA] 	#334 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 18:24:36,022][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:24:36,024][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:24:36,025][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:24:36,025][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:24:36,028][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:24:36,028][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:24:36,029][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:24:36,029][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:24:36,030][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:24:36,031][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:24:36,031][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:24:36,032][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:24:36,070][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.25/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 56.68it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.600 val/mre:    
                                                              0.043 train/auc:  
                                                              0.968 train/f1:   
                                                              0.967             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.937 train/mre:  
                                                              0.065             
[2024-06-05 18:24:51,036][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.25/4>
[2024-06-05 18:24:51,036][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:24:51,040][HYDRA] 	#335 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 18:24:51,238][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:24:51,240][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:24:51,241][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:24:51,241][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:24:51,243][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:24:51,244][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:24:51,244][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:24:51,245][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:24:51,246][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:24:51,246][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:24:51,247][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:24:51,248][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:24:51,291][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.25/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.74it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.040 train/auc:  
                                                              0.968 train/f1:   
                                                              0.968             
                                                              train/precision:  
                                                              0.968             
                                                              train/recall:     
                                                              0.968 train/mre:  
                                                              0.060             
[2024-06-05 18:25:07,628][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.25/5>
[2024-06-05 18:25:07,629][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:25:07,631][HYDRA] 	#336 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 18:25:07,854][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:25:07,855][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:25:07,856][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:25:07,857][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:25:07,859][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:25:07,859][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:25:07,860][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:25:07,860][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:25:07,861][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:25:07,865][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:25:07,865][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:25:07,866][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:25:08,009][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.25/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.15it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.400 val/mre:    
                                                              0.044 train/auc:  
                                                              0.937 train/f1:   
                                                              0.938             
                                                              train/precision:  
                                                              0.923             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.065             
[2024-06-05 18:25:22,938][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.25/6>
[2024-06-05 18:25:22,939][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:25:22,942][HYDRA] 	#337 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 18:25:23,154][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:25:23,156][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:25:23,157][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:25:23,157][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:25:23,160][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:25:23,160][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:25:23,161][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:25:23,161][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:25:23,162][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:25:23,163][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:25:23,163][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:25:23,164][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:25:23,203][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.25/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 54.01it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.200 val/mre:    
                                                              0.046 train/auc:  
                                                              0.921 train/f1:   
                                                              0.921             
                                                              train/precision:  
                                                              0.921             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              0.061             
[2024-06-05 18:25:38,268][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.25/7>
[2024-06-05 18:25:38,268][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:25:38,272][HYDRA] 	#338 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 18:25:38,476][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:25:38,477][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:25:38,479][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:25:38,479][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:25:38,481][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:25:38,481][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:25:38,482][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:25:38,482][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:25:38,483][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:25:38,484][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:25:38,484][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:25:38,485][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:25:38,528][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.25/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 58.72it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.600 val/mre:    
                                                              0.043 train/auc:  
                                                              0.968 train/f1:   
                                                              0.969             
                                                              train/precision:  
                                                              0.954             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.057             
[2024-06-05 18:25:53,591][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.25/8>
[2024-06-05 18:25:53,592][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:25:53,595][HYDRA] 	#339 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 18:25:53,798][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:25:53,799][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:25:53,801][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:25:53,801][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:25:53,804][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:25:53,804][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:25:53,805][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:25:53,805][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:25:53,806][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:25:53,807][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:25:53,807][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:25:53,809][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:25:53,847][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.25/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 58.00it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.400 val/mre:    
                                                              0.047 train/auc:  
                                                              0.952 train/f1:   
                                                              0.950             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.905 train/mre:  
                                                              0.072             
[2024-06-05 18:26:08,973][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.25/9>
[2024-06-05 18:26:08,975][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:26:08,982][HYDRA] 	#340 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 18:26:09,195][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:26:09,196][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:26:09,198][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:26:09,198][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:26:09,200][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:26:09,201][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:26:09,202][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:26:09,202][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:26:09,203][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:26:09,206][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:26:09,206][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:26:09,208][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:26:09,310][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.3/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.71it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.046 train/auc:  
                                                              0.960 train/f1:   
                                                              0.960             
                                                              train/precision:  
                                                              0.968             
                                                              train/recall:     
                                                              0.952 train/mre:  
                                                              0.064             
[2024-06-05 18:26:24,417][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.3/0>
[2024-06-05 18:26:24,417][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:26:24,420][HYDRA] 	#341 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 18:26:24,633][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:26:24,635][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:26:24,636][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:26:24,636][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:26:24,639][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:26:24,639][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:26:24,639][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:26:24,640][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:26:24,641][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:26:24,642][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:26:24,642][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:26:24,643][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:26:24,682][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.3/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.49it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.200 val/recall: 
                                                              0.200 val/mre:    
                                                              0.040 train/auc:  
                                                              0.952 train/f1:   
                                                              0.951             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              0.061             
[2024-06-05 18:26:39,328][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.3/1>
[2024-06-05 18:26:39,328][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:26:39,332][HYDRA] 	#342 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 18:26:39,537][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:26:39,539][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:26:39,540][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:26:39,540][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:26:39,545][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:26:39,545][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:26:39,546][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:26:39,546][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:26:39,547][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:26:39,548][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:26:39,548][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:26:39,550][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:26:39,600][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.3/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 52.15it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.600 val/mre:    
                                                              0.046 train/auc:  
                                                              0.952 train/f1:   
                                                              0.952             
                                                              train/precision:  
                                                              0.967             
                                                              train/recall:     
                                                              0.937 train/mre:  
                                                              0.063             
[2024-06-05 18:26:54,756][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.3/2>
[2024-06-05 18:26:54,756][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:26:54,758][HYDRA] 	#343 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 18:26:54,958][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:26:54,960][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:26:54,961][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:26:54,961][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:26:54,964][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:26:54,964][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:26:54,965][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:26:54,965][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:26:54,966][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:26:54,967][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:26:54,967][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:26:54,968][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:26:55,005][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.3/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.74it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.400 val/mre:    
                                                              0.039 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.062             
[2024-06-05 18:27:09,854][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.3/3>
[2024-06-05 18:27:09,855][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:27:09,860][HYDRA] 	#344 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 18:27:10,072][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:27:10,073][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:27:10,074][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:27:10,075][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:27:10,077][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:27:10,077][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:27:10,078][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:27:10,078][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:27:10,079][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:27:10,082][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:27:10,082][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:27:10,083][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:27:10,198][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.3/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.52it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.600 val/mre:    
                                                              0.044 train/auc:  
                                                              0.952 train/f1:   
                                                              0.951             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              0.065             
[2024-06-05 18:27:25,162][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.3/4>
[2024-06-05 18:27:25,164][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:27:25,166][HYDRA] 	#345 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 18:27:25,375][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:27:25,377][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:27:25,379][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:27:25,379][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:27:25,381][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:27:25,382][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:27:25,382][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:27:25,383][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:27:25,384][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:27:25,384][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:27:25,385][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:27:25,386][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:27:25,426][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.3/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.15it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.400 val/mre:    
                                                              0.046 train/auc:  
                                                              0.929 train/f1:   
                                                              0.932             
                                                              train/precision:  
                                                              0.886             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.065             
[2024-06-05 18:27:40,439][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.3/5>
[2024-06-05 18:27:40,439][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:27:40,441][HYDRA] 	#346 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 18:27:40,636][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:27:40,637][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:27:40,639][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:27:40,639][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:27:40,641][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:27:40,641][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:27:40,642][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:27:40,642][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:27:40,644][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:27:40,644][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:27:40,644][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:27:40,645][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:27:40,682][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.3/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 54.20it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.042 train/auc:  
                                                              0.952 train/f1:   
                                                              0.951             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              0.059             
[2024-06-05 18:27:55,649][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.3/6>
[2024-06-05 18:27:55,650][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:27:55,655][HYDRA] 	#347 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 18:27:55,853][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:27:55,854][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:27:55,856][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:27:55,856][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:27:55,858][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:27:55,858][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:27:55,859][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:27:55,859][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:27:55,860][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:27:55,861][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:27:55,861][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:27:55,862][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:27:55,898][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.3/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 53.85it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.400 val/mre:    
                                                              0.049 train/auc:  
                                                              0.952 train/f1:   
                                                              0.951             
                                                              train/precision:  
                                                              0.983             
                                                              train/recall:     
                                                              0.921 train/mre:  
                                                              0.064             
[2024-06-05 18:28:10,955][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.3/7>
[2024-06-05 18:28:10,955][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:28:10,959][HYDRA] 	#348 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 18:28:11,158][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:28:11,159][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:28:11,160][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:28:11,161][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:28:11,163][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:28:11,164][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:28:11,164][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:28:11,165][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:28:11,166][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:28:11,169][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:28:11,169][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:28:11,171][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:28:11,265][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.3/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.02it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.600 val/mre:    
                                                              0.037 train/auc:  
                                                              0.984 train/f1:   
                                                              0.984             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.058             
[2024-06-05 18:28:26,107][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.3/8>
[2024-06-05 18:28:26,108][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:28:26,111][HYDRA] 	#349 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_depression data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 18:28:26,318][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:28:26,320][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:28:26,321][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:28:26,322][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:28:26,324][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:28:26,324][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:28:26,325][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:28:26,325][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:28:26,326][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:28:26,327][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:28:26,327][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:28:26,328][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:28:26,365][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.3/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.43it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.400 val/mre:    
                                                              0.040 train/auc:  
                                                              0.992 train/f1:   
                                                              0.992             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.062             
[2024-06-05 18:28:40,977][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_depression/0.3/9>
[2024-06-05 18:28:40,978][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:28:40,980][HYDRA] 	#350 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 18:28:41,188][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:28:41,190][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:28:41,191][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:28:41,191][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:28:41,194][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:28:41,195][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:28:41,195][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:28:41,195][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:28:41,197][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:28:41,197][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:28:41,197][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:28:41,199][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:28:41,247][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.0/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 70.00it/s v_num: 0.000      
                                                              val/auc: 0.690    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.714 val/mre: nan
                                                              train/auc: 0.825  
                                                              train/f1: 0.846   
                                                              train/precision:  
                                                              0.753             
                                                              train/recall:     
                                                              0.965 train/mre:  
                                                              nan               
[2024-06-05 18:28:56,889][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.0/0>
[2024-06-05 18:28:56,890][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:28:56,892][HYDRA] 	#351 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 18:28:57,092][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:28:57,093][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:28:57,095][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:28:57,095][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:28:57,097][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:28:57,098][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:28:57,098][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:28:57,099][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:28:57,100][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:28:57,100][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:28:57,100][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:28:57,102][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:28:57,140][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.0/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.71it/s v_num: 0.000      
                                                              val/auc: 0.460    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.143 val/mre: nan
                                                              train/auc: 0.930  
                                                              train/f1: 0.931   
                                                              train/precision:  
                                                              0.915             
                                                              train/recall:     
                                                              0.947 train/mre:  
                                                              nan               
[2024-06-05 18:29:11,516][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.0/1>
[2024-06-05 18:29:11,520][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:29:11,525][HYDRA] 	#352 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 18:29:11,723][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:29:11,724][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:29:11,725][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:29:11,726][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:29:11,728][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:29:11,729][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:29:11,729][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:29:11,730][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:29:11,731][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:29:11,733][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:29:11,733][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:29:11,735][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:29:11,837][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.0/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 67.74it/s v_num: 0.000      
                                                              val/auc: 0.619    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.571 val/mre: nan
                                                              train/auc: 0.772  
                                                              train/f1: 0.768   
                                                              train/precision:  
                                                              0.782             
                                                              train/recall:     
                                                              0.754 train/mre:  
                                                              nan               
[2024-06-05 18:29:25,586][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.0/2>
[2024-06-05 18:29:25,586][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:29:25,589][HYDRA] 	#353 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 18:29:25,784][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:29:25,785][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:29:25,786][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:29:25,787][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:29:25,789][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:29:25,789][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:29:25,790][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:29:25,790][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:29:25,791][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:29:25,792][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:29:25,792][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:29:25,794][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:29:25,827][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.0/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.84it/s v_num: 0.000      
                                                              val/auc: 0.563    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.571 val/mre: nan
                                                              train/auc: 0.816  
                                                              train/f1: 0.788   
                                                              train/precision:  
                                                              0.929             
                                                              train/recall:     
                                                              0.684 train/mre:  
                                                              nan               
[2024-06-05 18:29:39,794][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.0/3>
[2024-06-05 18:29:39,795][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:29:39,798][HYDRA] 	#354 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 18:29:39,994][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:29:39,996][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:29:39,997][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:29:39,997][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:29:39,999][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:29:40,000][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:29:40,000][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:29:40,001][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:29:40,002][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:29:40,002][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:29:40,002][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:29:40,004][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:29:40,046][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.0/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 58.64it/s v_num: 0.000      
                                                              val/auc: 0.365    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.286 val/mre: nan
                                                              train/auc: 0.851  
                                                              train/f1: 0.860   
                                                              train/precision:  
                                                              0.812             
                                                              train/recall:     
                                                              0.912 train/mre:  
                                                              nan               
[2024-06-05 18:29:53,951][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.0/4>
[2024-06-05 18:29:53,952][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:29:53,955][HYDRA] 	#355 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 18:29:54,167][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:29:54,168][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:29:54,170][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:29:54,170][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:29:54,172][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:29:54,172][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:29:54,173][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:29:54,173][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:29:54,174][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:29:54,175][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:29:54,175][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:29:54,176][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:29:54,215][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.0/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 57.70it/s v_num: 0.000      
                                                              val/auc: 0.476    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.286 val/mre: nan
                                                              train/auc: 0.737  
                                                              train/f1: 0.786   
                                                              train/precision:  
                                                              0.663             
                                                              train/recall:     
                                                              0.965 train/mre:  
                                                              nan               
[2024-06-05 18:30:08,166][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.0/5>
[2024-06-05 18:30:08,167][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:30:08,170][HYDRA] 	#356 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 18:30:08,371][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:30:08,372][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:30:08,374][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:30:08,374][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:30:08,376][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:30:08,377][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:30:08,377][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:30:08,378][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:30:08,379][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:30:08,379][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:30:08,380][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:30:08,381][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:30:08,418][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.0/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 66.59it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.518  
                                                              train/f1: 0.098   
                                                              train/precision:  
                                                              0.750             
                                                              train/recall:     
                                                              0.053 train/mre:  
                                                              nan               
[2024-06-05 18:30:22,278][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.0/6>
[2024-06-05 18:30:22,279][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:30:22,280][HYDRA] 	#357 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 18:30:22,487][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:30:22,488][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:30:22,490][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:30:22,490][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:30:22,492][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:30:22,493][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:30:22,493][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:30:22,494][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:30:22,495][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:30:22,497][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:30:22,497][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:30:22,499][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:30:22,593][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.0/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 66.31it/s v_num: 0.000      
                                                              val/auc: 0.675    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.571 val/mre: nan
                                                              train/auc: 0.737  
                                                              train/f1: 0.712   
                                                              train/precision:  
                                                              0.787             
                                                              train/recall:     
                                                              0.649 train/mre:  
                                                              nan               
[2024-06-05 18:30:36,671][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.0/7>
[2024-06-05 18:30:36,671][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:30:36,675][HYDRA] 	#358 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 18:30:36,879][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:30:36,880][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:30:36,881][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:30:36,882][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:30:36,884][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:30:36,884][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:30:36,885][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:30:36,886][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:30:36,887][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:30:36,887][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:30:36,887][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:30:36,889][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:30:36,938][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.0/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.59it/s v_num: 0.000      
                                                              val/auc: 0.619    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.571 val/mre: nan
                                                              train/auc: 0.789  
                                                              train/f1: 0.793   
                                                              train/precision:  
                                                              0.780             
                                                              train/recall:     
                                                              0.807 train/mre:  
                                                              nan               
[2024-06-05 18:30:51,003][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.0/8>
[2024-06-05 18:30:51,004][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:30:51,007][HYDRA] 	#359 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 18:30:51,226][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:30:51,228][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:30:51,229][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:30:51,229][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:30:51,231][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:30:51,232][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:30:51,232][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:30:51,233][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:30:51,234][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:30:51,234][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:30:51,235][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:30:51,236][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:30:51,277][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.0/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.00it/s v_num: 0.000      
                                                              val/auc: 0.659    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.429 val/mre: nan
                                                              train/auc: 0.763  
                                                              train/f1: 0.761   
                                                              train/precision:  
                                                              0.768             
                                                              train/recall:     
                                                              0.754 train/mre:  
                                                              nan               
[2024-06-05 18:31:05,082][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.0/9>
[2024-06-05 18:31:05,083][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:31:05,085][HYDRA] 	#360 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 18:31:05,297][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:31:05,299][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:31:05,300][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:31:05,301][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:31:05,303][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:31:05,303][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:31:05,304][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:31:05,304][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:31:05,305][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:31:05,306][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:31:05,306][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:31:05,307][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:31:05,355][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.05/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 68.50it/s v_num: 0.000      
                                                              val/auc: 0.619    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.571 val/mre:    
                                                              0.038 train/auc:  
                                                              0.789 train/f1:   
                                                              0.800             
                                                              train/precision:  
                                                              0.762             
                                                              train/recall:     
                                                              0.842 train/mre:  
                                                              0.042             
[2024-06-05 18:31:19,247][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.05/0>
[2024-06-05 18:31:19,247][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:31:19,255][HYDRA] 	#361 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 18:31:19,466][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:31:19,468][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:31:19,469][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:31:19,469][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:31:19,471][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:31:19,472][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:31:19,472][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:31:19,473][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:31:19,474][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:31:19,474][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:31:19,474][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:31:19,476][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:31:19,520][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.05/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 71.85it/s v_num: 0.000      
                                                              val/auc: 0.690    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.714 val/mre:    
                                                              0.055 train/auc:  
                                                              0.825 train/f1:   
                                                              0.811             
                                                              train/precision:  
                                                              0.878             
                                                              train/recall:     
                                                              0.754 train/mre:  
                                                              0.064             
[2024-06-05 18:31:33,792][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.05/1>
[2024-06-05 18:31:33,793][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:31:33,797][HYDRA] 	#362 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 18:31:34,006][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:31:34,008][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:31:34,009][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:31:34,009][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:31:34,011][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:31:34,012][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:31:34,012][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:31:34,013][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:31:34,014][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:31:34,016][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:31:34,017][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:31:34,018][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:31:34,136][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.05/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 68.88it/s v_num: 0.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.035 train/auc:  
                                                              0.728 train/f1:   
                                                              0.760             
                                                              train/precision:  
                                                              0.681             
                                                              train/recall:     
                                                              0.860 train/mre:  
                                                              0.043             
[2024-06-05 18:31:48,047][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.05/2>
[2024-06-05 18:31:48,048][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:31:48,051][HYDRA] 	#363 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 18:31:48,247][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:31:48,248][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:31:48,250][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:31:48,250][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:31:48,252][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:31:48,253][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:31:48,253][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:31:48,254][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:31:48,255][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:31:48,255][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:31:48,256][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:31:48,257][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:31:48,291][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.05/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 67.86it/s v_num: 0.000      
                                                              val/auc: 0.714    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.429 val/mre:    
                                                              0.044 train/auc:  
                                                              0.675 train/f1:   
                                                              0.673             
                                                              train/precision:  
                                                              0.679             
                                                              train/recall:     
                                                              0.667 train/mre:  
                                                              0.045             
[2024-06-05 18:32:02,220][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.05/3>
[2024-06-05 18:32:02,221][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:32:02,224][HYDRA] 	#364 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 18:32:02,432][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:32:02,433][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:32:02,435][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:32:02,435][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:32:02,437][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:32:02,438][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:32:02,438][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:32:02,439][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:32:02,440][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:32:02,440][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:32:02,441][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:32:02,442][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:32:02,485][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.05/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.20it/s v_num: 0.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.038 train/auc:  
                                                              0.640 train/f1:   
                                                              0.672             
                                                              train/precision:  
                                                              0.618             
                                                              train/recall:     
                                                              0.737 train/mre:  
                                                              0.048             
[2024-06-05 18:32:16,654][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.05/4>
[2024-06-05 18:32:16,655][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:32:16,657][HYDRA] 	#365 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 18:32:16,867][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:32:16,868][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:32:16,870][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:32:16,870][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:32:16,874][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:32:16,874][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:32:16,875][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:32:16,875][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:32:16,876][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:32:16,877][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:32:16,877][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:32:16,878][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:32:16,932][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.05/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.00it/s v_num: 0.000      
                                                              val/auc: 0.468    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.417 val/recall: 
                                                              0.714 val/mre:    
                                                              0.034 train/auc:  
                                                              0.561 train/f1:   
                                                              0.643             
                                                              train/precision:  
                                                              0.542             
                                                              train/recall:     
                                                              0.789 train/mre:  
                                                              0.037             
[2024-06-05 18:32:31,239][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.05/5>
[2024-06-05 18:32:31,239][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:32:31,243][HYDRA] 	#366 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 18:32:31,436][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:32:31,438][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:32:31,439][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:32:31,440][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:32:31,443][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:32:31,444][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:32:31,444][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:32:31,445][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:32:31,446][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:32:31,447][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:32:31,447][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:32:31,448][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:32:31,491][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.05/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.49it/s v_num: 0.000      
                                                              val/auc: 0.563    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.571 val/mre:    
                                                              0.042 train/auc:  
                                                              0.693 train/f1:   
                                                              0.762             
                                                              train/precision:  
                                                              0.622             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.048             
[2024-06-05 18:32:45,368][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.05/6>
[2024-06-05 18:32:45,368][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:32:45,371][HYDRA] 	#367 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 18:32:45,574][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:32:45,576][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:32:45,577][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:32:45,577][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:32:45,580][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:32:45,581][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:32:45,581][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:32:45,582][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:32:45,583][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:32:45,585][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:32:45,586][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:32:45,587][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:32:45,702][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.05/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 54.45it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.609     
                                                              val/precision:    
                                                              0.438 val/recall: 
                                                              1.000 val/mre:    
                                                              0.044 train/auc:  
                                                              0.509 train/f1:   
                                                              0.671             
                                                              train/precision:  
                                                              0.504             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.051             
[2024-06-05 18:33:00,020][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.05/7>
[2024-06-05 18:33:00,022][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:33:00,024][HYDRA] 	#368 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 18:33:00,232][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:33:00,234][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:33:00,235][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:33:00,235][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:33:00,237][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:33:00,238][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:33:00,238][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:33:00,239][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:33:00,240][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:33:00,240][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:33:00,240][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:33:00,242][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:33:00,282][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.05/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 66.40it/s v_num: 0.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.040 train/auc:  
                                                              0.711 train/f1:   
                                                              0.713             
                                                              train/precision:  
                                                              0.707             
                                                              train/recall:     
                                                              0.719 train/mre:  
                                                              0.041             
[2024-06-05 18:33:14,468][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.05/8>
[2024-06-05 18:33:14,469][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:33:14,472][HYDRA] 	#369 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 18:33:14,681][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:33:14,683][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:33:14,684][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:33:14,684][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:33:14,687][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:33:14,687][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:33:14,687][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:33:14,688][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:33:14,689][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:33:14,689][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:33:14,690][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:33:14,691][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:33:14,733][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.05/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 68.46it/s v_num: 0.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.037 train/auc:  
                                                              0.825 train/f1:   
                                                              0.828             
                                                              train/precision:  
                                                              0.814             
                                                              train/recall:     
                                                              0.842 train/mre:  
                                                              0.044             
[2024-06-05 18:33:28,708][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.05/9>
[2024-06-05 18:33:28,709][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:33:28,713][HYDRA] 	#370 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 18:33:28,929][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:33:28,930][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:33:28,932][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:33:28,932][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:33:28,935][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:33:28,935][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:33:28,936][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:33:28,936][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:33:28,937][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:33:28,938][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:33:28,938][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:33:28,939][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:33:28,987][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.1/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 72.41it/s v_num: 0.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.031 train/auc:  
                                                              0.789 train/f1:   
                                                              0.812             
                                                              train/precision:  
                                                              0.732             
                                                              train/recall:     
                                                              0.912 train/mre:  
                                                              0.044             
[2024-06-05 18:33:43,098][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.1/0>
[2024-06-05 18:33:43,099][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:33:43,102][HYDRA] 	#371 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 18:33:43,297][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:33:43,299][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:33:43,300][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:33:43,300][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:33:43,303][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:33:43,303][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:33:43,304][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:33:43,304][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:33:43,305][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:33:43,306][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:33:43,306][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:33:43,307][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:33:43,352][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.1/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 67.20it/s v_num: 0.000      
                                                              val/auc: 0.460    
                                                              val/f1: 0.200     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.143 val/mre:    
                                                              0.045 train/auc:  
                                                              0.921 train/f1:   
                                                              0.924             
                                                              train/precision:  
                                                              0.887             
                                                              train/recall:     
                                                              0.965 train/mre:  
                                                              0.059             
[2024-06-05 18:33:59,027][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.1/1>
[2024-06-05 18:33:59,028][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:33:59,032][HYDRA] 	#372 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 18:33:59,268][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:33:59,270][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:33:59,271][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:33:59,271][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:33:59,275][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:33:59,275][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:33:59,275][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:33:59,276][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:33:59,277][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:33:59,282][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:33:59,282][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:33:59,283][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:33:59,459][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.1/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 68.11it/s v_num: 0.000      
                                                              val/auc: 0.563    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.571 val/mre:    
                                                              0.035 train/auc:  
                                                              0.728 train/f1:   
                                                              0.744             
                                                              train/precision:  
                                                              0.703             
                                                              train/recall:     
                                                              0.789 train/mre:  
                                                              0.050             
[2024-06-05 18:34:13,857][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.1/2>
[2024-06-05 18:34:13,858][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:34:13,861][HYDRA] 	#373 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 18:34:14,071][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:34:14,072][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:34:14,074][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:34:14,074][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:34:14,076][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:34:14,077][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:34:14,077][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:34:14,078][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:34:14,079][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:34:14,079][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:34:14,079][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:34:14,081][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:34:14,120][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.1/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.05it/s v_num: 0.000      
                                                              val/auc: 0.532    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.286 val/mre:    
                                                              0.038 train/auc:  
                                                              0.719 train/f1:   
                                                              0.750             
                                                              train/precision:  
                                                              0.676             
                                                              train/recall:     
                                                              0.842 train/mre:  
                                                              0.046             
[2024-06-05 18:34:28,256][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.1/3>
[2024-06-05 18:34:28,257][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:34:28,260][HYDRA] 	#374 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 18:34:28,466][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:34:28,468][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:34:28,469][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:34:28,469][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:34:28,472][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:34:28,473][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:34:28,473][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:34:28,474][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:34:28,475][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:34:28,475][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:34:28,475][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:34:28,476][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:34:28,514][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.1/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.36it/s v_num: 0.000      
                                                              val/auc: 0.659    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.429 val/mre:    
                                                              0.038 train/auc:  
                                                              0.754 train/f1:   
                                                              0.781             
                                                              train/precision:  
                                                              0.704             
                                                              train/recall:     
                                                              0.877 train/mre:  
                                                              0.051             
[2024-06-05 18:34:42,653][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.1/4>
[2024-06-05 18:34:42,653][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:34:42,656][HYDRA] 	#375 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 18:34:42,860][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:34:42,861][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:34:42,863][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:34:42,863][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:34:42,866][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:34:42,866][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:34:42,867][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:34:42,867][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:34:42,868][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:34:42,869][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:34:42,869][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:34:42,870][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:34:42,915][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.1/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 55.85it/s v_num: 0.000      
                                                              val/auc: 0.579    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.714 val/mre:    
                                                              0.032 train/auc:  
                                                              0.526 train/f1:   
                                                              0.630             
                                                              train/precision:  
                                                              0.517             
                                                              train/recall:     
                                                              0.807 train/mre:  
                                                              0.037             
[2024-06-05 18:34:57,783][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.1/5>
[2024-06-05 18:34:57,784][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:34:57,788][HYDRA] 	#376 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 18:34:58,002][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:34:58,003][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:34:58,005][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:34:58,005][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:34:58,008][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:34:58,008][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:34:58,009][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:34:58,009][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:34:58,010][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:34:58,011][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:34:58,011][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:34:58,012][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:34:58,057][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.1/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 66.75it/s v_num: 0.000      
                                                              val/auc: 0.492    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.429 val/mre:    
                                                              0.033 train/auc:  
                                                              0.798 train/f1:   
                                                              0.830             
                                                              train/precision:  
                                                              0.718             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.045             
[2024-06-05 18:35:12,160][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.1/6>
[2024-06-05 18:35:12,161][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:35:12,164][HYDRA] 	#377 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 18:35:12,378][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:35:12,380][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:35:12,381][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:35:12,381][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:35:12,384][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:35:12,384][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:35:12,385][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:35:12,385][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:35:12,386][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:35:12,389][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:35:12,389][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:35:12,390][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:35:12,495][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.1/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 58.30it/s v_num: 0.000      
                                                              val/auc: 0.468    
                                                              val/f1: 0.526     
                                                              val/precision:    
                                                              0.417 val/recall: 
                                                              0.714 val/mre:    
                                                              0.032 train/auc:  
                                                              0.632 train/f1:   
                                                              0.661             
                                                              train/precision:  
                                                              0.612             
                                                              train/recall:     
                                                              0.719 train/mre:  
                                                              0.037             
[2024-06-05 18:35:26,571][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.1/7>
[2024-06-05 18:35:26,572][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:35:26,574][HYDRA] 	#378 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 18:35:26,775][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:35:26,777][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:35:26,778][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:35:26,779][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:35:26,781][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:35:26,781][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:35:26,782][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:35:26,782][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:35:26,783][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:35:26,784][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:35:26,784][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:35:26,787][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:35:26,826][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.1/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.56it/s v_num: 0.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.048 train/auc:  
                                                              0.781 train/f1:   
                                                              0.771             
                                                              train/precision:  
                                                              0.808             
                                                              train/recall:     
                                                              0.737 train/mre:  
                                                              0.043             
[2024-06-05 18:35:42,484][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.1/8>
[2024-06-05 18:35:42,485][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:35:42,501][HYDRA] 	#379 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 18:35:42,758][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:35:42,760][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:35:42,761][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:35:42,761][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:35:42,766][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:35:42,767][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:35:42,767][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:35:42,768][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:35:42,769][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:35:42,769][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:35:42,769][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:35:42,771][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:35:42,872][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.1/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.43it/s v_num: 0.000      
                                                              val/auc: 0.540    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.857 val/mre:    
                                                              0.054 train/auc:  
                                                              0.544 train/f1:   
                                                              0.679             
                                                              train/precision:  
                                                              0.524             
                                                              train/recall:     
                                                              0.965 train/mre:  
                                                              0.074             
[2024-06-05 18:35:58,949][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.1/9>
[2024-06-05 18:35:58,952][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:35:58,957][HYDRA] 	#380 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 18:35:59,204][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:35:59,206][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:35:59,207][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:35:59,208][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:35:59,212][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:35:59,212][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:35:59,213][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:35:59,213][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:35:59,214][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:35:59,215][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:35:59,215][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:35:59,216][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:35:59,308][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.15/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.68it/s v_num: 0.000      
                                                              val/auc: 0.563    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.571 val/mre:    
                                                              0.033 train/auc:  
                                                              0.763 train/f1:   
                                                              0.777             
                                                              train/precision:  
                                                              0.734             
                                                              train/recall:     
                                                              0.825 train/mre:  
                                                              0.038             
[2024-06-05 18:36:15,225][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.15/0>
[2024-06-05 18:36:15,226][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:36:15,238][HYDRA] 	#381 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 18:36:15,493][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:36:15,495][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:36:15,496][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:36:15,497][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:36:15,499][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:36:15,500][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:36:15,500][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:36:15,500][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:36:15,502][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:36:15,504][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:36:15,504][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:36:15,506][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:36:15,702][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.15/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.39it/s v_num: 0.000      
                                                              val/auc: 0.571    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.143 val/mre:    
                                                              0.044 train/auc:  
                                                              0.877 train/f1:   
                                                              0.870             
                                                              train/precision:  
                                                              0.922             
                                                              train/recall:     
                                                              0.825 train/mre:  
                                                              0.052             
[2024-06-05 18:36:32,093][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.15/1>
[2024-06-05 18:36:32,093][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:36:32,096][HYDRA] 	#382 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 18:36:32,339][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:36:32,340][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:36:32,342][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:36:32,342][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:36:32,344][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:36:32,345][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:36:32,345][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:36:32,346][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:36:32,347][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:36:32,347][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:36:32,347][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:36:32,349][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:36:32,406][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.15/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 66.03it/s v_num: 0.000      
                                                              val/auc: 0.675    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.571 val/mre:    
                                                              0.034 train/auc:  
                                                              0.737 train/f1:   
                                                              0.758             
                                                              train/precision:  
                                                              0.701             
                                                              train/recall:     
                                                              0.825 train/mre:  
                                                              0.045             
[2024-06-05 18:36:48,156][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.15/2>
[2024-06-05 18:36:48,159][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:36:48,162][HYDRA] 	#383 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 18:36:48,467][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:36:48,469][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:36:48,471][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:36:48,471][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:36:48,474][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:36:48,474][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:36:48,475][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:36:48,475][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:36:48,476][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:36:48,477][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:36:48,477][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:36:48,478][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:36:48,563][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.15/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 71.42it/s v_num: 0.000      
                                                              val/auc: 0.675    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.571 val/mre:    
                                                              0.030 train/auc:  
                                                              0.588 train/f1:   
                                                              0.689             
                                                              train/precision:  
                                                              0.553             
                                                              train/recall:     
                                                              0.912 train/mre:  
                                                              0.041             
[2024-06-05 18:37:04,098][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.15/3>
[2024-06-05 18:37:04,099][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:37:04,102][HYDRA] 	#384 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 18:37:04,348][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:37:04,350][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:37:04,351][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:37:04,352][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:37:04,354][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:37:04,354][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:37:04,355][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:37:04,355][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:37:04,356][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:37:04,357][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:37:04,357][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:37:04,359][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:37:04,417][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.15/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.05it/s v_num: 0.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.037 train/auc:  
                                                              0.763 train/f1:   
                                                              0.773             
                                                              train/precision:  
                                                              0.742             
                                                              train/recall:     
                                                              0.807 train/mre:  
                                                              0.049             
[2024-06-05 18:37:19,536][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.15/4>
[2024-06-05 18:37:19,537][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:37:19,541][HYDRA] 	#385 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 18:37:19,779][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:37:19,780][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:37:19,781][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:37:19,782][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:37:19,784][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:37:19,784][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:37:19,785][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:37:19,785][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:37:19,786][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:37:19,789][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:37:19,789][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:37:19,790][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:37:19,922][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.15/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 68.70it/s v_num: 0.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.033 train/auc:  
                                                              0.596 train/f1:   
                                                              0.641             
                                                              train/precision:  
                                                              0.577             
                                                              train/recall:     
                                                              0.719 train/mre:  
                                                              0.038             
[2024-06-05 18:37:34,655][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.15/5>
[2024-06-05 18:37:34,657][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:37:34,662][HYDRA] 	#386 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 18:37:34,869][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:37:34,870][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:37:34,872][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:37:34,872][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:37:34,876][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:37:34,877][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:37:34,878][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:37:34,878][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:37:34,879][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:37:34,880][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:37:34,880][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:37:34,881][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:37:34,943][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.15/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.75it/s v_num: 0.000      
                                                              val/auc: 0.548    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.429 val/mre:    
                                                              0.038 train/auc:  
                                                              0.754 train/f1:   
                                                              0.788             
                                                              train/precision:  
                                                              0.693             
                                                              train/recall:     
                                                              0.912 train/mre:  
                                                              0.047             
[2024-06-05 18:37:49,576][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.15/6>
[2024-06-05 18:37:49,577][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:37:49,580][HYDRA] 	#387 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 18:37:49,809][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:37:49,811][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:37:49,812][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:37:49,813][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:37:49,815][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:37:49,815][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:37:49,816][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:37:49,816][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:37:49,817][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:37:49,818][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:37:49,818][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:37:49,819][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:37:49,871][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.15/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 66.11it/s v_num: 0.000      
                                                              val/auc: 0.540    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.857 val/mre:    
                                                              0.056 train/auc:  
                                                              0.579 train/f1:   
                                                              0.529             
                                                              train/precision:  
                                                              0.600             
                                                              train/recall:     
                                                              0.474 train/mre:  
                                                              0.057             
[2024-06-05 18:38:03,883][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.15/7>
[2024-06-05 18:38:03,884][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:38:03,886][HYDRA] 	#388 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 18:38:04,087][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:38:04,089][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:38:04,090][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:38:04,090][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:38:04,093][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:38:04,094][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:38:04,095][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:38:04,095][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:38:04,096][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:38:04,097][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:38:04,097][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:38:04,098][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:38:04,141][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.15/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 73.24it/s v_num: 0.000      
                                                              val/auc: 0.397    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.364 val/recall: 
                                                              0.571 val/mre:    
                                                              0.035 train/auc:  
                                                              0.754 train/f1:   
                                                              0.767             
                                                              train/precision:  
                                                              0.730             
                                                              train/recall:     
                                                              0.807 train/mre:  
                                                              0.047             
[2024-06-05 18:38:17,921][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.15/8>
[2024-06-05 18:38:17,922][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:38:17,928][HYDRA] 	#389 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 18:38:18,153][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:38:18,155][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:38:18,156][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:38:18,156][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:38:18,158][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:38:18,159][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:38:18,159][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:38:18,160][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:38:18,161][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:38:18,161][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:38:18,161][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:38:18,163][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:38:18,214][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.15/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 69.48it/s v_num: 0.000      
                                                              val/auc: 0.548    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.429 val/mre:    
                                                              0.041 train/auc:  
                                                              0.825 train/f1:   
                                                              0.844             
                                                              train/precision:  
                                                              0.761             
                                                              train/recall:     
                                                              0.947 train/mre:  
                                                              0.047             
[2024-06-05 18:38:32,457][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.15/9>
[2024-06-05 18:38:32,458][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:38:32,461][HYDRA] 	#390 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 18:38:32,682][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:38:32,683][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:38:32,685][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:38:32,685][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:38:32,687][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:38:32,688][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:38:32,688][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:38:32,689][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:38:32,690][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:38:32,692][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:38:32,693][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:38:32,695][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:38:32,850][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.2/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 68.79it/s v_num: 0.000      
                                                              val/auc: 0.587    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.286 val/mre:    
                                                              0.033 train/auc:  
                                                              0.851 train/f1:   
                                                              0.862             
                                                              train/precision:  
                                                              0.803             
                                                              train/recall:     
                                                              0.930 train/mre:  
                                                              0.041             
[2024-06-05 18:38:46,989][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.2/0>
[2024-06-05 18:38:46,990][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:38:46,994][HYDRA] 	#391 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 18:38:47,217][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:38:47,218][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:38:47,219][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:38:47,220][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:38:47,222][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:38:47,223][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:38:47,223][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:38:47,223][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:38:47,225][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:38:47,225][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:38:47,225][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:38:47,227][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:38:47,274][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.2/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 68.51it/s v_num: 0.000      
                                                              val/auc: 0.516    
                                                              val/f1: 0.222     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.143 val/mre:    
                                                              0.045 train/auc:  
                                                              0.895 train/f1:   
                                                              0.898             
                                                              train/precision:  
                                                              0.869             
                                                              train/recall:     
                                                              0.930 train/mre:  
                                                              0.059             
[2024-06-05 18:39:01,395][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.2/1>
[2024-06-05 18:39:01,395][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:39:01,400][HYDRA] 	#392 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 18:39:02,152][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:39:02,153][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:39:02,154][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:39:02,155][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:39:02,157][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:39:02,158][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:39:02,158][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:39:02,158][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:39:02,159][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:39:02,160][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:39:02,160][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:39:02,161][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:39:02,202][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.2/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 69.18it/s v_num: 0.000      
                                                              val/auc: 0.675    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.571 val/mre:    
                                                              0.036 train/auc:  
                                                              0.842 train/f1:   
                                                              0.852             
                                                              train/precision:  
                                                              0.800             
                                                              train/recall:     
                                                              0.912 train/mre:  
                                                              0.046             
[2024-06-05 18:39:17,448][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.2/2>
[2024-06-05 18:39:17,449][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:39:17,465][HYDRA] 	#393 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 18:39:17,679][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:39:17,680][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:39:17,681][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:39:17,682][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:39:17,684][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:39:17,685][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:39:17,686][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:39:17,686][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:39:17,687][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:39:17,688][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:39:17,688][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:39:17,689][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:39:17,722][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.2/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 67.19it/s v_num: 0.000      
                                                              val/auc: 0.476    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.286 val/mre:    
                                                              0.031 train/auc:  
                                                              0.711 train/f1:   
                                                              0.752             
                                                              train/precision:  
                                                              0.658             
                                                              train/recall:     
                                                              0.877 train/mre:  
                                                              0.043             
[2024-06-05 18:39:31,727][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.2/3>
[2024-06-05 18:39:31,727][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:39:31,731][HYDRA] 	#394 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 18:39:31,941][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:39:31,943][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:39:31,944][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:39:31,944][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:39:31,947][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:39:31,947][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:39:31,948][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:39:31,948][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:39:31,949][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:39:31,950][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:39:31,950][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:39:31,951][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:39:31,997][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.2/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 53.02it/s v_num: 0.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.036 train/auc:  
                                                              0.798 train/f1:   
                                                              0.810             
                                                              train/precision:  
                                                              0.766             
                                                              train/recall:     
                                                              0.860 train/mre:  
                                                              0.048             
[2024-06-05 18:39:46,635][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.2/4>
[2024-06-05 18:39:46,635][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:39:46,640][HYDRA] 	#395 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 18:39:46,847][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:39:46,848][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:39:46,850][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:39:46,850][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:39:46,852][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:39:46,852][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:39:46,853][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:39:46,853][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:39:46,854][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:39:46,856][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:39:46,856][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:39:46,857][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:39:46,958][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.2/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.81it/s v_num: 0.000      
                                                              val/auc: 0.341    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.571 val/mre:    
                                                              0.031 train/auc:  
                                                              0.526 train/f1:   
                                                              0.609             
                                                              train/precision:  
                                                              0.519             
                                                              train/recall:     
                                                              0.737 train/mre:  
                                                              0.038             
[2024-06-05 18:40:01,196][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.2/5>
[2024-06-05 18:40:01,197][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:40:01,200][HYDRA] 	#396 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 18:40:01,400][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:40:01,401][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:40:01,403][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:40:01,403][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:40:01,407][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:40:01,408][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:40:01,409][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:40:01,409][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:40:01,410][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:40:01,411][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:40:01,411][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:40:01,412][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:40:01,448][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.2/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 57.22it/s v_num: 0.000      
                                                              val/auc: 0.556    
                                                              val/f1: 0.636     
                                                              val/precision:    
                                                              0.467 val/recall: 
                                                              1.000 val/mre:    
                                                              0.045 train/auc:  
                                                              0.702 train/f1:   
                                                              0.767             
                                                              train/precision:  
                                                              0.629             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.051             
[2024-06-05 18:40:15,854][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.2/6>
[2024-06-05 18:40:15,855][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:40:15,857][HYDRA] 	#397 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 18:40:16,063][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:40:16,065][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:40:16,066][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:40:16,066][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:40:16,069][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:40:16,069][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:40:16,070][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:40:16,070][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:40:16,071][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:40:16,072][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:40:16,072][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:40:16,073][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:40:16,114][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.2/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.22it/s v_num: 0.000      
                                                              val/auc: 0.381    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.429 val/mre:    
                                                              0.047 train/auc:  
                                                              0.518 train/f1:   
                                                              0.530             
                                                              train/precision:  
                                                              0.517             
                                                              train/recall:     
                                                              0.544 train/mre:  
                                                              0.041             
[2024-06-05 18:40:30,097][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.2/7>
[2024-06-05 18:40:30,097][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:40:30,101][HYDRA] 	#398 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 18:40:30,315][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:40:30,316][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:40:30,317][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:40:30,318][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:40:30,320][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:40:30,321][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:40:30,321][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:40:30,321][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:40:30,323][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:40:30,323][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:40:30,323][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:40:30,325][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:40:30,368][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.2/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.45it/s v_num: 0.000      
                                                              val/auc: 0.675    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.571 val/mre:    
                                                              0.032 train/auc:  
                                                              0.553 train/f1:   
                                                              0.400             
                                                              train/precision:  
                                                              0.607             
                                                              train/recall:     
                                                              0.298 train/mre:  
                                                              0.037             
[2024-06-05 18:40:44,694][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.2/8>
[2024-06-05 18:40:44,695][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:40:44,698][HYDRA] 	#399 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 18:40:44,899][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:40:44,901][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:40:44,902][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:40:44,902][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:40:44,904][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:40:44,905][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:40:44,905][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:40:44,906][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:40:44,907][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:40:44,909][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:40:44,909][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:40:44,910][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:40:44,965][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.2/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.22it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.043 train/auc:  
                                                              0.833 train/f1:   
                                                              0.848             
                                                              train/precision:  
                                                              0.779             
                                                              train/recall:     
                                                              0.930 train/mre:  
                                                              0.052             
[2024-06-05 18:40:58,898][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.2/9>
[2024-06-05 18:40:58,899][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:40:58,902][HYDRA] 	#400 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 18:40:59,107][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:40:59,108][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:40:59,110][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:40:59,110][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:40:59,115][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:40:59,115][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:40:59,115][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:40:59,116][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:40:59,117][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:40:59,119][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:40:59,119][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:40:59,120][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:40:59,232][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.25/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.94it/s v_num: 0.000      
                                                              val/auc: 0.690    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.714 val/mre:    
                                                              0.043 train/auc:  
                                                              0.842 train/f1:   
                                                              0.845             
                                                              train/precision:  
                                                              0.831             
                                                              train/recall:     
                                                              0.860 train/mre:  
                                                              0.045             
[2024-06-05 18:41:13,359][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.25/0>
[2024-06-05 18:41:13,360][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:41:13,362][HYDRA] 	#401 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 18:41:13,566][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:41:13,568][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:41:13,569][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:41:13,569][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:41:13,571][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:41:13,572][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:41:13,572][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:41:13,573][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:41:13,574][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:41:13,574][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:41:13,575][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:41:13,576][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:41:13,619][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.25/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 67.81it/s v_num: 0.000      
                                                              val/auc: 0.524    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.714 val/mre:    
                                                              0.039 train/auc:  
                                                              0.939 train/f1:   
                                                              0.939             
                                                              train/precision:  
                                                              0.931             
                                                              train/recall:     
                                                              0.947 train/mre:  
                                                              0.054             
[2024-06-05 18:41:27,635][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.25/1>
[2024-06-05 18:41:27,635][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:41:27,640][HYDRA] 	#402 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 18:41:27,863][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:41:27,864][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:41:27,866][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:41:27,866][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:41:27,868][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:41:27,869][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:41:27,869][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:41:27,870][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:41:27,871][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:41:27,872][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:41:27,872][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:41:27,873][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:41:27,915][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.25/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 69.49it/s v_num: 0.000      
                                                              val/auc: 0.690    
                                                              val/f1: 0.667     
                                                              val/precision:    
                                                              0.625 val/recall: 
                                                              0.714 val/mre:    
                                                              0.033 train/auc:  
                                                              0.833 train/f1:   
                                                              0.846             
                                                              train/precision:  
                                                              0.788             
                                                              train/recall:     
                                                              0.912 train/mre:  
                                                              0.051             
[2024-06-05 18:41:41,848][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.25/2>
[2024-06-05 18:41:41,849][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:41:41,859][HYDRA] 	#403 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 18:41:42,079][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:41:42,080][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:41:42,081][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:41:42,082][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:41:42,084][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:41:42,084][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:41:42,085][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:41:42,085][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:41:42,086][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:41:42,087][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:41:42,087][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:41:42,088][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:41:42,125][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.25/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 54.27it/s v_num: 0.000      
                                                              val/auc: 0.452    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.571 val/mre:    
                                                              0.032 train/auc:  
                                                              0.623 train/f1:   
                                                              0.719             
                                                              train/precision:  
                                                              0.573             
                                                              train/recall:     
                                                              0.965 train/mre:  
                                                              0.042             
[2024-06-05 18:41:56,363][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.25/3>
[2024-06-05 18:41:56,364][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:41:56,366][HYDRA] 	#404 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 18:41:56,571][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:41:56,572][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:41:56,574][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:41:56,574][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:41:56,576][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:41:56,577][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:41:56,577][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:41:56,578][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:41:56,579][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:41:56,581][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:41:56,581][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:41:56,582][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:41:56,622][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.25/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 57.80it/s v_num: 0.000      
                                                              val/auc: 0.675    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.571 val/mre:    
                                                              0.039 train/auc:  
                                                              0.921 train/f1:   
                                                              0.926             
                                                              train/precision:  
                                                              0.875             
                                                              train/recall:     
                                                              0.982 train/mre:  
                                                              0.057             
[2024-06-05 18:42:10,759][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.25/4>
[2024-06-05 18:42:10,759][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:42:10,762][HYDRA] 	#405 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 18:42:10,978][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:42:10,979][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:42:10,981][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:42:10,981][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:42:10,983][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:42:10,983][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:42:10,984][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:42:10,984][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:42:10,985][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:42:10,987][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:42:10,987][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:42:10,988][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:42:11,092][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.25/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 67.50it/s v_num: 0.000      
                                                              val/auc: 0.659    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.429 val/mre:    
                                                              0.030 train/auc:  
                                                              0.535 train/f1:   
                                                              0.602             
                                                              train/precision:  
                                                              0.526             
                                                              train/recall:     
                                                              0.702 train/mre:  
                                                              0.039             
[2024-06-05 18:42:24,919][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.25/5>
[2024-06-05 18:42:24,920][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:42:24,922][HYDRA] 	#406 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 18:42:25,124][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:42:25,126][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:42:25,127][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:42:25,127][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:42:25,130][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:42:25,130][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:42:25,131][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:42:25,131][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:42:25,132][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:42:25,133][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:42:25,133][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:42:25,134][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:42:25,179][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.25/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.81it/s v_num: 0.000      
                                                              val/auc: 0.635    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.714 val/mre:    
                                                              0.034 train/auc:  
                                                              0.825 train/f1:   
                                                              0.833             
                                                              train/precision:  
                                                              0.794             
                                                              train/recall:     
                                                              0.877 train/mre:  
                                                              0.043             
[2024-06-05 18:42:39,168][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.25/6>
[2024-06-05 18:42:39,169][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:42:39,172][HYDRA] 	#407 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 18:42:39,374][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:42:39,375][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:42:39,377][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:42:39,377][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:42:39,379][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:42:39,380][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:42:39,380][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:42:39,381][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:42:39,382][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:42:39,382][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:42:39,382][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:42:39,384][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:42:39,424][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.25/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.46it/s v_num: 0.000      
                                                              val/auc: 0.595    
                                                              val/f1: 0.632     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.857 val/mre:    
                                                              0.043 train/auc:  
                                                              0.614 train/f1:   
                                                              0.627             
                                                              train/precision:  
                                                              0.607             
                                                              train/recall:     
                                                              0.649 train/mre:  
                                                              0.042             
[2024-06-05 18:42:53,780][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.25/7>
[2024-06-05 18:42:53,781][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:42:53,784][HYDRA] 	#408 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 18:42:53,979][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:42:53,980][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:42:53,982][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:42:53,982][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:42:53,984][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:42:53,984][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:42:53,985][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:42:53,986][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:42:53,987][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:42:53,987][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:42:53,987][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:42:53,988][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:42:54,026][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.25/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 68.63it/s v_num: 0.000      
                                                              val/auc: 0.635    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.556 val/recall: 
                                                              0.714 val/mre:    
                                                              0.041 train/auc:  
                                                              0.658 train/f1:   
                                                              0.655             
                                                              train/precision:  
                                                              0.661             
                                                              train/recall:     
                                                              0.649 train/mre:  
                                                              0.038             
[2024-06-05 18:43:08,330][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.25/8>
[2024-06-05 18:43:08,331][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:43:08,337][HYDRA] 	#409 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 18:43:08,537][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:43:08,538][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:43:08,539][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:43:08,540][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:43:08,542][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:43:08,542][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:43:08,543][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:43:08,543][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:43:08,544][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:43:08,546][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:43:08,546][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:43:08,547][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:43:08,586][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.25/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.02it/s v_num: 0.000      
                                                              val/auc: 0.532    
                                                              val/f1: 0.364     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.286 val/mre:    
                                                              0.036 train/auc:  
                                                              0.895 train/f1:   
                                                              0.902             
                                                              train/precision:  
                                                              0.846             
                                                              train/recall:     
                                                              0.965 train/mre:  
                                                              0.052             
[2024-06-05 18:43:22,827][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.25/9>
[2024-06-05 18:43:22,829][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:43:22,831][HYDRA] 	#410 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 18:43:23,037][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:43:23,039][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:43:23,040][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:43:23,040][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:43:23,043][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:43:23,044][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:43:23,044][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:43:23,045][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:43:23,046][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:43:23,047][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:43:23,047][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:43:23,049][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:43:23,149][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.3/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 70.76it/s v_num: 0.000      
                                                              val/auc: 0.476    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.286 val/mre:    
                                                              0.030 train/auc:  
                                                              0.798 train/f1:   
                                                              0.816             
                                                              train/precision:  
                                                              0.750             
                                                              train/recall:     
                                                              0.895 train/mre:  
                                                              0.044             
[2024-06-05 18:43:37,679][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.3/0>
[2024-06-05 18:43:37,679][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:43:37,684][HYDRA] 	#411 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 18:43:37,940][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:43:37,942][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:43:37,943][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:43:37,943][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:43:37,947][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:43:37,947][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:43:37,948][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:43:37,948][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:43:37,949][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:43:37,950][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:43:37,950][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:43:37,951][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:43:37,997][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.3/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 70.60it/s v_num: 0.000      
                                                              val/auc: 0.405    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.143 val/mre:    
                                                              0.071 train/auc:  
                                                              0.886 train/f1:   
                                                              0.876             
                                                              train/precision:  
                                                              0.958             
                                                              train/recall:     
                                                              0.807 train/mre:  
                                                              0.062             
[2024-06-05 18:43:52,430][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.3/1>
[2024-06-05 18:43:52,431][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:43:52,433][HYDRA] 	#412 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 18:43:52,646][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:43:52,648][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:43:52,649][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:43:52,650][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:43:52,652][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:43:52,653][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:43:52,653][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:43:52,654][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:43:52,655][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:43:52,655][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:43:52,655][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:43:52,657][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:43:52,696][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.3/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.66it/s v_num: 0.000      
                                                              val/auc: 0.603    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.429 val/mre:    
                                                              0.038 train/auc:  
                                                              0.868 train/f1:   
                                                              0.874             
                                                              train/precision:  
                                                              0.839             
                                                              train/recall:     
                                                              0.912 train/mre:  
                                                              0.046             
[2024-06-05 18:44:08,084][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.3/2>
[2024-06-05 18:44:08,086][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:44:08,093][HYDRA] 	#413 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 18:44:08,326][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:44:08,328][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:44:08,329][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:44:08,329][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:44:08,332][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:44:08,332][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:44:08,333][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:44:08,333][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:44:08,334][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:44:08,335][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:44:08,335][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:44:08,336][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:44:08,408][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.3/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 67.50it/s v_num: 0.000      
                                                              val/auc: 0.508    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.571 val/mre:    
                                                              0.031 train/auc:  
                                                              0.623 train/f1:   
                                                              0.667             
                                                              train/precision:  
                                                              0.597             
                                                              train/recall:     
                                                              0.754 train/mre:  
                                                              0.042             
[2024-06-05 18:44:23,138][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.3/3>
[2024-06-05 18:44:23,138][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:44:23,142][HYDRA] 	#414 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 18:44:23,374][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:44:23,375][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:44:23,377][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:44:23,377][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:44:23,379][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:44:23,380][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:44:23,380][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:44:23,381][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:44:23,382][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:44:23,384][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:44:23,385][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:44:23,386][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:44:23,499][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.3/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 70.94it/s v_num: 0.000      
                                                              val/auc: 0.587    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.286 val/mre:    
                                                              0.034 train/auc:  
                                                              0.921 train/f1:   
                                                              0.923             
                                                              train/precision:  
                                                              0.900             
                                                              train/recall:     
                                                              0.947 train/mre:  
                                                              0.053             
[2024-06-05 18:44:37,497][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.3/4>
[2024-06-05 18:44:37,497][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:44:37,503][HYDRA] 	#415 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 18:44:37,712][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:44:37,714][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:44:37,715][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:44:37,715][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:44:37,717][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:44:37,718][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:44:37,718][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:44:37,719][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:44:37,720][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:44:37,720][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:44:37,720][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:44:37,722][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:44:37,755][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.3/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 70.27it/s v_num: 0.000      
                                                              val/auc: 0.675    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.571 val/mre:    
                                                              0.032 train/auc:  
                                                              0.623 train/f1:   
                                                              0.661             
                                                              train/precision:  
                                                              0.600             
                                                              train/recall:     
                                                              0.737 train/mre:  
                                                              0.039             
[2024-06-05 18:44:51,706][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.3/5>
[2024-06-05 18:44:51,707][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:44:51,712][HYDRA] 	#416 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 18:44:51,907][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:44:51,909][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:44:51,910][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:44:51,910][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:44:51,913][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:44:51,913][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:44:51,914][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:44:51,914][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:44:51,915][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:44:51,916][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:44:51,916][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:44:51,917][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:44:51,961][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.3/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 57.46it/s v_num: 0.000      
                                                              val/auc: 0.524    
                                                              val/f1: 0.556     
                                                              val/precision:    
                                                              0.455 val/recall: 
                                                              0.714 val/mre:    
                                                              0.035 train/auc:  
                                                              0.851 train/f1:   
                                                              0.860             
                                                              train/precision:  
                                                              0.812             
                                                              train/recall:     
                                                              0.912 train/mre:  
                                                              0.041             
[2024-06-05 18:45:06,048][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.3/6>
[2024-06-05 18:45:06,050][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:45:06,053][HYDRA] 	#417 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 18:45:06,261][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:45:06,262][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:45:06,264][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:45:06,264][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:45:06,266][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:45:06,267][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:45:06,267][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:45:06,268][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:45:06,269][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:45:06,269][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:45:06,269][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:45:06,271][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:45:06,317][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.3/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.59it/s v_num: 0.000      
                                                              val/auc: 0.675    
                                                              val/f1: 0.615     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.571 val/mre:    
                                                              0.036 train/auc:  
                                                              0.842 train/f1:   
                                                              0.845             
                                                              train/precision:  
                                                              0.831             
                                                              train/recall:     
                                                              0.860 train/mre:  
                                                              0.054             
[2024-06-05 18:45:20,371][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.3/7>
[2024-06-05 18:45:20,372][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:45:20,374][HYDRA] 	#418 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 18:45:20,588][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:45:20,589][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:45:20,590][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:45:20,591][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:45:20,593][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:45:20,593][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:45:20,594][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:45:20,594][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:45:20,595][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:45:20,596][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:45:20,596][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:45:20,597][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:45:20,640][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.3/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.10it/s v_num: 0.000      
                                                              val/auc: 0.508    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.571 val/mre:    
                                                              0.033 train/auc:  
                                                              0.579 train/f1:   
                                                              0.547             
                                                              train/precision:  
                                                              0.592             
                                                              train/recall:     
                                                              0.509 train/mre:  
                                                              0.039             
[2024-06-05 18:45:35,132][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.3/8>
[2024-06-05 18:45:35,133][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:45:35,139][HYDRA] 	#419 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=diagnosed_p_t_s_d data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 18:45:35,349][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:45:35,350][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:45:35,352][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:45:35,352][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:45:35,357][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:45:35,357][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:45:35,358][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:45:35,358][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:45:35,359][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:45:35,362][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:45:35,362][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:45:35,363][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:45:35,486][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.3/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 68.84it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.036 train/auc:  
                                                              0.921 train/f1:   
                                                              0.923             
                                                              train/precision:  
                                                              0.900             
                                                              train/recall:     
                                                              0.947 train/mre:  
                                                              0.045             
[2024-06-05 18:45:49,832][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/diagnosed_p_t_s_d/0.3/9>
[2024-06-05 18:45:49,833][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:45:49,837][HYDRA] 	#420 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 18:45:50,041][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:45:50,043][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:45:50,044][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:45:50,044][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:45:50,046][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:45:50,047][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:45:50,047][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:45:50,048][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:45:50,049][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:45:50,049][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:45:50,050][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:45:50,051][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:45:50,095][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.0/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.62it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre: nan
                                                              train/auc: 0.906  
                                                              train/f1: 0.912   
                                                              train/precision:  
                                                              0.861             
                                                              train/recall:     
                                                              0.969 train/mre:  
                                                              nan               
[2024-06-05 18:46:06,619][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.0/0>
[2024-06-05 18:46:06,619][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:46:06,621][HYDRA] 	#421 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 18:46:06,825][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:46:06,826][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:46:06,828][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:46:06,828][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:46:06,830][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:46:06,831][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:46:06,831][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:46:06,832][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:46:06,833][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:46:06,833][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:46:06,833][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:46:06,835][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:46:06,870][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.0/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 56.81it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.250     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.906  
                                                              train/f1: 0.910   
                                                              train/precision:  
                                                              0.871             
                                                              train/recall:     
                                                              0.953 train/mre:  
                                                              nan               
[2024-06-05 18:46:22,405][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.0/1>
[2024-06-05 18:46:22,405][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:46:22,410][HYDRA] 	#422 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 18:46:22,621][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:46:22,623][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:46:22,624][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:46:22,624][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:46:22,626][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:46:22,627][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:46:22,627][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:46:22,628][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:46:22,629][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:46:22,629][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:46:22,630][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:46:22,631][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:46:22,676][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.0/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.65it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.143     
                                                              val/precision:    
                                                              0.250 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 0.914  
                                                              train/f1: 0.916   
                                                              train/precision:  
                                                              0.896             
                                                              train/recall:     
                                                              0.938 train/mre:  
                                                              nan               
[2024-06-05 18:46:38,274][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.0/2>
[2024-06-05 18:46:38,275][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:46:38,279][HYDRA] 	#423 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 18:46:38,486][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:46:38,488][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:46:38,489][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:46:38,490][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:46:38,492][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:46:38,493][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:46:38,493][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:46:38,494][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:46:38,495][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:46:38,497][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:46:38,497][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:46:38,500][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:46:38,604][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.0/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.21it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 0.891  
                                                              train/f1: 0.892   
                                                              train/precision:  
                                                              0.879             
                                                              train/recall:     
                                                              0.906 train/mre:  
                                                              nan               
[2024-06-05 18:46:54,140][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.0/3>
[2024-06-05 18:46:54,141][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:46:54,144][HYDRA] 	#424 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 18:46:54,348][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:46:54,350][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:46:54,351][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:46:54,351][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:46:54,353][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:46:54,354][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:46:54,354][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:46:54,355][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:46:54,356][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:46:54,357][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:46:54,357][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:46:54,358][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:46:54,396][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.0/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.40it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 0.875  
                                                              train/f1: 0.864   
                                                              train/precision:  
                                                              0.944             
                                                              train/recall:     
                                                              0.797 train/mre:  
                                                              nan               
[2024-06-05 18:47:09,773][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.0/4>
[2024-06-05 18:47:09,773][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:47:09,776][HYDRA] 	#425 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 18:47:09,992][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:47:09,993][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:47:09,995][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:47:09,995][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:47:09,997][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:47:09,998][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:47:09,998][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:47:09,999][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:47:10,000][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:47:10,000][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:47:10,001][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:47:10,002][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:47:10,049][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.0/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.87it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.400 val/mre: nan
                                                              train/auc: 0.781  
                                                              train/f1: 0.767   
                                                              train/precision:  
                                                              0.821             
                                                              train/recall:     
                                                              0.719 train/mre:  
                                                              nan               
[2024-06-05 18:47:26,023][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.0/5>
[2024-06-05 18:47:26,023][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:47:26,026][HYDRA] 	#426 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 18:47:26,235][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:47:26,236][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:47:26,237][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:47:26,238][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:47:26,242][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:47:26,243][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:47:26,243][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:47:26,244][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:47:26,245][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:47:26,245][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:47:26,245][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:47:26,247][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:47:26,289][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.0/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 53.81it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.930  
                                                              train/f1: 0.928   
                                                              train/precision:  
                                                              0.951             
                                                              train/recall:     
                                                              0.906 train/mre:  
                                                              nan               
[2024-06-05 18:47:42,211][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.0/6>
[2024-06-05 18:47:42,212][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:47:42,214][HYDRA] 	#427 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 18:47:42,421][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:47:42,423][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:47:42,424][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:47:42,424][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:47:42,428][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:47:42,429][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:47:42,430][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:47:42,430][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:47:42,431][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:47:42,434][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:47:42,434][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:47:42,435][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:47:42,550][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.0/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.12it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre: nan
                                                              train/auc: 0.883  
                                                              train/f1: 0.884   
                                                              train/precision:  
                                                              0.877             
                                                              train/recall:     
                                                              0.891 train/mre:  
                                                              nan               
[2024-06-05 18:47:58,137][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.0/7>
[2024-06-05 18:47:58,139][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:47:58,142][HYDRA] 	#428 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 18:47:58,348][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:47:58,350][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:47:58,351][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:47:58,351][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:47:58,354][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:47:58,354][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:47:58,354][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:47:58,355][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:47:58,356][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:47:58,357][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:47:58,357][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:47:58,358][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:47:58,416][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.0/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.41it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre: nan
                                                              train/auc: 0.828  
                                                              train/f1: 0.804   
                                                              train/precision:  
                                                              0.938             
                                                              train/recall:     
                                                              0.703 train/mre:  
                                                              nan               
[2024-06-05 18:48:14,152][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.0/8>
[2024-06-05 18:48:14,153][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:48:14,155][HYDRA] 	#429 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.0 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 18:48:14,381][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:48:14,382][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:48:14,383][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:48:14,384][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:48:14,386][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:48:14,386][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:48:14,387][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:48:14,387][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:48:14,388][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:48:14,389][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:48:14,389][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:48:14,391][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:48:14,437][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.0/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.08it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre: nan
                                                              train/auc: 0.859  
                                                              train/f1: 0.864   
                                                              train/precision:  
                                                              0.838             
                                                              train/recall:     
                                                              0.891 train/mre:  
                                                              nan               
[2024-06-05 18:48:29,855][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.0/9>
[2024-06-05 18:48:29,855][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:48:29,857][HYDRA] 	#430 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 18:48:30,055][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:48:30,057][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:48:30,058][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:48:30,059][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:48:30,061][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:48:30,061][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:48:30,062][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:48:30,062][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:48:30,063][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:48:30,064][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:48:30,064][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:48:30,065][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:48:30,107][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.05/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 67.01it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.421     
                                                              val/precision:    
                                                              0.444 val/recall: 
                                                              0.400 val/mre:    
                                                              0.056 train/auc:  
                                                              0.836 train/f1:   
                                                              0.840             
                                                              train/precision:  
                                                              0.821             
                                                              train/recall:     
                                                              0.859 train/mre:  
                                                              0.061             
[2024-06-05 18:48:45,993][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.05/0>
[2024-06-05 18:48:45,993][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:48:45,996][HYDRA] 	#431 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 18:48:46,225][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:48:46,226][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:48:46,228][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:48:46,228][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:48:46,230][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:48:46,231][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:48:46,231][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:48:46,232][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:48:46,233][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:48:46,235][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:48:46,235][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:48:46,237][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:48:46,347][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.05/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 66.26it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.057 train/auc:  
                                                              0.891 train/f1:   
                                                              0.894             
                                                              train/precision:  
                                                              0.868             
                                                              train/recall:     
                                                              0.922 train/mre:  
                                                              0.061             
[2024-06-05 18:49:02,029][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.05/1>
[2024-06-05 18:49:02,029][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:49:02,040][HYDRA] 	#432 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 18:49:02,265][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:49:02,266][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:49:02,268][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:49:02,268][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:49:02,271][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:49:02,272][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:49:02,272][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:49:02,272][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:49:02,274][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:49:02,274][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:49:02,275][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:49:02,276][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:49:02,328][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.05/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.71it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.625     
                                                              val/precision:    
                                                              0.833 val/recall: 
                                                              0.500 val/mre:    
                                                              0.060 train/auc:  
                                                              0.805 train/f1:   
                                                              0.800             
                                                              train/precision:  
                                                              0.820             
                                                              train/recall:     
                                                              0.781 train/mre:  
                                                              0.063             
[2024-06-05 18:49:18,559][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.05/2>
[2024-06-05 18:49:18,560][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:49:18,563][HYDRA] 	#433 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 18:49:18,789][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:49:18,791][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:49:18,792][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:49:18,792][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:49:18,796][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:49:18,797][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:49:18,798][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:49:18,798][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:49:18,799][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:49:18,800][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:49:18,800][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:49:18,801][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:49:18,851][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.05/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 58.79it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              0.375 val/recall: 
                                                              0.300 val/mre:    
                                                              0.057 train/auc:  
                                                              0.836 train/f1:   
                                                              0.821             
                                                              train/precision:  
                                                              0.906             
                                                              train/recall:     
                                                              0.750 train/mre:  
                                                              0.067             
[2024-06-05 18:49:34,812][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.05/3>
[2024-06-05 18:49:34,813][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:49:34,817][HYDRA] 	#434 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 18:49:35,014][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:49:35,016][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:49:35,017][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:49:35,017][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:49:35,020][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:49:35,020][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:49:35,021][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:49:35,021][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:49:35,022][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:49:35,023][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:49:35,023][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:49:35,024][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:49:35,058][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.05/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 56.75it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.522     
                                                              val/precision:    
                                                              0.462 val/recall: 
                                                              0.600 val/mre:    
                                                              0.070 train/auc:  
                                                              0.742 train/f1:   
                                                              0.718             
                                                              train/precision:  
                                                              0.792             
                                                              train/recall:     
                                                              0.656 train/mre:  
                                                              0.070             
[2024-06-05 18:49:51,571][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.05/4>
[2024-06-05 18:49:51,572][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:49:51,575][HYDRA] 	#435 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 18:49:51,792][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:49:51,794][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:49:51,795][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:49:51,795][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:49:51,798][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:49:51,798][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:49:51,799][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:49:51,799][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:49:51,800][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:49:51,802][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:49:51,803][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:49:51,804][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:49:51,928][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.05/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 56.74it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.375     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.300 val/mre:    
                                                              0.064 train/auc:  
                                                              0.797 train/f1:   
                                                              0.787             
                                                              train/precision:  
                                                              0.828             
                                                              train/recall:     
                                                              0.750 train/mre:  
                                                              0.066             
[2024-06-05 18:50:07,883][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.05/5>
[2024-06-05 18:50:07,886][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:50:07,890][HYDRA] 	#436 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 18:50:08,100][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:50:08,101][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:50:08,102][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:50:08,103][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:50:08,105][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:50:08,105][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:50:08,106][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:50:08,106][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:50:08,107][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:50:08,108][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:50:08,108][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:50:08,110][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:50:08,149][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.05/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.57it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.063 train/auc:  
                                                              0.789 train/f1:   
                                                              0.814             
                                                              train/precision:  
                                                              0.728             
                                                              train/recall:     
                                                              0.922 train/mre:  
                                                              0.069             
[2024-06-05 18:50:23,984][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.05/6>
[2024-06-05 18:50:23,987][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:50:23,990][HYDRA] 	#437 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 18:50:24,200][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:50:24,202][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:50:24,203][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:50:24,203][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:50:24,205][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:50:24,206][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:50:24,206][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:50:24,207][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:50:24,208][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:50:24,208][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:50:24,208][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:50:24,210][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:50:24,249][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.05/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.17it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.444     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.400 val/mre:    
                                                              0.059 train/auc:  
                                                              0.805 train/f1:   
                                                              0.806             
                                                              train/precision:  
                                                              0.800             
                                                              train/recall:     
                                                              0.812 train/mre:  
                                                              0.060             
[2024-06-05 18:50:40,191][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.05/7>
[2024-06-05 18:50:40,192][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:50:40,197][HYDRA] 	#438 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 18:50:40,407][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:50:40,408][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:50:40,410][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:50:40,410][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:50:40,412][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:50:40,413][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:50:40,413][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:50:40,414][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:50:40,415][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:50:40,416][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:50:40,416][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:50:40,417][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:50:40,455][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.05/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.14it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre:    
                                                              0.058 train/auc:  
                                                              0.766 train/f1:   
                                                              0.789             
                                                              train/precision:  
                                                              0.718             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              0.060             
[2024-06-05 18:50:56,060][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.05/8>
[2024-06-05 18:50:56,060][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:50:56,063][HYDRA] 	#439 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.05 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 18:50:56,281][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:50:56,283][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:50:56,284][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:50:56,284][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:50:56,287][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:50:56,288][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:50:56,288][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:50:56,289][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:50:56,290][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:50:56,292][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:50:56,292][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:50:56,294][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:50:56,426][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.05/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.93it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.064 train/auc:  
                                                              0.875 train/f1:   
                                                              0.882             
                                                              train/precision:  
                                                              0.833             
                                                              train/recall:     
                                                              0.938 train/mre:  
                                                              0.067             
[2024-06-05 18:51:12,590][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.05/9>
[2024-06-05 18:51:12,591][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:51:12,594][HYDRA] 	#440 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 18:51:12,819][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:51:12,821][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:51:12,822][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:51:12,823][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:51:12,825][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:51:12,825][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:51:12,826][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:51:12,826][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:51:12,827][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:51:12,828][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:51:12,828][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:51:12,830][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:51:12,867][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.1/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 50.82it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.052 train/auc:  
                                                              0.875 train/f1:   
                                                              0.882             
                                                              train/precision:  
                                                              0.833             
                                                              train/recall:     
                                                              0.938 train/mre:  
                                                              0.056             
[2024-06-05 18:51:28,905][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.1/0>
[2024-06-05 18:51:28,906][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:51:28,910][HYDRA] 	#441 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 18:51:29,115][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:51:29,116][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:51:29,118][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:51:29,118][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:51:29,123][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:51:29,123][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:51:29,124][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:51:29,124][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:51:29,125][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:51:29,126][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:51:29,126][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:51:29,127][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:51:29,178][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.1/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 58.00it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.056 train/auc:  
                                                              0.938 train/f1:   
                                                              0.938             
                                                              train/precision:  
                                                              0.924             
                                                              train/recall:     
                                                              0.953 train/mre:  
                                                              0.061             
[2024-06-05 18:51:45,133][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.1/1>
[2024-06-05 18:51:45,134][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:51:45,136][HYDRA] 	#442 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 18:51:45,341][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:51:45,343][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:51:45,344][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:51:45,344][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:51:45,346][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:51:45,347][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:51:45,347][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:51:45,348][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:51:45,349][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:51:45,349][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:51:45,350][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:51:45,351][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:51:45,396][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.1/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.36it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.600 val/mre:    
                                                              0.058 train/auc:  
                                                              0.789 train/f1:   
                                                              0.791             
                                                              train/precision:  
                                                              0.785             
                                                              train/recall:     
                                                              0.797 train/mre:  
                                                              0.061             
[2024-06-05 18:52:00,867][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.1/2>
[2024-06-05 18:52:00,868][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:52:00,870][HYDRA] 	#443 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 18:52:01,092][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:52:01,094][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:52:01,095][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:52:01,095][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:52:01,098][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:52:01,098][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:52:01,099][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:52:01,099][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:52:01,100][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:52:01,103][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:52:01,103][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:52:01,104][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:52:01,214][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.1/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.12it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre:    
                                                              0.053 train/auc:  
                                                              0.828 train/f1:   
                                                              0.847             
                                                              train/precision:  
                                                              0.762             
                                                              train/recall:     
                                                              0.953 train/mre:  
                                                              0.063             
[2024-06-05 18:52:17,162][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.1/3>
[2024-06-05 18:52:17,165][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:52:17,169][HYDRA] 	#444 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 18:52:17,383][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:52:17,385][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:52:17,386][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:52:17,386][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:52:17,388][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:52:17,389][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:52:17,389][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:52:17,390][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:52:17,391][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:52:17,392][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:52:17,392][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:52:17,393][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:52:17,441][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.1/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.47it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.588     
                                                              val/precision:    
                                                              0.714 val/recall: 
                                                              0.500 val/mre:    
                                                              0.058 train/auc:  
                                                              0.852 train/f1:   
                                                              0.861             
                                                              train/precision:  
                                                              0.808             
                                                              train/recall:     
                                                              0.922 train/mre:  
                                                              0.063             
[2024-06-05 18:52:33,223][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.1/4>
[2024-06-05 18:52:33,223][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:52:33,226][HYDRA] 	#445 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 18:52:33,440][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:52:33,442][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:52:33,443][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:52:33,443][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:52:33,445][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:52:33,446][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:52:33,446][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:52:33,447][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:52:33,448][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:52:33,448][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:52:33,448][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:52:33,450][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:52:33,498][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.1/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.55it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.059 train/auc:  
                                                              0.844 train/f1:   
                                                              0.859             
                                                              train/precision:  
                                                              0.782             
                                                              train/recall:     
                                                              0.953 train/mre:  
                                                              0.062             
[2024-06-05 18:52:49,237][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.1/5>
[2024-06-05 18:52:49,238][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:52:49,242][HYDRA] 	#446 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 18:52:49,454][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:52:49,456][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:52:49,457][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:52:49,457][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:52:49,459][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:52:49,460][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:52:49,460][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:52:49,461][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:52:49,462][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:52:49,462][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:52:49,462][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:52:49,464][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:52:49,512][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.1/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.30it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.059 train/auc:  
                                                              0.758 train/f1:   
                                                              0.789             
                                                              train/precision:  
                                                              0.699             
                                                              train/recall:     
                                                              0.906 train/mre:  
                                                              0.063             
[2024-06-05 18:53:05,496][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.1/6>
[2024-06-05 18:53:05,497][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:53:05,500][HYDRA] 	#447 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 18:53:05,705][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:53:05,707][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:53:05,708][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:53:05,709][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:53:05,711][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:53:05,712][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:53:05,712][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:53:05,713][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:53:05,714][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:53:05,716][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:53:05,716][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:53:05,718][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:53:05,841][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.1/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.73it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.545     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.600 val/mre:    
                                                              0.056 train/auc:  
                                                              0.727 train/f1:   
                                                              0.762             
                                                              train/precision:  
                                                              0.675             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              0.056             
[2024-06-05 18:53:22,273][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.1/7>
[2024-06-05 18:53:22,273][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:53:22,276][HYDRA] 	#448 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 18:53:22,494][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:53:22,496][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:53:22,497][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:53:22,497][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:53:22,499][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:53:22,500][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:53:22,500][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:53:22,501][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:53:22,502][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:53:22,503][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:53:22,503][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:53:22,504][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:53:22,546][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.1/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 58.88it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.050 train/auc:  
                                                              0.805 train/f1:   
                                                              0.820             
                                                              train/precision:  
                                                              0.760             
                                                              train/recall:     
                                                              0.891 train/mre:  
                                                              0.053             
[2024-06-05 18:53:38,507][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.1/8>
[2024-06-05 18:53:38,508][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:53:38,511][HYDRA] 	#449 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.1 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 18:53:38,717][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:53:38,719][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:53:38,720][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:53:38,720][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:53:38,723][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:53:38,723][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:53:38,724][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:53:38,724][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:53:38,725][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:53:38,726][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:53:38,726][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:53:38,727][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:53:38,766][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.1/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 58.65it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.154     
                                                              val/precision:    
                                                              0.333 val/recall: 
                                                              0.100 val/mre:    
                                                              0.059 train/auc:  
                                                              0.930 train/f1:   
                                                              0.931             
                                                              train/precision:  
                                                              0.910             
                                                              train/recall:     
                                                              0.953 train/mre:  
                                                              0.064             
[2024-06-05 18:53:55,019][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.1/9>
[2024-06-05 18:53:55,020][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:53:55,025][HYDRA] 	#450 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 18:53:55,325][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:53:55,327][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:53:55,329][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:53:55,329][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:53:55,332][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:53:55,332][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:53:55,333][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:53:55,333][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:53:55,334][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:53:55,335][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:53:55,335][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:53:55,336][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:53:55,434][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.15/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 58.00it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.051 train/auc:  
                                                              0.883 train/f1:   
                                                              0.895             
                                                              train/precision:  
                                                              0.810             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.057             
[2024-06-05 18:54:12,311][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.15/0>
[2024-06-05 18:54:12,312][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:54:12,316][HYDRA] 	#451 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 18:54:12,592][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:54:12,594][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:54:12,595][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:54:12,596][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:54:12,598][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:54:12,599][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:54:12,599][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:54:12,600][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:54:12,601][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:54:12,603][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:54:12,603][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:54:12,605][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:54:12,902][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.15/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.97it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.353     
                                                              val/precision:    
                                                              0.429 val/recall: 
                                                              0.300 val/mre:    
                                                              0.057 train/auc:  
                                                              0.977 train/f1:   
                                                              0.976             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              0.969 train/mre:  
                                                              0.059             
[2024-06-05 18:54:29,039][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.15/1>
[2024-06-05 18:54:29,039][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:54:29,042][HYDRA] 	#452 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 18:54:29,244][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:54:29,245][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:54:29,246][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:54:29,247][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:54:29,249][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:54:29,249][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:54:29,250][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:54:29,250][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:54:29,251][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:54:29,254][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:54:29,254][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:54:29,256][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:54:29,303][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.15/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.31it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.059 train/auc:  
                                                              0.875 train/f1:   
                                                              0.879             
                                                              train/precision:  
                                                              0.853             
                                                              train/recall:     
                                                              0.906 train/mre:  
                                                              0.063             
[2024-06-05 18:54:45,084][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.15/2>
[2024-06-05 18:54:45,085][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:54:45,087][HYDRA] 	#453 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 18:54:45,313][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:54:45,315][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:54:45,316][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:54:45,317][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:54:45,319][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:54:45,319][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:54:45,320][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:54:45,320][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:54:45,321][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:54:45,322][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:54:45,322][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:54:45,323][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:54:45,376][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.15/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 57.79it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.055 train/auc:  
                                                              0.961 train/f1:   
                                                              0.961             
                                                              train/precision:  
                                                              0.968             
                                                              train/recall:     
                                                              0.953 train/mre:  
                                                              0.061             
[2024-06-05 18:55:02,079][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.15/3>
[2024-06-05 18:55:02,084][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:55:02,089][HYDRA] 	#454 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 18:55:02,293][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:55:02,295][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:55:02,296][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:55:02,296][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:55:02,298][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:55:02,299][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:55:02,299][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:55:02,300][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:55:02,301][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:55:02,301][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:55:02,302][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:55:02,303][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:55:02,353][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.15/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 58.08it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.400 val/mre:    
                                                              0.061 train/auc:  
                                                              0.914 train/f1:   
                                                              0.915             
                                                              train/precision:  
                                                              0.908             
                                                              train/recall:     
                                                              0.922 train/mre:  
                                                              0.065             
[2024-06-05 18:55:19,105][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.15/4>
[2024-06-05 18:55:19,106][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:55:19,109][HYDRA] 	#455 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 18:55:19,382][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:55:19,383][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:55:19,384][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:55:19,385][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:55:19,387][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:55:19,388][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:55:19,388][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:55:19,389][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:55:19,390][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:55:19,393][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:55:19,393][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:55:19,394][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:55:19,544][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.15/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.32it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.267     
                                                              val/precision:    
                                                              0.400 val/recall: 
                                                              0.200 val/mre:    
                                                              0.058 train/auc:  
                                                              0.922 train/f1:   
                                                              0.924             
                                                              train/precision:  
                                                              0.897             
                                                              train/recall:     
                                                              0.953 train/mre:  
                                                              0.066             
[2024-06-05 18:55:36,666][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.15/5>
[2024-06-05 18:55:36,667][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:55:36,669][HYDRA] 	#456 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 18:55:36,869][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:55:36,871][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:55:36,872][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:55:36,873][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:55:36,875][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:55:36,875][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:55:36,876][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:55:36,876][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:55:36,877][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:55:36,878][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:55:36,878][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:55:36,880][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:55:36,922][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.15/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 57.07it/s v_num: 0.000      
                                                              val/auc: 0.350    
                                                              val/f1: 0.235     
                                                              val/precision:    
                                                              0.286 val/recall: 
                                                              0.200 val/mre:    
                                                              0.062 train/auc:  
                                                              0.844 train/f1:   
                                                              0.841             
                                                              train/precision:  
                                                              0.855             
                                                              train/recall:     
                                                              0.828 train/mre:  
                                                              0.058             
[2024-06-05 18:55:53,792][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.15/6>
[2024-06-05 18:55:53,793][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:55:53,795][HYDRA] 	#457 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 18:55:53,997][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:55:53,998][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:55:54,000][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:55:54,000][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:55:54,003][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:55:54,003][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:55:54,004][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:55:54,004][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:55:54,005][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:55:54,006][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:55:54,006][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:55:54,007][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:55:54,044][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.15/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.26it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.053 train/auc:  
                                                              0.867 train/f1:   
                                                              0.870             
                                                              train/precision:  
                                                              0.851             
                                                              train/recall:     
                                                              0.891 train/mre:  
                                                              0.061             
[2024-06-05 18:56:10,914][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.15/7>
[2024-06-05 18:56:10,915][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:56:10,918][HYDRA] 	#458 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 18:56:11,117][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:56:11,119][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:56:11,120][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:56:11,120][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:56:11,123][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:56:11,123][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:56:11,123][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:56:11,124][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:56:11,125][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:56:11,125][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:56:11,126][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:56:11,127][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:56:11,173][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.15/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.66it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.070 train/auc:  
                                                              0.641 train/f1:   
                                                              0.736             
                                                              train/precision:  
                                                              0.582             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.064             
[2024-06-05 18:56:27,018][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.15/8>
[2024-06-05 18:56:27,019][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:56:27,022][HYDRA] 	#459 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.15 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 18:56:27,238][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:56:27,239][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:56:27,241][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:56:27,241][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:56:27,243][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:56:27,244][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:56:27,244][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:56:27,244][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:56:27,245][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:56:27,248][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:56:27,248][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:56:27,250][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:56:27,374][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.15/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.06it/s v_num: 0.000      
                                                              val/auc: 0.400    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.057 train/auc:  
                                                              0.906 train/f1:   
                                                              0.902             
                                                              train/precision:  
                                                              0.948             
                                                              train/recall:     
                                                              0.859 train/mre:  
                                                              0.060             
[2024-06-05 18:56:43,244][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.15/9>
[2024-06-05 18:56:43,246][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:56:43,249][HYDRA] 	#460 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 18:56:43,448][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:56:43,450][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:56:43,451][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:56:43,452][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:56:43,454][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:56:43,454][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:56:43,455][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:56:43,455][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:56:43,456][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:56:43,457][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:56:43,457][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:56:43,459][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:56:43,497][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.2/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.35it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.054 train/auc:  
                                                              0.914 train/f1:   
                                                              0.906             
                                                              train/precision:  
                                                              1.000             
                                                              train/recall:     
                                                              0.828 train/mre:  
                                                              0.062             
[2024-06-05 18:56:59,954][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.2/0>
[2024-06-05 18:56:59,955][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:56:59,974][HYDRA] 	#461 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 18:57:00,183][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:57:00,184][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:57:00,186][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:57:00,186][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:57:00,188][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:57:00,189][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:57:00,189][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:57:00,190][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:57:00,191][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:57:00,191][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:57:00,192][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:57:00,193][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:57:00,230][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.2/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.11it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.056 train/auc:  
                                                              0.938 train/f1:   
                                                              0.937             
                                                              train/precision:  
                                                              0.952             
                                                              train/recall:     
                                                              0.922 train/mre:  
                                                              0.060             
[2024-06-05 18:57:16,157][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.2/1>
[2024-06-05 18:57:16,158][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:57:16,160][HYDRA] 	#462 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 18:57:16,368][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:57:16,370][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:57:16,371][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:57:16,371][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:57:16,373][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:57:16,374][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:57:16,374][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:57:16,375][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:57:16,376][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:57:16,376][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:57:16,377][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:57:16,378][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:57:16,415][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.2/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.16it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.059 train/auc:  
                                                              0.914 train/f1:   
                                                              0.912             
                                                              train/precision:  
                                                              0.934             
                                                              train/recall:     
                                                              0.891 train/mre:  
                                                              0.062             
[2024-06-05 18:57:32,057][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.2/2>
[2024-06-05 18:57:32,058][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:57:32,060][HYDRA] 	#463 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 18:57:32,275][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:57:32,277][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:57:32,278][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:57:32,278][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:57:32,280][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:57:32,281][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:57:32,281][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:57:32,282][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:57:32,283][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:57:32,285][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:57:32,286][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:57:32,287][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:57:32,404][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.2/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.00it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre:    
                                                              0.060 train/auc:  
                                                              0.812 train/f1:   
                                                              0.831             
                                                              train/precision:  
                                                              0.756             
                                                              train/recall:     
                                                              0.922 train/mre:  
                                                              0.065             
[2024-06-05 18:57:48,233][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.2/3>
[2024-06-05 18:57:48,234][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:57:48,237][HYDRA] 	#464 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 18:57:48,506][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:57:48,507][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:57:48,508][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:57:48,509][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:57:48,512][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:57:48,513][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:57:48,514][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:57:48,514][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:57:48,515][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:57:48,516][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:57:48,516][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:57:48,518][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:57:48,610][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.2/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.61it/s v_num: 0.000      
                                                              val/auc: 0.700    
                                                              val/f1: 0.571     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.400 val/mre:    
                                                              0.061 train/auc:  
                                                              0.859 train/f1:   
                                                              0.873             
                                                              train/precision:  
                                                              0.795             
                                                              train/recall:     
                                                              0.969 train/mre:  
                                                              0.064             
[2024-06-05 18:58:04,483][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.2/4>
[2024-06-05 18:58:04,486][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:58:04,489][HYDRA] 	#465 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 18:58:04,720][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:58:04,722][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:58:04,723][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:58:04,724][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:58:04,726][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:58:04,726][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:58:04,727][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:58:04,727][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:58:04,728][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:58:04,729][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:58:04,729][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:58:04,731][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:58:04,775][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.2/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 64.19it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.060 train/auc:  
                                                              0.906 train/f1:   
                                                              0.912             
                                                              train/precision:  
                                                              0.861             
                                                              train/recall:     
                                                              0.969 train/mre:  
                                                              0.068             
[2024-06-05 18:58:20,019][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.2/5>
[2024-06-05 18:58:20,021][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:58:20,023][HYDRA] 	#466 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 18:58:20,245][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:58:20,246][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:58:20,248][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:58:20,248][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:58:20,250][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:58:20,251][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:58:20,251][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:58:20,252][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:58:20,253][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:58:20,253][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:58:20,254][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:58:20,255][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:58:20,289][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.2/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 65.47it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.055 train/auc:  
                                                              0.859 train/f1:   
                                                              0.862             
                                                              train/precision:  
                                                              0.848             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              0.061             
[2024-06-05 18:58:35,477][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.2/6>
[2024-06-05 18:58:35,477][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:58:35,479][HYDRA] 	#467 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 18:58:35,683][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:58:35,684][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:58:35,685][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:58:35,686][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:58:35,688][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:58:35,688][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:58:35,689][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:58:35,689][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:58:35,690][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:58:35,693][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:58:35,693][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:58:35,695][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:58:35,808][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.2/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 58.50it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre:    
                                                              0.055 train/auc:  
                                                              0.812 train/f1:   
                                                              0.824             
                                                              train/precision:  
                                                              0.778             
                                                              train/recall:     
                                                              0.875 train/mre:  
                                                              0.057             
[2024-06-05 18:58:51,520][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.2/7>
[2024-06-05 18:58:51,520][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:58:51,522][HYDRA] 	#468 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 18:58:51,730][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:58:51,731][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:58:51,733][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:58:51,733][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:58:51,735][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:58:51,736][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:58:51,736][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:58:51,737][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:58:51,738][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:58:51,738][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:58:51,739][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:58:51,740][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:58:51,779][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.2/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.95it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.400     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.300 val/mre:    
                                                              0.054 train/auc:  
                                                              0.836 train/f1:   
                                                              0.840             
                                                              train/precision:  
                                                              0.821             
                                                              train/recall:     
                                                              0.859 train/mre:  
                                                              0.058             
[2024-06-05 18:59:07,290][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.2/8>
[2024-06-05 18:59:07,291][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:59:07,300][HYDRA] 	#469 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.2 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 18:59:07,509][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:59:07,510][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:59:07,512][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:59:07,512][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:59:07,514][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:59:07,515][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:59:07,515][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:59:07,516][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:59:07,517][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:59:07,517][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:59:07,518][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:59:07,519][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:59:07,557][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.2/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.61it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.056 train/auc:  
                                                              0.930 train/f1:   
                                                              0.930             
                                                              train/precision:  
                                                              0.923             
                                                              train/recall:     
                                                              0.938 train/mre:  
                                                              0.063             
[2024-06-05 18:59:23,587][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.2/9>
[2024-06-05 18:59:23,588][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:59:23,594][HYDRA] 	#470 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 18:59:23,816][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:59:23,817][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:59:23,819][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:59:23,819][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:59:23,822][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:59:23,822][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:59:23,823][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:59:23,823][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:59:23,824][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:59:23,825][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:59:23,825][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:59:23,826][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:59:23,885][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.25/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.44it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.471     
                                                              val/precision:    
                                                              0.571 val/recall: 
                                                              0.400 val/mre:    
                                                              0.058 train/auc:  
                                                              0.906 train/f1:   
                                                              0.898             
                                                              train/precision:  
                                                              0.981             
                                                              train/recall:     
                                                              0.828 train/mre:  
                                                              0.058             
[2024-06-05 18:59:39,364][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.25/0>
[2024-06-05 18:59:39,365][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:59:39,367][HYDRA] 	#471 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 18:59:39,573][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:59:39,575][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:59:39,576][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:59:39,577][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:59:39,579][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:59:39,579][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:59:39,580][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:59:39,580][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:59:39,581][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:59:39,584][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:59:39,584][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:59:39,586][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:59:39,691][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.25/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.94it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.059 train/auc:  
                                                              0.961 train/f1:   
                                                              0.960             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              0.938 train/mre:  
                                                              0.061             
[2024-06-05 18:59:55,243][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.25/1>
[2024-06-05 18:59:55,244][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 18:59:55,248][HYDRA] 	#472 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 18:59:55,454][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 18:59:55,456][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 18:59:55,457][train.py][INFO] - Instantiating callbacks...
[2024-06-05 18:59:55,457][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 18:59:55,460][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 18:59:55,460][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 18:59:55,461][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 18:59:55,461][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 18:59:55,462][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 18:59:55,463][train.py][INFO] - Instantiating loggers...
[2024-06-05 18:59:55,463][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 18:59:55,465][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 18:59:55,503][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.25/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.00it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.600     
                                                              val/precision:    
                                                              0.600 val/recall: 
                                                              0.600 val/mre:    
                                                              0.056 train/auc:  
                                                              0.953 train/f1:   
                                                              0.952             
                                                              train/precision:  
                                                              0.968             
                                                              train/recall:     
                                                              0.938 train/mre:  
                                                              0.061             
[2024-06-05 19:00:11,702][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.25/2>
[2024-06-05 19:00:11,703][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 19:00:11,706][HYDRA] 	#473 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 19:00:11,938][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 19:00:11,940][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 19:00:11,941][train.py][INFO] - Instantiating callbacks...
[2024-06-05 19:00:11,941][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 19:00:11,944][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 19:00:11,944][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 19:00:11,944][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 19:00:11,945][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 19:00:11,946][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 19:00:11,946][train.py][INFO] - Instantiating loggers...
[2024-06-05 19:00:11,947][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 19:00:11,948][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 19:00:11,994][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.25/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.06it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.462     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.300 val/mre:    
                                                              0.053 train/auc:  
                                                              0.938 train/f1:   
                                                              0.939             
                                                              train/precision:  
                                                              0.912             
                                                              train/recall:     
                                                              0.969 train/mre:  
                                                              0.060             
[2024-06-05 19:00:28,462][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.25/3>
[2024-06-05 19:00:28,463][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 19:00:28,470][HYDRA] 	#474 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 19:00:28,669][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 19:00:28,670][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 19:00:28,672][train.py][INFO] - Instantiating callbacks...
[2024-06-05 19:00:28,672][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 19:00:28,674][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 19:00:28,675][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 19:00:28,675][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 19:00:28,675][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 19:00:28,676][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 19:00:28,677][train.py][INFO] - Instantiating loggers...
[2024-06-05 19:00:28,677][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 19:00:28,679][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 19:00:28,714][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.25/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 54.23it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.500 val/mre:    
                                                              0.058 train/auc:  
                                                              0.945 train/f1:   
                                                              0.947             
                                                              train/precision:  
                                                              0.913             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.064             
[2024-06-05 19:00:44,416][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.25/4>
[2024-06-05 19:00:44,416][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 19:00:44,419][HYDRA] 	#475 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 19:00:44,618][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 19:00:44,620][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 19:00:44,621][train.py][INFO] - Instantiating callbacks...
[2024-06-05 19:00:44,622][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 19:00:44,626][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 19:00:44,626][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 19:00:44,627][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 19:00:44,627][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 19:00:44,628][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 19:00:44,631][train.py][INFO] - Instantiating loggers...
[2024-06-05 19:00:44,631][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 19:00:44,632][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 19:00:44,731][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.25/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 63.00it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.057 train/auc:  
                                                              0.961 train/f1:   
                                                              0.962             
                                                              train/precision:  
                                                              0.940             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.066             
[2024-06-05 19:01:00,082][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.25/5>
[2024-06-05 19:01:00,082][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 19:01:00,085][HYDRA] 	#476 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 19:01:00,275][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 19:01:00,276][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 19:01:00,278][train.py][INFO] - Instantiating callbacks...
[2024-06-05 19:01:00,278][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 19:01:00,280][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 19:01:00,280][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 19:01:00,281][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 19:01:00,281][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 19:01:00,282][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 19:01:00,283][train.py][INFO] - Instantiating loggers...
[2024-06-05 19:01:00,283][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 19:01:00,285][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 19:01:00,332][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.25/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 58.48it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.053 train/auc:  
                                                              0.906 train/f1:   
                                                              0.912             
                                                              train/precision:  
                                                              0.861             
                                                              train/recall:     
                                                              0.969 train/mre:  
                                                              0.058             
[2024-06-05 19:01:16,488][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.25/6>
[2024-06-05 19:01:16,490][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 19:01:16,495][HYDRA] 	#477 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 19:01:16,803][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 19:01:16,804][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 19:01:16,805][train.py][INFO] - Instantiating callbacks...
[2024-06-05 19:01:16,806][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 19:01:16,809][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 19:01:16,810][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 19:01:16,810][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 19:01:16,811][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 19:01:16,812][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 19:01:16,813][train.py][INFO] - Instantiating loggers...
[2024-06-05 19:01:16,813][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 19:01:16,815][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 19:01:16,908][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.25/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.95it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.055 train/auc:  
                                                              0.680 train/f1:   
                                                              0.602             
                                                              train/precision:  
                                                              0.795             
                                                              train/recall:     
                                                              0.484 train/mre:  
                                                              0.055             
[2024-06-05 19:01:34,076][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.25/7>
[2024-06-05 19:01:34,077][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 19:01:34,080][HYDRA] 	#478 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 19:01:34,330][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 19:01:34,332][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 19:01:34,333][train.py][INFO] - Instantiating callbacks...
[2024-06-05 19:01:34,334][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 19:01:34,336][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 19:01:34,337][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 19:01:34,337][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 19:01:34,338][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 19:01:34,339][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 19:01:34,340][train.py][INFO] - Instantiating loggers...
[2024-06-05 19:01:34,340][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 19:01:34,341][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 19:01:34,405][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.25/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 62.02it/s v_num: 0.000      
                                                              val/auc: 0.650    
                                                              val/f1: 0.533     
                                                              val/precision:    
                                                              0.800 val/recall: 
                                                              0.400 val/mre:    
                                                              0.056 train/auc:  
                                                              0.844 train/f1:   
                                                              0.844             
                                                              train/precision:  
                                                              0.844             
                                                              train/recall:     
                                                              0.844 train/mre:  
                                                              0.060             
[2024-06-05 19:01:51,403][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.25/8>
[2024-06-05 19:01:51,405][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 19:01:51,410][HYDRA] 	#479 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.25 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 19:01:51,631][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 19:01:51,632][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 19:01:51,634][train.py][INFO] - Instantiating callbacks...
[2024-06-05 19:01:51,634][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 19:01:51,638][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 19:01:51,639][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 19:01:51,639][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 19:01:51,640][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 19:01:51,641][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 19:01:51,643][train.py][INFO] - Instantiating loggers...
[2024-06-05 19:01:51,643][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 19:01:51,645][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 19:01:51,795][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.25/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 54.38it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.058 train/auc:  
                                                              0.969 train/f1:   
                                                              0.969             
                                                              train/precision:  
                                                              0.969             
                                                              train/recall:     
                                                              0.969 train/mre:  
                                                              0.062             
[2024-06-05 19:02:08,668][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.25/9>
[2024-06-05 19:02:08,669][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 19:02:08,673][HYDRA] 	#480 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=0
[2024-06-05 19:02:08,884][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 19:02:08,886][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 19:02:08,887][train.py][INFO] - Instantiating callbacks...
[2024-06-05 19:02:08,887][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 19:02:08,890][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 19:02:08,890][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 19:02:08,891][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 19:02:08,891][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 19:02:08,892][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 19:02:08,893][train.py][INFO] - Instantiating loggers...
[2024-06-05 19:02:08,893][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 19:02:08,894][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 19:02:08,929][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.3/0/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.17it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.500     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.400 val/mre:    
                                                              0.056 train/auc:  
                                                              0.969 train/f1:   
                                                              0.970             
                                                              train/precision:  
                                                              0.941             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.061             
[2024-06-05 19:02:25,148][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.3/0>
[2024-06-05 19:02:25,148][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 19:02:25,153][HYDRA] 	#481 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=1
[rank: 0] Seed set to 1
[2024-06-05 19:02:25,357][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 19:02:25,359][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 19:02:25,360][train.py][INFO] - Instantiating callbacks...
[2024-06-05 19:02:25,360][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 19:02:25,362][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 19:02:25,363][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 19:02:25,363][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 19:02:25,364][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 19:02:25,365][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 19:02:25,365][train.py][INFO] - Instantiating loggers...
[2024-06-05 19:02:25,366][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 19:02:25,367][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 19:02:25,411][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.3/1/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 58.86it/s v_num: 0.000      
                                                              val/auc: 0.450    
                                                              val/f1: 0.000     
                                                              val/precision:    
                                                              0.000 val/recall: 
                                                              0.000 val/mre:    
                                                              0.058 train/auc:  
                                                              0.961 train/f1:   
                                                              0.960             
                                                              train/precision:  
                                                              0.984             
                                                              train/recall:     
                                                              0.938 train/mre:  
                                                              0.060             
[2024-06-05 19:02:40,882][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.3/1>
[2024-06-05 19:02:40,885][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 19:02:40,893][HYDRA] 	#482 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=2
[rank: 0] Seed set to 2
[2024-06-05 19:02:41,094][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 19:02:41,095][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 19:02:41,096][train.py][INFO] - Instantiating callbacks...
[2024-06-05 19:02:41,097][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 19:02:41,099][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 19:02:41,100][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 19:02:41,100][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 19:02:41,101][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 19:02:41,102][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 19:02:41,102][train.py][INFO] - Instantiating loggers...
[2024-06-05 19:02:41,103][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 19:02:41,104][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 19:02:41,142][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.3/2/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.52it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.429     
                                                              val/precision:    
                                                              0.750 val/recall: 
                                                              0.300 val/mre:    
                                                              0.065 train/auc:  
                                                              0.953 train/f1:   
                                                              0.955             
                                                              train/precision:  
                                                              0.926             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.064             
[2024-06-05 19:02:56,703][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.3/2>
[2024-06-05 19:02:56,705][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 19:02:56,707][HYDRA] 	#483 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=3
[rank: 0] Seed set to 3
[2024-06-05 19:02:56,935][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 19:02:56,936][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 19:02:56,937][train.py][INFO] - Instantiating callbacks...
[2024-06-05 19:02:56,938][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 19:02:56,940][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 19:02:56,940][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 19:02:56,941][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 19:02:56,941][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 19:02:56,942][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 19:02:56,945][train.py][INFO] - Instantiating loggers...
[2024-06-05 19:02:56,945][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 19:02:56,946][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 19:02:57,057][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.3/3/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 59.41it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre:    
                                                              0.061 train/auc:  
                                                              0.938 train/f1:   
                                                              0.938             
                                                              train/precision:  
                                                              0.938             
                                                              train/recall:     
                                                              0.938 train/mre:  
                                                              0.062             
[2024-06-05 19:03:12,731][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.3/3>
[2024-06-05 19:03:12,737][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 19:03:12,740][HYDRA] 	#484 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=4
[rank: 0] Seed set to 4
[2024-06-05 19:03:12,948][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 19:03:12,949][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 19:03:12,951][train.py][INFO] - Instantiating callbacks...
[2024-06-05 19:03:12,951][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 19:03:12,953][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 19:03:12,954][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 19:03:12,954][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 19:03:12,955][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 19:03:12,956][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 19:03:12,957][train.py][INFO] - Instantiating loggers...
[2024-06-05 19:03:12,957][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 19:03:12,958][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 19:03:12,999][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.3/4/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 61.55it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.167     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.100 val/mre:    
                                                              0.061 train/auc:  
                                                              0.781 train/f1:   
                                                              0.754             
                                                              train/precision:  
                                                              0.860             
                                                              train/recall:     
                                                              0.672 train/mre:  
                                                              0.067             
[2024-06-05 19:03:28,499][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.3/4>
[2024-06-05 19:03:28,499][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 19:03:28,503][HYDRA] 	#485 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=5
[rank: 0] Seed set to 5
[2024-06-05 19:03:28,698][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 19:03:28,699][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 19:03:28,701][train.py][INFO] - Instantiating callbacks...
[2024-06-05 19:03:28,701][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 19:03:28,703][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 19:03:28,703][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 19:03:28,704][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 19:03:28,704][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 19:03:28,705][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 19:03:28,706][train.py][INFO] - Instantiating loggers...
[2024-06-05 19:03:28,706][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 19:03:28,707][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 19:03:28,741][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.3/5/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.67it/s v_num: 0.000      
                                                              val/auc: 0.600    
                                                              val/f1: 0.333     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.200 val/mre:    
                                                              0.059 train/auc:  
                                                              0.938 train/f1:   
                                                              0.940             
                                                              train/precision:  
                                                              0.900             
                                                              train/recall:     
                                                              0.984 train/mre:  
                                                              0.064             
[2024-06-05 19:03:43,939][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.3/5>
[2024-06-05 19:03:43,940][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 19:03:43,946][HYDRA] 	#486 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=6
[rank: 0] Seed set to 6
[2024-06-05 19:03:44,150][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 19:03:44,151][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 19:03:44,153][train.py][INFO] - Instantiating callbacks...
[2024-06-05 19:03:44,153][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 19:03:44,155][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 19:03:44,156][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 19:03:44,156][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 19:03:44,156][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 19:03:44,158][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 19:03:44,158][train.py][INFO] - Instantiating loggers...
[2024-06-05 19:03:44,158][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 19:03:44,160][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 19:03:44,197][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.3/6/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 60.68it/s v_num: 0.000      
                                                              val/auc: 0.500    
                                                              val/f1: 0.286     
                                                              val/precision:    
                                                              0.500 val/recall: 
                                                              0.200 val/mre:    
                                                              0.055 train/auc:  
                                                              0.859 train/f1:   
                                                              0.868             
                                                              train/precision:  
                                                              0.819             
                                                              train/recall:     
                                                              0.922 train/mre:  
                                                              0.060             
[2024-06-05 19:03:59,491][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.3/6>
[2024-06-05 19:03:59,492][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 19:03:59,496][HYDRA] 	#487 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=7
[rank: 0] Seed set to 7
[2024-06-05 19:03:59,706][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 19:03:59,708][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 19:03:59,709][train.py][INFO] - Instantiating callbacks...
[2024-06-05 19:03:59,710][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 19:03:59,713][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 19:03:59,714][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 19:03:59,714][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 19:03:59,715][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 19:03:59,716][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 19:03:59,718][train.py][INFO] - Instantiating loggers...
[2024-06-05 19:03:59,718][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 19:03:59,719][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 19:03:59,820][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.3/7/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 57.16it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.056 train/auc:  
                                                              0.844 train/f1:   
                                                              0.839             
                                                              train/precision:  
                                                              0.867             
                                                              train/recall:     
                                                              0.812 train/mre:  
                                                              0.059             
[2024-06-05 19:04:15,530][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.3/7>
[2024-06-05 19:04:15,531][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 19:04:15,534][HYDRA] 	#488 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=8
[rank: 0] Seed set to 8
[2024-06-05 19:04:15,760][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 19:04:15,762][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 19:04:15,763][train.py][INFO] - Instantiating callbacks...
[2024-06-05 19:04:15,763][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 19:04:15,765][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 19:04:15,766][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 19:04:15,766][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 19:04:15,767][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 19:04:15,768][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 19:04:15,769][train.py][INFO] - Instantiating loggers...
[2024-06-05 19:04:15,769][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 19:04:15,770][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 19:04:15,815][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.3/8/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 53.38it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.308     
                                                              val/precision:    
                                                              0.667 val/recall: 
                                                              0.200 val/mre:    
                                                              0.058 train/auc:  
                                                              0.875 train/f1:   
                                                              0.877             
                                                              train/precision:  
                                                              0.864             
                                                              train/recall:     
                                                              0.891 train/mre:  
                                                              0.056             
[2024-06-05 19:04:31,554][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.3/8>
[2024-06-05 19:04:31,557][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
[2024-06-05 19:04:31,559][HYDRA] 	#489 : task=debug trainer=cpu trainer.max_epochs=50 data=daicwoz data.type_missing=Random data.ricardo=True data.question=doing_today data.open_face=all data.ratio_missing=0.3 data.num_workers=1 data.batch_size=32 model=saits callbacks=default callbacks.model_checkpoint.monitor=val/f1 callbacks.clf_metrics.boot_val=False callbacks.imputation_metrics.boot_val=False logger=csv test=False seed=9
[rank: 0] Seed set to 9
[2024-06-05 19:04:31,778][train.py][INFO] - Instantiating datamodule <src.datamodules.daicwoz.DAICWOZDatamodule>
[2024-06-05 19:04:31,779][train.py][INFO] - Instantiating model <src.methods.transformer.lightningmodule.TransformerLightningModule>
[2024-06-05 19:04:31,781][train.py][INFO] - Instantiating callbacks...
[2024-06-05 19:04:31,781][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.ModelCheckpoint>
[2024-06-05 19:04:31,783][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichModelSummary>
[2024-06-05 19:04:31,784][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ClassificationMetricsCallback>
[2024-06-05 19:04:31,784][train.py][INFO] - Instantiating callback <src.callbacks.metrics.ImputationMetricsCallback>
[2024-06-05 19:04:31,784][train.py][INFO] - Instantiating callback <src.callbacks.generation.GenerationCallback>
[2024-06-05 19:04:31,786][train.py][INFO] - Instantiating callback <lightning.pytorch.callbacks.RichProgressBar>
[2024-06-05 19:04:31,786][train.py][INFO] - Instantiating loggers...
[2024-06-05 19:04:31,786][train.py][INFO] - Instantiating logger <lightning.pytorch.loggers.csv_logs.CSVLogger>
[2024-06-05 19:04:31,788][train.py][INFO] - Instantiating trainer <lightning.pytorch.Trainer>
Trainer already configured with model summary callbacks: [<class 'lightning.pytorch.callbacks.rich_model_summary.RichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[2024-06-05 19:04:31,832][train.py][INFO] - Starting training...
Missing logger folder: /home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.3/9/csv_artifacts/
┏━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━┳━━┓
┃  ┃ Name                                                                ┃  ┃  ┃
┡━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━╇━━┩
│  │ model                                                               │  │  │
│  │ model.embedding                                                     │  │  │
│  │ model.dropout                                                       │  │  │
│  │ model.position_enc                                                  │  │  │
│  │ model.encoder                                                       │  │  │
│  │ model.encoder.enc_layer_stack                                       │  │  │
│  │ model.encoder.enc_layer_stack.0                                     │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn                            │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_qs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_ks                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.w_vs                       │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator         │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.attention_operator.dropout │  │  │
│  │ model.encoder.enc_layer_stack.0.slf_attn.fc                         │  │  │
│  │ model.encoder.enc_layer_stack.0.dropout                             │  │  │
│  │ model.encoder.enc_layer_stack.0.layer_norm                          │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn                             │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_1                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.linear_2                    │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.layer_norm                  │  │  │
│  │ model.encoder.enc_layer_stack.0.pos_ffn.dropout                     │  │  │
│  │ model.output_projection                                             │  │  │
│  │ model.self_attention                                                │  │  │
│  │ model.output_net                                                    │  │  │
│  │ model.output_net.0                                                  │  │  │
│  │ model.output_net.1                                                  │  │  │
│  │ model.output_net.2                                                  │  │  │
│  │ model.output_net.3                                                  │  │  │
│  │ model.output_net.4                                                  │  │  │
│  │ model.saits_loss_func                                               │  │  │
│  │ clf_criterion                                                       │  │  │
│  │ saits_loss_func                                                     │  │  │
└──┴─────────────────────────────────────────────────────────────────────┴──┴──┘
Trainable params: 22.3 K                                                        
Non-trainable params: 0                                                         
Total params: 22.3 K                                                            
Total estimated model params size (MB): 0                                       
SLURM auto-requeueing enabled. Setting signal handlers.
/home/khickey/test_impute/env/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
`Trainer.fit` stopped: `max_epochs=50` reached.
Epoch 49/49 ━━━━━━━━━━━━━━━━━ 4/4 0:00:00 • 0:00:00 56.16it/s v_num: 0.000      
                                                              val/auc: 0.550    
                                                              val/f1: 0.182     
                                                              val/precision:    
                                                              1.000 val/recall: 
                                                              0.100 val/mre:    
                                                              0.062 train/auc:  
                                                              0.930 train/f1:   
                                                              0.934             
                                                              train/precision:  
                                                              0.877             
                                                              train/recall:     
                                                              1.000 train/mre:  
                                                              0.061             
[2024-06-05 19:04:47,422][train.py][INFO] - Training finished. Retrieving metrics. Experiment details found at </home/khickey/test_impute/logs/debug/multiruns/06-05-16-06-32/doing_today/0.3/9>
[2024-06-05 19:04:47,424][train.py][INFO] - Metric name is None! Skipping metric value retrieval...
